import os
import asyncio
import pandas as pd
import tiktoken
from typing import Dict, Any, List, Optional

from graphrag.config.enums import ModelType
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.language_model.manager import ModelManager
from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
    read_indexer_relationships,
    read_indexer_text_units,
    read_indexer_covariates,
)
from graphrag.query.structured_search.global_search.community_context import (
    GlobalCommunityContext,
)
from graphrag.query.structured_search.global_search.search import GlobalSearch
from graphrag.query.structured_search.local_search.mixed_context import (
    LocalSearchMixedContext,
)
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from dotenv import load_dotenv

load_dotenv()

class RAGEngine:
    """GraphRAGå¼•æ“ï¼Œå°†æ£€ç´¢å’ŒLLMè°ƒç”¨åˆ†ç¦»ï¼Œæ”¯æŒå¤šä¹¦æœ¬"""
    
    def __init__(self, input_dir: str = "./rag/output"):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„ï¼ŒåŒ…å«GraphRAGå¤„ç†åçš„æ•°æ®æ–‡ä»¶
        """
        self.input_dir = input_dir
        self.api_key = os.getenv("AZURE_OPENAI_API_KEY")
        self.embedding_key = os.getenv("Embedding_key")
        
        # åˆå§‹åŒ–é…ç½®
        self._init_configs()
        self._init_data()
        self._init_engines()
        
    def _init_configs(self):
        """åˆå§‹åŒ–LLMé…ç½®"""
        self.chat_config = LanguageModelConfig(
            type=ModelType.AzureOpenAIChat,
            api_base="https://tcamp.openai.azure.com/",
            api_version="2025-01-01-preview",
            auth_type="api_key",
            api_key=self.api_key,
            model="gpt-4o",
            deployment_name="gpt-4o",
            model_supports_json=True,
            concurrent_requests=25,
            async_mode="threaded",
            retry_strategy="native",
            max_retries=10,
            tokens_per_minute=120000,
            requests_per_minute=100,
            encoding_model="cl100k_base",
        )


        
        # self.embedding_config = LanguageModelConfig(
        #     api_key=self.embedding_key,
        #     type=ModelType.OpenAIEmbedding,
        #     api_base="https://open.bigmodel.cn/api/paas/v4",
        #     model="embedding-3",
        #     deployment_name="embedding-3",
        #     auth_type="api_key",
        #     max_retries=20,
        #     tokens_per_minute=120000000,
        #     requests_per_minute=100,
        #     encoding_model="cl100k_base",
        # )

        self.embedding_config = LanguageModelConfig(
                        type=ModelType.AzureOpenAIChat,
            api_base="https://tcamp.openai.azure.com/",
            api_version="2023-05-15",
            auth_type="api_key",
            api_key=self.api_key,
            model="text-embedding-ada-002",
            deployment_name="text-embedding-ada-002",
            model_supports_json=True,
            concurrent_requests=25,
            async_mode="threaded",
            retry_strategy="native",
            max_retries=20,
            tokens_per_minute=120000000,
            requests_per_minute=100,
            encoding_model="cl100k_base",
        )
        
        self.token_encoder = tiktoken.get_encoding("cl100k_base")
        
    def _init_data(self):
        """åˆå§‹åŒ–æ•°æ®"""
        COMMUNITY_LEVEL = 2
        
        # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.input_dir):
            raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
        
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ä¹¦æœ¬æ•°æ®: {self.input_dir}")
        
        # è¯»å–æ•°æ®æ–‡ä»¶
        self.community_df = pd.read_parquet(f"{self.input_dir}/communities.parquet")
        self.entity_df = pd.read_parquet(f"{self.input_dir}/entities.parquet")
        self.report_df = pd.read_parquet(f"{self.input_dir}/community_reports.parquet")
        self.relationship_df = pd.read_parquet(f"{self.input_dir}/relationships.parquet")
        self.text_unit_df = pd.read_parquet(f"{self.input_dir}/text_units.parquet")
        
        # åˆå§‹åŒ–GraphRAGæ•°æ®ç»“æ„
        self.communities = read_indexer_communities(self.community_df, self.report_df)
        self.reports = read_indexer_reports(self.report_df, self.community_df, COMMUNITY_LEVEL)
        self.entities = read_indexer_entities(self.entity_df, self.community_df, COMMUNITY_LEVEL)
        self.relationships = read_indexer_relationships(self.relationship_df)
        self.text_units = read_indexer_text_units(self.text_unit_df)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.description_embedding_store = LanceDBVectorStore(
            collection_name="default-entity-description",
        )
        self.description_embedding_store.connect(db_uri=f"{self.input_dir}/lancedb")
        
        print(f"âœ… ä¹¦æœ¬æ•°æ®åŠ è½½å®Œæˆ: {self.input_dir}")
        
    def _init_engines(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager()
        self.chat_model = self.model_manager.get_or_create_chat_model(
            name="rag_engine",
            model_type=ModelType.AzureOpenAIChat,
            config=self.chat_config,
        )
        self.text_embedder = self.model_manager.get_or_create_embedding_model(
            name="rag_engine_embedding",
            model_type=ModelType.OpenAIEmbedding,
            config=self.embedding_config,
        )
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„å»ºå™¨
        self._init_global_context()
        self._init_local_context()
        
    def _init_global_context(self):
        """åˆå§‹åŒ–å…¨å±€æœç´¢ä¸Šä¸‹æ–‡"""
        self.global_context_builder = GlobalCommunityContext(
            community_reports=self.reports,
            communities=self.communities,
            entities=self.entities,
            token_encoder=self.token_encoder,
        )
        
        # å¤§å¹…å‡å°‘max_tokensä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™ï¼Œä¼˜åŒ–APIè°ƒç”¨æ•ˆç‡
        self.global_context_params = {
            "use_community_summary": False,
            "shuffle_data": True,
            "include_community_rank": True,
            "min_community_rank": 0,
            "community_rank_name": "rank",
            "include_community_weight": True,
            "community_weight_name": "occurrence weight",
            "normalize_community_weight": True,
            "max_tokens": 6000,  # ä»10000å‡å°‘åˆ°6000ä»¥å‡å°‘æ£€ç´¢å†…å®¹
            "context_name": "Reports",
        }
        
        self.map_llm_params = {
            "max_tokens": 1200,  # ä»800å¢åŠ åˆ°1200
            "temperature": 0.0,
            # ç§»é™¤ "response_format": {"type": "json_object"},
        }
        
        self.reduce_llm_params = {
            "max_tokens": 1500,  # ä»2500å‡å°‘åˆ°1500
            "temperature": 0.0,
        }
        
        self.global_search_engine = GlobalSearch(
            model=self.chat_model,
            context_builder=self.global_context_builder,
            token_encoder=self.token_encoder,
            max_data_tokens=6000,  # ä»10000å‡å°‘åˆ°6000ä»¥å‡å°‘æ£€ç´¢å†…å®¹
            map_llm_params=self.map_llm_params,
            reduce_llm_params=self.reduce_llm_params,
            allow_general_knowledge=False,
            json_mode=False,  # å…³é—­JSONæ¨¡å¼ï¼Œé¿å…JSONè§£æé”™è¯¯
            context_builder_params=self.global_context_params,
            concurrent_coroutines=128,
            response_type="multiple paragraphs",
        )
        
    def _init_local_context(self):
        """åˆå§‹åŒ–å±€éƒ¨æœç´¢ä¸Šä¸‹æ–‡"""
        self.local_context_builder = LocalSearchMixedContext(
            community_reports=self.reports,
            text_units=self.text_units,
            entities=self.entities,
            relationships=self.relationships,
            covariates=None,
            entity_text_embeddings=self.description_embedding_store,
            embedding_vectorstore_key=EntityVectorStoreKey.ID,
            text_embedder=self.text_embedder,
            token_encoder=self.token_encoder,
        )

        # å¤§å¹…å‡å°‘max_tokensä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™ï¼Œä¼˜åŒ–APIè°ƒç”¨æ•ˆç‡
        self.local_context_params = {
            "text_unit_prop": 0.4,  # ä»0.5å‡å°‘åˆ°0.4
            "community_prop": 0.05,  # ä»0.1å‡å°‘åˆ°0.05
            "conversation_history_max_turns": 2,  # ä»3å‡å°‘åˆ°2
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 3,  # ä»5å‡å°‘åˆ°3
            "top_k_relationships": 3,  # ä»5å‡å°‘åˆ°3
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 6000,  # ä»10000å‡å°‘åˆ°6000ä»¥å‡å°‘æ£€ç´¢å†…å®¹
        }
        
        self.local_model_params = {
            "max_tokens": 1500,  # ä»2500å‡å°‘åˆ°1500
            "temperature": 0.0,
        }
        
        self.local_search_engine = LocalSearch(
            model=self.chat_model,
            context_builder=self.local_context_builder,
            token_encoder=self.token_encoder,
            model_params=self.local_model_params,
            context_builder_params=self.local_context_params,
            response_type="multiple paragraphs",
        )
    
    def _truncate_text(self, text: str, max_tokens: int = 2000) -> str:
        """æˆªæ–­æ–‡æœ¬ä»¥é¿å…tokenè¶…é™ - å·²å¼ƒç”¨ï¼Œä¿ç•™å‘åå…¼å®¹"""
        if not text:
            return text
        
        tokens = self.token_encoder.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        # æˆªæ–­åˆ°æŒ‡å®štokenæ•°
        truncated_tokens = tokens[:max_tokens]
        return self.token_encoder.decode(truncated_tokens)
    
    def _chunk_text(self, text: str, max_tokens_per_chunk: int = 15000, overlap_tokens: int = 500) -> List[Dict[str, Any]]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å—ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ï¼ˆå¤§å—ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
        
        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            max_tokens_per_chunk: æ¯ä¸ªåˆ†å—çš„æœ€å¤§tokenæ•°ï¼ˆå¤§å¹…å¢åŠ åˆ°15000ä»¥å‡å°‘åˆ†å—æ•°é‡ï¼‰
            overlap_tokens: åˆ†å—ä¹‹é—´çš„é‡å tokenæ•°ï¼ˆå¢åŠ åˆ°500ï¼‰
            
        Returns:
            åˆ†å—åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«æ–‡æœ¬ã€ä½ç½®ä¿¡æ¯ç­‰
        """
        if not text:
            return []
        
        tokens = self.token_encoder.encode(text)
        total_tokens = len(tokens)
        
        # å¦‚æœæ–‡æœ¬ä¸å¤ªé•¿ï¼Œç›´æ¥è¿”å›å•ä¸ªåˆ†å—
        if total_tokens <= max_tokens_per_chunk:
            return [{
                "chunk_id": 0,
                "text": text,
                "start_token": 0,
                "end_token": total_tokens,
                "total_tokens": total_tokens,
                "chunk_tokens": total_tokens,
                "is_complete": True
            }]
        
        chunks = []
        chunk_id = 0
        start = 0
        
        # å¤§å¹…å‡å°‘æœ€å¤§åˆ†å—æ•°é‡ä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°
        max_chunks = 10  # ä»8å‡å°‘åˆ°4ï¼Œæœ€å¤š4ä¸ªAPIè°ƒç”¨
        min_chunk_size = total_tokens // max_chunks if total_tokens > max_tokens_per_chunk * max_chunks else max_tokens_per_chunk
        
        while start < total_tokens and len(chunks) < max_chunks:
            if len(chunks) == max_chunks - 1:
                # æœ€åä¸€ä¸ªåˆ†å—åŒ…å«æ‰€æœ‰å‰©ä½™å†…å®¹
                end = total_tokens
            else:
                end = min(start + max_tokens_per_chunk, total_tokens)
                # ç¡®ä¿åˆ†å—ä¸ä¼šå¤ªå°
                if total_tokens - end < min_chunk_size and end < total_tokens:
                    end = total_tokens
            
            # æå–å½“å‰åˆ†å—çš„token
            chunk_tokens = tokens[start:end]
            chunk_text = self.token_encoder.decode(chunk_tokens)
            
            chunks.append({
                "chunk_id": chunk_id,
                "text": chunk_text,
                "start_token": start,
                "end_token": end,
                "total_tokens": total_tokens,
                "chunk_tokens": len(chunk_tokens),
                "is_complete": (len(chunks) == 1 and end >= total_tokens)
            })
            
            chunk_id += 1
            
            # ä¸‹ä¸€ä¸ªåˆ†å—çš„èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            if end >= total_tokens:
                break
            start = end - overlap_tokens
        
        print(f"ğŸ“Š [å¤§å—ä¼˜åŒ–] åŸå§‹ {total_tokens} tokens åˆ†ä¸º {len(chunks)} ä¸ªå¤§åˆ†å—ï¼ˆæ¯å—æœ€å¤š {max_tokens_per_chunk} tokensï¼Œé™åˆ¶æœ€å¤š {max_chunks} ä¸ªåˆ†å—ä»¥å‡å°‘APIè°ƒç”¨ï¼‰")
        return chunks
    
    async def global_search_retrieve(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œè¿”å›å®Œæ•´å¬å›å†…å®¹ç”¨äºåˆ†å—å¤„ç†"""
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨æ£€ç´¢å…¨å±€ä¿¡æ¯: {query}")
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼‰
            # build_context æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦ await
            context = await self.global_context_builder.build_context(
                query=query,
                **self.global_context_params
            )
            
            # å¤„ç†contextå¯¹è±¡ï¼Œè·å–åŸå§‹æ–‡æœ¬
            if hasattr(context, 'context_text'):
                context_text = context.context_text
            elif hasattr(context, 'text'):
                context_text = context.text
            elif hasattr(context, 'content'):
                context_text = context.content
            elif hasattr(context, 'response'):
                context_text = context.response
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                context_text = str(context)
            
            # è®¡ç®—åŸå§‹é•¿åº¦
            original_tokens = len(self.token_encoder.encode(context_text))
            
            # å¤§åˆ†å—å¤„ç†ï¼ˆå¤§å¹…å‡å°‘åˆ†å—æ•°é‡ä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
            chunks = self._chunk_text(context_text, max_tokens_per_chunk=20000, overlap_tokens=1500)
            
            print(f"âœ… [RAGæ£€ç´¢] å…¨å±€æ£€ç´¢å®Œæˆï¼ŒåŸå§‹å†…å®¹ {original_tokens} tokensï¼Œåˆ†ä¸º {len(chunks)} ä¸ªå¤§åˆ†å—")
            
            return {
                "method": "global_retrieve",
                "query": query,
                "retrieved_context": {
                    "full_text": context_text,
                    "original_length": len(context_text),
                    "original_tokens": original_tokens,
                    "chunks": chunks,
                    "total_chunks": len(chunks),
                    "context_summary": f"GraphRAGå…¨å±€æœç´¢æ£€ç´¢åˆ°çš„ç¤¾åŒºæŠ¥å‘Šå’Œå®ä½“ä¿¡æ¯ï¼ˆå®Œæ•´å†…å®¹ï¼Œ{len(chunks)}ä¸ªå¤§åˆ†å—ï¼Œå‡å°‘APIè°ƒç”¨ï¼‰"
                },
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨parallel_chunk_analysis_toolè¿›è¡Œåˆ†æ"
            }
        except Exception as e:
            print(f"âŒ [RAGæ£€ç´¢] å…¨å±€æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "global_retrieve", 
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def global_search_generate(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        try:
            print(f" [LLMç”Ÿæˆ] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå…¨å±€æœç´¢å›ç­”: {query}")
            
            # ä¸ç›´æ¥è°ƒç”¨GraphRAGçš„searchæ–¹æ³•ï¼Œè€Œæ˜¯è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            # è®©agenté€šè¿‡ç‹¬ç«‹LLMå·¥å…·æ¥å¤„ç†ç”Ÿæˆ
            context_text = retrieved_context.get('context_text', '') if isinstance(retrieved_context, dict) else str(retrieved_context)
            
            print(f"[LLMç”Ÿæˆ] å‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context_text)} å­—ç¬¦")
            
            return {
                "method": "global_generate",
                "query": query,
                "retrieved_context": retrieved_context,
                "context_ready": True,
                "success": True,
                "note": "è¯·ä½¿ç”¨llm_generate_toolæ¥å¤„ç†ç”Ÿæˆ"
            }
        except Exception as e:
            print(f" [LLMç”Ÿæˆ] å…¨å±€æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "method": "global_generate",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def global_search_full(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        try:
            print(f"ğŸš€ [å®Œæ•´æµç¨‹] å¼€å§‹å…¨å±€æœç´¢: {query}")
            
            # 1. å…ˆæ£€ç´¢
            retrieve_result = await self.global_search_retrieve(query)
            if not retrieve_result['success']:
                return retrieve_result
            
            # 2. è¿”å›æ£€ç´¢ç»“æœï¼Œè®©agentå†³å®šå¦‚ä½•å¤„ç†
            print(f"âœ… [å®Œæ•´æµç¨‹] å…¨å±€æ£€ç´¢å®Œæˆï¼Œç­‰å¾…agentå¤„ç†")
            print(f"ğŸ¤– [Agentè°ƒç”¨] æ­£åœ¨ä½¿ç”¨Agentè°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            
            return {
                "method": "global_full",
                "query": query,
                "retrieved_context": retrieve_result['retrieved_context'],
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨llm_generate_toolè¿›è¡Œç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [å®Œæ•´æµç¨‹] å…¨å±€æœç´¢å¤±è´¥: {e}")
            return {
                "method": "global_full",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_retrieve(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œè¿”å›å®Œæ•´å¬å›å†…å®¹ç”¨äºåˆ†å—å¤„ç†"""
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨æ£€ç´¢å±€éƒ¨ä¿¡æ¯: {query}")
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼‰
            # LocalSearchMixedContext.build_context æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¸éœ€è¦ await
            context = self.local_context_builder.build_context(
                query=query,
                **self.local_context_params
            )
            
            # å¤„ç†contextå¯¹è±¡ï¼Œè·å–åŸå§‹æ–‡æœ¬
            if hasattr(context, 'context_text'):
                context_text = context.context_text
            elif hasattr(context, 'text'):
                context_text = context.text
            elif hasattr(context, 'content'):
                context_text = context.content
            elif hasattr(context, 'response'):
                context_text = context.response
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                context_text = str(context)
            
            # è®¡ç®—åŸå§‹é•¿åº¦
            original_tokens = len(self.token_encoder.encode(context_text))
            
            # å¤§åˆ†å—å¤„ç†ï¼ˆå¤§å¹…å‡å°‘åˆ†å—æ•°é‡ä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
            chunks = self._chunk_text(context_text, max_tokens_per_chunk=20000, overlap_tokens=1500)
            
            print(f"âœ… [RAGæ£€ç´¢] å±€éƒ¨æ£€ç´¢å®Œæˆï¼ŒåŸå§‹å†…å®¹ {original_tokens} tokensï¼Œåˆ†ä¸º {len(chunks)} ä¸ªå¤§åˆ†å—")
            
            return {
                "method": "local_retrieve",
                "query": query,
                "retrieved_context": {
                    "full_text": context_text,
                    "original_length": len(context_text),
                    "original_tokens": original_tokens,
                    "chunks": chunks,
                    "total_chunks": len(chunks),
                    "context_summary": f"GraphRAGå±€éƒ¨æœç´¢æ£€ç´¢åˆ°çš„æ–‡æœ¬å•å…ƒã€å®ä½“å’Œå…³ç³»ä¿¡æ¯ï¼ˆå®Œæ•´å†…å®¹ï¼Œ{len(chunks)}ä¸ªå¤§åˆ†å—ï¼Œå‡å°‘APIè°ƒç”¨ï¼‰"
                },
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨parallel_chunk_analysis_toolè¿›è¡Œåˆ†æ"
            }
        except Exception as e:
            print(f"âŒ [RAGæ£€ç´¢] å±€éƒ¨æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "local_retrieve",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_generate(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        try:
            print(f"ğŸ¤– [LLMç”Ÿæˆ] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå±€éƒ¨æœç´¢å›ç­”: {query}")
            
            # ä¸ç›´æ¥è°ƒç”¨GraphRAGçš„searchæ–¹æ³•ï¼Œè€Œæ˜¯è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            # è®©agenté€šè¿‡ç‹¬ç«‹LLMå·¥å…·æ¥å¤„ç†ç”Ÿæˆ
            context_text = retrieved_context.get('context_text', '') if isinstance(retrieved_context, dict) else str(retrieved_context)
            
            print(f"âœ… [LLMç”Ÿæˆ] å‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context_text)} å­—ç¬¦")
            
            return {
                "method": "local_generate",
                "query": query,
                "retrieved_context": retrieved_context,
                "context_ready": True,
                "success": True,
                "note": "è¯·ä½¿ç”¨llm_generate_toolæ¥å¤„ç†ç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [LLMç”Ÿæˆ] å±€éƒ¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "method": "local_generate",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_full(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        try:
            print(f"ğŸš€ [å®Œæ•´æµç¨‹] å¼€å§‹å±€éƒ¨æœç´¢: {query}")
            
            # 1. å…ˆæ£€ç´¢
            retrieve_result = await self.local_search_retrieve(query)
            if not retrieve_result['success']:
                return retrieve_result
            
            # 2. è¿”å›æ£€ç´¢ç»“æœï¼Œè®©agentå†³å®šå¦‚ä½•å¤„ç†
            print(f"âœ… [å®Œæ•´æµç¨‹] å±€éƒ¨æ£€ç´¢å®Œæˆï¼Œç­‰å¾…agentå¤„ç†")
            print(f"ğŸ¤– [Agentè°ƒç”¨] æ­£åœ¨ä½¿ç”¨Agentè°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            
            return {
                "method": "local_full",
                "query": query,
                "retrieved_context": retrieve_result['retrieved_context'],
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨llm_generate_toolè¿›è¡Œç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [å®Œæ•´æµç¨‹] å±€éƒ¨æœç´¢å¤±è´¥: {e}")
            return {
                "method": "local_full",
                "query": query,
                "error": str(e),
                "success": False
            }


# å…¨å±€å®ä¾‹ - é»˜è®¤ä½¿ç”¨ ./rag/output
rag_engine = RAGEngine()

# å¤šä¹¦æœ¬ç®¡ç†å™¨
class MultiBookManager:
    """å¤šä¹¦æœ¬ç®¡ç†å™¨ï¼Œç”¨äºç®¡ç†å¤šä¸ªRAGå¼•æ“å®ä¾‹"""
    
    def __init__(self):
        self.engines = {}  # ç¼“å­˜å¼•æ“å®ä¾‹
        self.current_book = None
    
    def add_book(self, book_name: str, book_folder: str):
        """æ·»åŠ æ–°ä¹¦æœ¬"""
        if not os.path.exists(book_folder):
            raise ValueError(f"ä¹¦æœ¬æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {book_folder}")
        
        # åˆ›å»ºæ–°çš„RAGå¼•æ“å®ä¾‹
        self.engines[book_name] = RAGEngine(input_dir=book_folder)
        print(f"âœ… æ·»åŠ ä¹¦æœ¬: {book_name} -> {book_folder}")
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æœ¬ä¹¦ï¼Œè‡ªåŠ¨è®¾ç½®ä¸ºå½“å‰ä¹¦æœ¬
        if self.current_book is None:
            self.current_book = book_name
    
    def switch_book(self, book_name: str):
        """åˆ‡æ¢åˆ°æŒ‡å®šä¹¦æœ¬"""
        if book_name not in self.engines:
            raise ValueError(f"ä¹¦æœ¬ä¸å­˜åœ¨: {book_name}")
        
        if self.current_book == book_name:
            print(f"â„¹ï¸ å·²ç»åœ¨ä¹¦æœ¬: {book_name}")
            return
        
        print(f"ğŸ”„ åˆ‡æ¢åˆ°ä¹¦æœ¬: {book_name}")
        self.current_book = book_name
    
    def get_current_engine(self) -> RAGEngine:
        """è·å–å½“å‰ä¹¦æœ¬çš„å¼•æ“"""
        if self.current_book is None:
            raise ValueError("æ²¡æœ‰é€‰æ‹©ä»»ä½•ä¹¦æœ¬ï¼Œè¯·å…ˆä½¿ç”¨ add_book() æ·»åŠ ä¹¦æœ¬")
        
        return self.engines[self.current_book]
    
    def list_books(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬"""
        return list(self.engines.keys())
    
    def get_current_book(self) -> Optional[str]:
        """è·å–å½“å‰ä¹¦æœ¬åç§°"""
        return self.current_book
    
    def remove_book(self, book_name: str):
        """ç§»é™¤ä¹¦æœ¬"""
        if book_name in self.engines:
            del self.engines[book_name]
            print(f"âœ… ç§»é™¤ä¹¦æœ¬: {book_name}")
            
            # å¦‚æœç§»é™¤çš„æ˜¯å½“å‰ä¹¦æœ¬ï¼Œåˆ‡æ¢åˆ°å…¶ä»–ä¹¦æœ¬
            if self.current_book == book_name:
                if self.engines:
                    first_book = list(self.engines.keys())[0]
                    self.current_book = first_book
                else:
                    self.current_book = None


# å…¨å±€å¤šä¹¦æœ¬ç®¡ç†å™¨å®ä¾‹
multi_book_manager = MultiBookManager()