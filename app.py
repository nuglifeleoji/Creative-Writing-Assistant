from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import asyncio
import json
import os
import queue
import threading
import time
import uuid
from langchain_agent import GraphAnalysisAgent
from polish_agent import create_polish_agent
from cross_book_agent import create_cross_langchain_agent
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any, Dict, List
from langchain_core.agents import AgentAction, AgentFinish

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡
graph_agent = None
graph_agent_instance = None  # ä¿å­˜ GraphAnalysisAgent å®ä¾‹çš„å¼•ç”¨
polish_agent = None          # ç‹¬ç«‹æ¶¦è‰² Agent
cross_agent = None           # è·¨ä¹¦åˆ›ä½œ Agent

# çº¿ç¨‹å®‰å…¨çš„å›è°ƒæ¶ˆæ¯é˜Ÿåˆ—
callback_queue = queue.Queue()
class StreamingCallbackHandler(BaseCallbackHandler):
    """åªæš´éœ²è¿‡ç¨‹ï¼Œä¸æ³„éœ²æ¨ç†ç»†èŠ‚ï¼ˆä¸è¾“å‡º action.logï¼‰ã€‚"""
    def __init__(self, yield_func):
        self.yield_func = yield_func
        self.current_tool = None
        self._tool_start_ts = {}
        self._run_id = str(uuid.uuid4())
        self._sent_events = set()  # ç”¨äºå»é‡

    def _send(self, etype: str, payload: Dict[str, Any]):
        # å¯¹äº token äº‹ä»¶ï¼šä¸å»é‡ï¼Œç›´æ¥æ¨é€ï¼Œå®ç°çœŸæµå¼è¾“å‡º
        if etype == "llm_token":
            sse_message = f"event: {etype}\ndata: {json.dumps(payload, ensure_ascii=False)}\n\n"
            self.yield_func(sse_message)
            return
            
        # å¯¹äºå…¶ä»–äº‹ä»¶ï¼Œä½¿ç”¨äº‹ä»¶ç±»å‹+å…³é”®å†…å®¹è¿›è¡Œå»é‡
        if etype in ["run_start", "run_end"]:
            event_key = f"{etype}"  # è¿™äº›äº‹ä»¶æ¯æ¬¡åªåº”è¯¥å‘é€ä¸€æ¬¡
        else:
            event_key = f"{etype}:{str(payload)[:50]}"
        
        if event_key in self._sent_events:
            print(f"âš ï¸ è·³è¿‡é‡å¤äº‹ä»¶: {etype}")
            return
        
        self._sent_events.add(event_key)
        payload = {"runId": self._run_id, **payload}
        # æŒ‰ SSE æ ‡å‡†å†™æ³•è¾“å‡ºï¼ševent + data + ç©ºè¡Œ
        sse_message = f"event: {etype}\ndata: {json.dumps(payload, ensure_ascii=False)}\n\n"
        print(f"ğŸ”„ å›è°ƒå‘é€SSE: {etype} -> {str(payload)[:100]}...")
        self.yield_func(sse_message)

    # ä»»åŠ¡èµ·æ­¢
    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        self._send("run_start", {
            "chain": serialized.get("name", "chain"),
            "inputsPreview": str(inputs)[:300]
        })

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        self._send("run_end", {
            "outputsPreview": str(outputs)[:300]
        })

    # Agent å†³ç­–ï¼ˆä¸è¾“å‡ºæ€ç»´æ–‡æœ¬ï¼‰
    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        self._send("plan", {
            "nextTool": action.tool,
            "argsPreview": str(action.tool_input)[:300]
        })

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        self._send("plan_done", {
            "finalReturnPreview": str(finish.return_values)[:300]
        })

    # å·¥å…·è°ƒç”¨
    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
        tool_name = serialized.get("name", "unknown_tool")
        self.current_tool = tool_name
        self._tool_start_ts[tool_name] = time.time()
        self._send("tool_start", {
            "tool": tool_name,
            "inputPreview": input_str[:500]
        })

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        tool = self.current_tool or 'unknown_tool'
        latency = None
        if tool in self._tool_start_ts:
            latency = round((time.time() - self._tool_start_ts.pop(tool)), 3)
        self._send("tool_end", {
            "tool": tool,
            "latencySec": latency,
            "outputPreview": output[:800],
            "truncated": len(output) > 800
        })

    # LLM token æµ + ç”¨é‡
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        self._send("llm_start", {
            "model": serialized.get("name", "llm"),
            "promptPreview": (prompts[0][:300] if prompts else "")
        })

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self._send("llm_token", {"token": token})

    def on_llm_end(self, response, **kwargs: Any) -> None:
        usage = {}
        try:
            usage = getattr(response, "usage_metadata", None) or {}
            if not usage and hasattr(response, "generations"):
                gi = response.generations[0][0].generation_info or {}
                usage = gi.get("usage", gi)
        except Exception:
            usage = {}
        self._send("llm_end", {"usage": usage})

def initialize_agent():
    """åˆå§‹åŒ–LangChainä»£ç†"""
    global graph_agent, graph_agent_instance
    try:
        # åˆ›å»º GraphAnalysisAgent å®ä¾‹
        graph_agent_instance = GraphAnalysisAgent(use_multi_book=True)
        
        # è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬
        print("ğŸ“š æ­£åœ¨è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬...")
        
        # å®šä¹‰è¦åŠ è½½çš„ä¹¦æœ¬åˆ—è¡¨
        books_to_load = [
            ("å¹³å‡¡çš„ä¸–ç•Œ", "./book4/output"),
            ("ä¸‰ä½“", "./book5/output"), 
            ("ä¸‰ä½“2", "./book6/output"),
            ("è¶…æ–°æ˜Ÿçºªå…ƒ", "./cxx/output"),
            ("ç™½å¤œè¡Œ", "./rag_book2/ragtest/output"),
            ("å¼—å…°è‚¯æ–¯å¦", "./tencent/output"),
            ("æ²™ä¸˜", "./rag/output"),
            ("å«Œç–‘äººxçš„çŒ®èº«", "./book7/output"),
            ("æ–—ç½—å¤§é™†4", "./book8/output"),
            ("ä¸‰å›½æ¼”ä¹‰","./sanguo/output")
        ]
        
        loaded_books = []
        for book_name, book_path in books_to_load:
            try:
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                if os.path.exists(book_path):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
                    required_files = ["communities.parquet", "entities.parquet", "community_reports.parquet", "relationships.parquet", "text_units.parquet"]
                    missing_files = [f for f in required_files if not os.path.exists(os.path.join(book_path, f))]
                    
                    if not missing_files:
                        graph_agent_instance.add_book(book_name, book_path)
                        loaded_books.append(book_name)
                        print(f"âœ… æˆåŠŸåŠ è½½ä¹¦æœ¬: {book_name} -> {book_path}")
                    else:
                        print(f"âš ï¸ è·³è¿‡ {book_name}: ç¼ºå°‘å¿…è¦æ–‡ä»¶ {missing_files}")
                else:
                    print(f"âš ï¸ è·³è¿‡ {book_name}: è·¯å¾„ä¸å­˜åœ¨ {book_path}")
            except Exception as e:
                print(f"âŒ åŠ è½½ {book_name} å¤±è´¥: {e}")
        
        print(f"âœ… æ€»å…±åŠ è½½äº† {len(loaded_books)} æœ¬ä¹¦: {', '.join(loaded_books)}")
        
        # ä¸è‡ªåŠ¨é€‰æ‹©ä¹¦æœ¬ï¼Œè®©ç”¨æˆ·ä¸»åŠ¨é€‰æ‹©
        if loaded_books:
            print(f"ğŸ“š å·²åŠ è½½ä¹¦æœ¬ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©: {', '.join(loaded_books)}")
        else:
            print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•ä¹¦æœ¬ï¼Œè¯·æ‰‹åŠ¨æ·»åŠ ä¹¦æœ¬")
        
        # ä½¿ç”¨è¿™ä¸ªå®ä¾‹åˆ›å»º LangChain Agent
        from langchain_agent import create_graphrag_agent
        graph_agent = create_graphrag_agent(graph_agent_instance)
        graph_agent_instance = graph_agent_instance  # ä¿å­˜å¼•ç”¨

        # åˆå§‹åŒ–ç‹¬ç«‹æ¶¦è‰²Agent
        global polish_agent
        polish_agent = create_polish_agent()
        # åˆå§‹åŒ–è·¨ä¹¦Agent
        global cross_agent
        cross_agent = create_cross_langchain_agent()
        
        print("âœ… ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ ä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

try:
    if os.environ.get("INIT_ON_IMPORT", "1") == "1" and graph_agent is None:
        print("âš™ï¸ WSGI å¯¼å…¥é˜¶æ®µï¼šå°è¯•è‡ªåŠ¨åˆå§‹åŒ– Agent...")
        initialize_agent()
except Exception as _import_init_err:
    print(f"âš ï¸ å¯¼å…¥æœŸåˆå§‹åŒ–å¼‚å¸¸ï¼ˆå¯å¿½ç•¥ï¼Œå°†åœ¨é¦–ä¸ªè¯·æ±‚æ—¶å†è¯•ï¼‰ï¼š{_import_init_err}")

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return send_from_directory('frontend', 'index.html')

@app.route('/test')
def test_page():
    """æµ‹è¯•é¡µé¢"""
    return send_from_directory('frontend', 'test.html')

@app.route('/<path:filename>')
def serve_static(filename):
    """æä¾›é™æ€æ–‡ä»¶"""
    return send_from_directory('frontend', filename)

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©API - æ”¯æŒæµå¼å“åº”"""
    try:
        data = request.get_json()
        message = data.get('message', '')
        current_book = data.get('currentBook')
        history = data.get('history', [])
        stream = data.get('stream', False)  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
        
        if not graph_agent:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        # æ„å»ºå†å²å¯¹è¯
        chat_history = []
        for msg in history[-10:]:  # åªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
            if msg['type'] == 'user':
                chat_history.append(f"ç”¨æˆ·: {msg['content']}")
            elif msg['type'] == 'assistant':
                chat_history.append(f"åŠ©æ‰‹: {msg['content']}")
        
        # è§£æå½“å‰ä¹¦æœ¬ï¼ˆä¼˜å…ˆä»¥åç«¯å®é™…é€‰æ‹©ä¸ºå‡†ï¼‰
        try:
            backend_current_book = graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else None
        except Exception:
            backend_current_book = None
        effective_current_book = backend_current_book or current_book

        # å°†å½“å‰ä¹¦æœ¬æ˜¾å¼æ³¨å…¥åˆ°Agentè¾“å…¥ï¼Œé¿å…â€œè¿™æœ¬ä¹¦â€æ­§ä¹‰
        message_for_agent = message
        if effective_current_book:
            message_for_agent = (
                f"ã€å½“å‰ä¹¦æœ¬ã€‘{effective_current_book}\n"
                f"ã€ä»»åŠ¡ã€‘è¯·åŸºäºå½“å‰ä¹¦æœ¬å›ç­”ä¸‹è¿°é—®é¢˜ï¼›è‹¥ç”¨æˆ·æåˆ°â€˜è¿™æœ¬ä¹¦â€™ï¼Œé»˜è®¤æŒ‡å½“å‰ä¹¦æœ¬ã€‚\n"
                f"ã€ç”¨æˆ·é—®é¢˜ã€‘{message}"
            )

        if stream:
            # æµå¼å“åº”ï¼ˆSSEï¼‰
            def generate():
                try:
                    # åˆå§‹çŠ¶æ€
                    yield "event: status\n"
                    yield f"data: {json.dumps({'prompt':'æ­£åœ¨åˆ†æé—®é¢˜...'}, ensure_ascii=False)}\n\n"

                    # åˆ›å»ºæ–°çš„é˜Ÿåˆ—å®ä¾‹ï¼Œç¡®ä¿å®Œå…¨æ¸…ç©º
                    callback_queue = queue.Queue()
                    
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    def callback_yield(sse_chunk: str):
                        # è¿™é‡Œ sse_chunk å·²ç»æ˜¯ "event:...\n data:...\n\n" çš„å®Œæ•´ç‰‡æ®µ
                        callback_queue.put(sse_chunk)

                    # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºæ–°çš„å›è°ƒå¤„ç†å™¨å®ä¾‹
                    callback_handler = StreamingCallbackHandler(callback_yield)
                    print(f"ğŸ†• åˆ›å»ºæ–°çš„å›è°ƒå¤„ç†å™¨: {callback_handler._run_id}")

                    def run_agent():
                        try:
                            print(f"ğŸ¤– å¼€å§‹æ‰§è¡Œä»£ç†: {message}")
                            resp = loop.run_until_complete(
                                graph_agent.ainvoke(
                                    {"input": message_for_agent, "chat_history": chat_history},
                                    config={"callbacks": [callback_handler]}
                                )
                            )
                            print(f"âœ… ä»£ç†æ‰§è¡Œå®Œæˆ: {type(resp)}")
                            callback_queue.put(("response", resp))
                        except Exception as e:
                            print(f"âŒ ä»£ç†æ‰§è¡Œå¤±è´¥: {e}")
                            callback_queue.put(("error", str(e)))

                    agent_thread = threading.Thread(target=run_agent, daemon=True)
                    agent_thread.start()

                    response = None
                    message_count = 0
                    while True:
                        try:
                            item = callback_queue.get(timeout=0.1)
                            message_count += 1
                            print(f"ğŸ“¨ æ”¶åˆ°é˜Ÿåˆ—æ¶ˆæ¯ #{message_count}: {type(item)}")
                            
                            if isinstance(item, tuple):
                                if item[0] == "response":
                                    print(f"ğŸ¯ æ”¶åˆ°æœ€ç»ˆå“åº”")
                                    response = item[1]
                                    break
                                elif item[0] == "error":
                                    print(f"ğŸ’¥ æ”¶åˆ°é”™è¯¯: {item[1]}")
                                    raise Exception(item[1])
                            else:
                                # ç”±å›è°ƒå™¨æ„é€ çš„ SSE äº‹ä»¶ï¼Œç›´æ¥é€ä¼ 
                                print(f"ğŸ“¡ è½¬å‘SSEæ¶ˆæ¯: {item[:100]}...")
                                yield item
                        except queue.Empty:
                            if not agent_thread.is_alive():
                                print(f"ğŸ”š ä»£ç†çº¿ç¨‹å·²ç»“æŸï¼Œé€€å‡ºå¾ªç¯")
                                break
                            continue

                    agent_thread.join()

                    # ç»“æŸå‘é€ final/done
                    if isinstance(response, dict) and 'output' in response:
                        output = response['output']
                    else:
                        output = str(response)

                    # å°è¯•è§„çº¦ä¸­é—´æ­¥éª¤ï¼ˆå·¥å…·å + å‚æ•°/è§‚æµ‹æ‘˜è¦ï¼‰
                    intermediate = []
                    if isinstance(response, dict):
                        for st in response.get("intermediate_steps", []):
                            try:
                                action, observation = st
                                intermediate.append({
                                    "tool": getattr(action, "tool", "unknown"),
                                    "argsPreview": str(getattr(action, "tool_input", ""))[:300],
                                    "observationPreview": str(observation)[:500]
                                })
                            except Exception:
                                pass

                    final_payload = {
                        'response': output,
                        'currentBook': effective_current_book,
                        'needBookSelection': ("éœ€è¦é€‰æ‹©ä¹¦æœ¬" in output) or ("è¯·å…ˆé€‰æ‹©" in output),
                        'intermediate': intermediate
                    }
                    yield "event: final\n"
                    yield f"data: {json.dumps(final_payload, ensure_ascii=False)}\n\n"
                    yield "event: done\n"
                    yield "data: {}\n\n"

                except Exception as e:
                    yield "event: error\n"
                    yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

            return app.response_class(
                generate(),
                mimetype='text/event-stream',  # <<< å…³é”®
                headers={
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'X-Accel-Buffering': 'no'  # Nginx ä¸‹å…³é—­ç¼“å†²
                }
            )
        else:
            # éæµå¼å“åº”ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                response = loop.run_until_complete(graph_agent.ainvoke({
                    "input": message_for_agent,
                    "chat_history": chat_history
                }))
                steps = []
                if isinstance(response, dict):
                    for st in response.get("intermediate_steps", []):
                        try:
                            action, observation = st
                            steps.append({
                                "tool": getattr(action, "tool", "unknown"),
                                "argsPreview": str(getattr(action, "tool_input", ""))[:300],
                                "observationPreview": str(observation)[:500]
                            })
                        except Exception:
                            pass
                # è§£æå“åº”
                if isinstance(response, dict) and 'output' in response:
                    output = response['output']
                else:
                    output = str(response)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‰æ‹©ä¹¦æœ¬
                if "éœ€è¦é€‰æ‹©ä¹¦æœ¬" in output or "è¯·å…ˆé€‰æ‹©" in output:
                    return jsonify({
                        'response': output,
                        'needBookSelection': True,
                        'currentBook': current_book
                    })
                
                return jsonify({
                    'response': output,
                    'currentBook': effective_current_book,
                    'toolCalls': []
                })
                
            finally:
                loop.close()
            
    except Exception as e:
        print(f"èŠå¤©APIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/polish', methods=['POST'])
def api_polish():
    """æ¶¦è‰²APIï¼šæ ¹æ®è‰ç¨¿ä¸å†å²å¯¹è¯è¿›è¡Œæ¶¦è‰²ï¼Œæ”¯æŒSSEæµå¼è¾“å‡ºã€‚"""
    try:
        data = request.get_json(silent=True) or {}
        draft = data.get('draft', '')
        history = data.get('history', [])  # å¯ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯æ•°ç»„
        user_prompt = data.get('userPrompt', '')
        tone = data.get('tone', 'neutral')
        target_length = data.get('targetLength', 'original')
        stream = data.get('stream', True)

        if not draft:
            return jsonify({'error': 'draftä¸èƒ½ä¸ºç©º'}), 400

        # å½’ä¸€åŒ–å†å²ä¸ºçº¯æ–‡æœ¬
        if isinstance(history, list):
            history_text = []
            for msg in history[-10:]:
                role = msg.get('type') or msg.get('role')
                content = msg.get('content', '')
                if role == 'user':
                    history_text.append(f"ç”¨æˆ·: {content}")
                elif role in ('assistant', 'ai'):
                    history_text.append(f"åŠ©æ‰‹: {content}")
            history_text = "\n".join(history_text)
        else:
            history_text = str(history or '')

        if not stream:
            # éæµå¼ï¼šç›´æ¥è¿”å›æ¶¦è‰²ç»“æœ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(
                    polish_agent.polish_async(
                        draft=draft,
                        chat_history_text=history_text,
                        user_prompt=user_prompt,
                        tone=tone,
                        target_length=target_length,
                    )
                )
                return jsonify({'result': result})
            finally:
                loop.close()

        # æµå¼ï¼šä½¿ç”¨SSEè¿”å›tokenä¸æœ€ç»ˆç»“æœ
        def generate():
            try:
                yield "event: status\n"
                yield f"data: {json.dumps({'status': 'å¼€å§‹æ¶¦è‰²...'}, ensure_ascii=False)}\n\n"

                callback_q = queue.Queue()

                class PolishStreamingHandler(BaseCallbackHandler):
                    def __init__(self, yield_fn):
                        self.yield_fn = yield_fn
                    def on_llm_start(self, serialized, prompts, **kwargs):
                        self.yield_fn(f"event: llm_start\ndata: {json.dumps({'model': serialized.get('name','llm')}, ensure_ascii=False)}\n\n")
                    def on_llm_new_token(self, token: str, **kwargs):
                        # å‘é€tokenï¼Œå®ç°çœŸæµå¼
                        self.yield_fn(f"event: llm_token\ndata: {json.dumps({'token': token}, ensure_ascii=False)}\n\n")
                    def on_llm_end(self, response, **kwargs):
                        # å‘é€ä¸€ä¸ªç©ºå¯¹è±¡ï¼Œè¡¨ç¤ºè¯¥æ®µtokenæµç»“æŸ
                        self.yield_fn("event: llm_end\n")
                        self.yield_fn("data: {}\n\n")

                def q_yield(sse_chunk: str):
                    callback_q.put(sse_chunk)

                handler = PolishStreamingHandler(q_yield)

                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                async def run_polish():
                    from langchain_core.messages import SystemMessage, HumanMessage
                    system_rules = (
                        "ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡å†™ä½œæ¶¦è‰²åŠ©æ‰‹ï¼Œæå‡æ¸…æ™°åº¦ã€ç»“æ„æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸æ”¹å˜äº‹å®ä¸æ ¸å¿ƒå«ä¹‰ã€‚\n"
                        "- ä¿®æ­£è¯­æ³•/ç”¨è¯/æ ‡ç‚¹/æ ¼å¼\n"
                        "- ä¼˜åŒ–é€»è¾‘ä¸æ®µè½è¡”æ¥\n"
                        "- æå‡å¯è¯»æ€§ä¸ä¸“ä¸šæ€§ï¼Œé¿å…å†—ä½™\n"
                        "- ä¿ç•™å…³é”®ä¿¡æ¯ä¸æœ¯è¯­ï¼Œä¸ç¼–é€ å†…å®¹\n"
                        f"- ç›®æ ‡è¯­æ°”: {tone}ï¼›é•¿åº¦ç­–ç•¥: {target_length}"
                    )
                    hist_part = f"\n\n[å¯¹è¯å†å²]\n{history_text}" if history_text else ""
                    requirement_part = f"\n\n[ç”¨æˆ·éœ€æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼æ»¡è¶³ï¼‰]\n{user_prompt}" if user_prompt else ""
                    prompt = (
                        "è¯·å…ˆå¯¹ç…§[ç”¨æˆ·éœ€æ±‚]æ£€æŸ¥è‰ç¨¿æ˜¯å¦å®Œå…¨ç¬¦åˆè¦æ±‚ï¼ˆé¢˜æã€é£æ ¼ã€ç»“æ„ã€é•¿åº¦ã€ç¦å¿Œç­‰ï¼‰ã€‚è‹¥å­˜åœ¨ä¸ç¬¦åˆæˆ–é—æ¼ï¼Œè¯·åœ¨æ¶¦è‰²æ—¶ä¸€å¹¶ä¿®æ­£ï¼›å¦åˆ™åœ¨ä¿æŒå«ä¹‰ä¸å˜çš„å‰æä¸‹ä¼˜åŒ–è¡¨è¾¾ã€‚\n"
                        "ä»…è¾“å‡ºæ¶¦è‰²åçš„æœ€ç»ˆæ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºè§£é‡Šæˆ–æ‰“åˆ†ã€‚\n\n"
                        f"[è‰ç¨¿]\n{draft}{hist_part}{requirement_part}"
                    )
                    messages = [SystemMessage(content=system_rules), HumanMessage(content=prompt)]

                    # ç›´æ¥ä½¿ç”¨ç‹¬ç«‹llmï¼ˆå¸¦å›è°ƒï¼‰è¿›è¡Œæµå¼ç”Ÿæˆ
                    resp = await polish_agent.llm_polish.ainvoke(messages, config={"callbacks": [handler]})
                    return resp.content if hasattr(resp, 'content') else str(resp)

                result_text = loop.run_until_complete(run_polish())

                # è½¬å‘é˜Ÿåˆ—ä¸­çš„SSEç‰‡æ®µ
                while not callback_q.empty():
                    yield callback_q.get()

                # æœ€ç»ˆç»“æœ
                yield "event: final\n"
                yield f"data: {json.dumps({'result': result_text}, ensure_ascii=False)}\n\n"
                yield "event: done\n"
                yield "data: {}\n\n"

            except Exception as e:
                yield "event: error\n"
                yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

        return app.response_class(
            generate(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        print(f"æ¶¦è‰²APIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/books', methods=['GET'])
def list_books():
    """è·å–ä¹¦æœ¬åˆ—è¡¨"""
    try:
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        book_names = graph_agent_instance.list_books() if hasattr(graph_agent_instance, 'list_books') else []
        current_book = graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else None
        
        # è¿”å›æ›´è¯¦ç»†çš„ä¹¦æœ¬ä¿¡æ¯
        books = []
        for name in book_names:
            books.append({
                'name': name,
                'isCurrent': name == current_book
            })
        
        print(f"ğŸ“š è¿”å›ä¹¦æœ¬åˆ—è¡¨: {books}")
        return jsonify(books)
        
    except Exception as e:
        print(f"è·å–ä¹¦æœ¬åˆ—è¡¨é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/switch-book', methods=['POST'])
def switch_book():
    """åˆ‡æ¢ä¹¦æœ¬"""
    try:
        data = request.get_json()
        book_name = data.get('bookName')
        
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        if hasattr(graph_agent_instance, 'switch_book'):
            graph_agent_instance.switch_book(book_name)
            # é‡ç½®Agentå¯¹è¯è®°å¿†å¹¶é‡å»ºæ‰§è¡Œå™¨ï¼Œé¿å…è·¨ä¹¦æœ¬ä¸²æ‰°
            try:
                from langchain_agent import create_graphrag_agent, memory
                memory.clear()
                global graph_agent
                graph_agent = create_graphrag_agent(graph_agent_instance)
                print(f"ğŸ”„ å·²é‡å»ºAgentæ‰§è¡Œå™¨å¹¶æ¸…ç©ºè®°å¿†ï¼Œå½“å‰ä¹¦æœ¬: {book_name}")
            except Exception as e:
                print(f"âš ï¸ é‡å»ºAgentæˆ–æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")
            return jsonify({'success': True, 'currentBook': book_name})
        else:
            return jsonify({'error': 'åˆ‡æ¢ä¹¦æœ¬åŠŸèƒ½ä¸å¯ç”¨'}), 500
            
    except Exception as e:
        print(f"åˆ‡æ¢ä¹¦æœ¬é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/add-book', methods=['POST'])
def add_book():
    """æ·»åŠ ä¹¦æœ¬"""
    try:
        data = request.get_json()
        path = data.get('path')
        name = data.get('name')
        
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        if hasattr(graph_agent_instance, 'add_book'):
            graph_agent_instance.add_book(name, path)
            return jsonify({'success': True})
        else:
            return jsonify({'error': 'æ·»åŠ ä¹¦æœ¬åŠŸèƒ½ä¸å¯ç”¨'}), 500
            
    except Exception as e:
        print(f"æ·»åŠ ä¹¦æœ¬é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/current-book', methods=['GET'])
def get_current_book():
    """è·å–å½“å‰ä¹¦æœ¬"""
    try:
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        current_book = graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else None
        return jsonify({'currentBook': current_book})
        
    except Exception as e:
        print(f"è·å–å½“å‰ä¹¦æœ¬é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'agent_initialized': graph_agent is not None
    })

@app.route('/api/cross-chat', methods=['POST'])
def cross_chat():
    """è·¨ä¹¦åˆ›ä½œï¼ˆSSEï¼‰ï¼šä¸æ”¹å˜currentBookï¼ŒæŒ‰æ‰€é€‰ä¹¦æœ¬å¹¶è¡Œæ£€ç´¢å¹¶ç”Ÿæˆã€‚"""
    try:
        data = request.get_json(silent=True) or {}
        books = data.get('books', [])
        prompt = data.get('message', '') or data.get('prompt', '')
        history = data.get('history', [])
        mode = data.get('mode', 'both')
        topk = int(data.get('topK', 5))
        if not isinstance(books, list) or len(books) == 0:
            return jsonify({'error': 'è¯·è‡³å°‘é€‰æ‹©ä¸€æœ¬ä¹¦'}), 400
        if not prompt:
            return jsonify({'error': 'promptä¸èƒ½ä¸ºç©º'}), 400

        def generate():
            try:
                yield "event: status\n"
                yield f"data: {json.dumps({'status':'å¼€å§‹å¹¶è¡Œæ£€ç´¢ä¸Šä¸‹æ–‡...'}, ensure_ascii=False)}\n\n"

                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                # é€šè¿‡å­Agentæ‰§è¡ŒåŸå·¥å…·é“¾ï¼Œé€ä¹¦å‘å·¥å…·äº‹ä»¶
                sse_queue = queue.Queue()
                def sse_emit(evt, payload):
                    sse_queue.put(f"event: {evt}\n" + f"data: {json.dumps(payload, ensure_ascii=False)}\n\n")

                from cross_book_agent import CrossOrchestrator
                orchestrator = CrossOrchestrator(sse_emit)

                # å›è°ƒhandlerå·¥å‚ï¼šé‡ç”¨ç°æœ‰ StreamingCallbackHandler ä»¥æ¨é€ llm/tool äº‹ä»¶
                def handler_factory_for_book(book_label: str):
                    def yield_with_book(s: str):
                        try:
                            if s.startswith("event:"):
                                # inject book into data json
                                parts = s.split("\n")
                                if len(parts) >= 2 and parts[1].startswith("data: "):
                                    et = parts[0][7:].strip()
                                    raw = parts[1][6:]
                                    payload = json.loads(raw)
                                    if isinstance(payload, dict):
                                        payload['book'] = book_label
                                        s_mod = f"event: {et}\n" + f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
                                        sse_queue.put(s_mod)
                                        return
                        except Exception:
                            pass
                        sse_queue.put(s)
                    return StreamingCallbackHandler(yield_with_book)

                # ä¸ºæ¯æœ¬ä¹¦åˆ›å»ºå¸¦bookæ ‡è®°çš„å›è°ƒhandler
                def cb_factory_picker(book_name: str):
                    return lambda: handler_factory_for_book(book_name)

                # è¿è¡Œå¹¶å‘å­ä»£ç†ï¼ˆå®æ—¶æ¨é€äº‹ä»¶ï¼‰
                contexts_holder = {"data": None, "error": None}

                async def run_all():
                    from cross_book_agent import CrossOrchestrator
                    orch = orchestrator
                    tasks = []
                    for b in books:
                        tasks.append(orch._run_one_book_agent(b, prompt, history or [], cb_factory_picker(b)))
                    return await asyncio.gather(*tasks)

                def run_retrieve_thread():
                    try:
                        contexts_holder["data"] = loop.run_until_complete(run_all())
                    except Exception as re:
                        contexts_holder["error"] = str(re)

                retrieve_thread = threading.Thread(target=run_retrieve_thread, daemon=True)
                retrieve_thread.start()

                # å®æ—¶è½¬å‘å›è°ƒè¾“å‡ºï¼ˆæ£€ç´¢é˜¶æ®µï¼‰
                while True:
                    try:
                        chunk = sse_queue.get(timeout=0.1)
                        yield chunk
                    except queue.Empty:
                        if not retrieve_thread.is_alive():
                            break
                        continue

                retrieve_thread.join()

                if contexts_holder["error"]:
                    yield "event: error\n"
                    yield f"data: {json.dumps({'error': contexts_holder['error']}, ensure_ascii=False)}\n\n"
                    return
                contexts = contexts_holder["data"] or []

                # æ„é€ ç”Ÿæˆæ¶ˆæ¯ï¼ˆèåˆï¼‰
                messages = cross_agent.build_messages(contexts, prompt)

                # æµå¼ç”Ÿæˆ
                class CrossStreamHandler(BaseCallbackHandler):
                    def __init__(self, yield_fn):
                        self.yield_fn = yield_fn
                    def on_llm_start(self, serialized, prompts, **kwargs):
                        self.yield_fn(f"event: llm_start\ndata: {json.dumps({'model': serialized.get('name','llm')}, ensure_ascii=False)}\n\n")
                    def on_llm_new_token(self, token: str, **kwargs):
                        self.yield_fn(f"event: llm_token\ndata: {json.dumps({'token': token}, ensure_ascii=False)}\n\n")
                    def on_llm_end(self, response, **kwargs):
                        self.yield_fn("event: llm_end\n")
                        self.yield_fn("data: {}\n\n")

                handler = CrossStreamHandler(lambda s: None)

                async def run_gen():
                    resp = await cross_agent.llm_gen.ainvoke(messages, config={"callbacks": [handler]})
                    return resp.content if hasattr(resp, 'content') else str(resp)

                # é€šè¿‡å›è°ƒç›´æ¥yieldsseï¼šé‡å†™yieldå‡½æ•°ï¼ˆå®æ—¶æ¨é€åˆ°é˜Ÿåˆ—ï¼‰
                output_queue = queue.Queue()
                def sse_yield(s):
                    output_queue.put(s)

                # æ›¿æ¢handlerçš„yield_fn
                handler.yield_fn = sse_yield

                final_holder = {"text": None, "error": None}

                # åœ¨åå°çº¿ç¨‹æ‰§è¡Œç”Ÿæˆï¼Œä¸»çº¿ç¨‹æŒç»­ä»é˜Ÿåˆ—å–äº‹ä»¶å¹¶å‘å‰ç«¯æ¨é€
                def run_gen_thread():
                    try:
                        result = loop.run_until_complete(run_gen())
                        final_holder["text"] = result
                    except Exception as ge:
                        final_holder["error"] = str(ge)

                gen_thread = threading.Thread(target=run_gen_thread, daemon=True)
                gen_thread.start()

                # å®æ—¶è½¬å‘å›è°ƒè¾“å‡º
                while True:
                    try:
                        chunk = output_queue.get(timeout=0.1)
                        yield chunk
                    except queue.Empty:
                        if not gen_thread.is_alive():
                            break
                        continue

                gen_thread.join()

                # æ”¶å°¾
                if final_holder["error"]:
                    yield "event: error\n"
                    yield f"data: {json.dumps({'error': final_holder['error']}, ensure_ascii=False)}\n\n"
                else:
                    yield "event: final\n"
                    yield f"data: {json.dumps({'response': final_holder['text'] or ''}, ensure_ascii=False)}\n\n"
                yield "event: done\n"
                yield "data: {}\n\n"
            except Exception as e:
                yield "event: error\n"
                yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

        return app.response_class(
            generate(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
    except Exception as e:
        print(f"è·¨ä¹¦åˆ›ä½œAPIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/critique', methods=['POST'])
def api_critique():
    """ç‚¹è¯„APIï¼šæ ¹æ®æ–‡æœ¬ä¸ç”¨æˆ·éœ€æ±‚ç»™å‡ºä¿®æ”¹æ„è§ï¼ˆSSEæµå¼ï¼‰ã€‚"""
    try:
        data = request.get_json(silent=True) or {}
        text = data.get('text', '')
        history = data.get('history', [])
        user_prompt = data.get('userPrompt', '')
        stream = data.get('stream', True)

        if not text:
            return jsonify({'error': 'textä¸èƒ½ä¸ºç©º'}), 400

        # å†å²å½’ä¸€åŒ–
        if isinstance(history, list):
            history_text = []
            for msg in history[-10:]:
                role = msg.get('type') or msg.get('role')
                content = msg.get('content', '')
                if role == 'user':
                    history_text.append(f"ç”¨æˆ·: {content}")
                elif role in ('assistant', 'ai'):
                    history_text.append(f"åŠ©æ‰‹: {content}")
            history_text = "\n".join(history_text)
        else:
            history_text = str(history or '')

        if not stream:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(
                    polish_agent.critique_async(
                        text=text,
                        chat_history_text=history_text,
                        criteria=f"ä¸¥æ ¼å¯¹ç…§ç”¨æˆ·éœ€æ±‚ï¼š{user_prompt}ï¼ŒæŒ‡å‡ºä¸ç¬¦åˆç‚¹å¹¶ç»™å‡ºç®€æ´ä¿®æ”¹å»ºè®®",
                    )
                )
                return jsonify({'critique': result})
            finally:
                loop.close()

        def generate():
            try:
                yield "event: status\n"
                yield f"data: {json.dumps({'status': 'å¼€å§‹ç”Ÿæˆä¿®æ”¹æ„è§...'}, ensure_ascii=False)}\n\n"

                callback_q = queue.Queue()
                class CritiqueStreamingHandler(BaseCallbackHandler):
                    def __init__(self, yield_fn):
                        self.yield_fn = yield_fn
                    def on_llm_new_token(self, token: str, **kwargs):
                        self.yield_fn(f"event: llm_token\ndata: {json.dumps({'token': token}, ensure_ascii=False)}\n\n")

                def q_yield(sse_chunk: str):
                    callback_q.put(sse_chunk)

                handler = CritiqueStreamingHandler(q_yield)
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                async def run_critique():
                    from langchain_core.messages import SystemMessage, HumanMessage
                    system_rules = (
                        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡æ–‡æœ¬ç‚¹è¯„åŠ©æ‰‹ã€‚\n"
                        "- ä»æ¸…æ™°åº¦ã€ç»“æ„æ€§ã€è¯­æ°”ä¸€è‡´æ€§ã€äº‹å®ä¿çœŸã€ä¸ç”¨æˆ·éœ€æ±‚ä¸€è‡´æ€§ç­‰ç»´åº¦è¯„ä¼°\n"
                        "- æŒ‡å‡ºå…·ä½“å¯æ”¹è¿›ä¹‹å¤„ï¼Œå¹¶ç»™å‡ºç®€çŸ­ä¿®æ”¹å»ºè®®\n"
                        "- ä¸ç¼–é€ å†…å®¹ï¼Œä¸åŠ å…¥æ— æ ¹æ®çš„ä¿¡æ¯"
                    )
                    hist_part = f"\n\n[å¯¹è¯å†å²]\n{history_text}" if history_text else ""
                    requirement = f"\n\n[ç”¨æˆ·éœ€æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼æ»¡è¶³ï¼‰]\n{user_prompt}" if user_prompt else ""
                    prompt = (
                        "è¯·ä¸¥æ ¼å¯¹ç…§[ç”¨æˆ·éœ€æ±‚]ç‚¹è¯„ä¸‹è¿°æ–‡æœ¬ï¼Œåˆ—å‡ºä¸ç¬¦åˆç‚¹ä¸æ”¹è¿›å»ºè®®è¦ç‚¹ï¼›è‹¥å®Œå…¨ç¬¦åˆï¼Œä¹Ÿè¯·ç®€è¿°ä¼˜åŒ–ç©ºé—´ã€‚\n"
                        "ä»…è¾“å‡ºç‚¹è¯„å†…å®¹ï¼Œä¸è¦å¤è¿°åŸæ–‡ã€‚\n\n"
                        f"[æ–‡æœ¬]\n{text}{hist_part}{requirement}"
                    )
                    messages = [SystemMessage(content=system_rules), HumanMessage(content=prompt)]
                    resp = await polish_agent.llm_polish.ainvoke(messages, config={"callbacks": [handler]})
                    return resp.content if hasattr(resp, 'content') else str(resp)

                critique_text = loop.run_until_complete(run_critique())
                while not callback_q.empty():
                    yield callback_q.get()
                yield "event: final\n"
                yield f"data: {json.dumps({'critique': critique_text}, ensure_ascii=False)}\n\n"
                yield "event: done\n"
                yield "data: {}\n\n"
            except Exception as e:
                yield "event: error\n"
                yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

        return app.response_class(
            generate(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
    except Exception as e:
        print(f"ç‚¹è¯„APIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500
if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½åˆ›ä½œåŠ©æ‰‹...")
    
    # åˆå§‹åŒ–ä»£ç†
    if initialize_agent():
        print("ğŸŒ å¯åŠ¨WebæœåŠ¡å™¨...")
        app.run(host='0.0.0.0', port=5000, debug=True)
    else:
        print("âŒ æ— æ³•å¯åŠ¨æœåŠ¡å™¨ï¼Œä»£ç†åˆå§‹åŒ–å¤±è´¥")