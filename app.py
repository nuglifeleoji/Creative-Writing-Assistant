from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import asyncio
import json
import os
import queue
import threading
import time
import uuid
from langchain_agent import GraphAnalysisAgent
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any, Dict, List
from langchain_core.agents import AgentAction, AgentFinish

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡
graph_agent = None
graph_agent_instance = None  # ä¿å­˜ GraphAnalysisAgent å®ä¾‹çš„å¼•ç”¨

# çº¿ç¨‹å®‰å…¨çš„å›è°ƒæ¶ˆæ¯é˜Ÿåˆ—
callback_queue = queue.Queue()
class StreamingCallbackHandler(BaseCallbackHandler):
    """åªæš´éœ²è¿‡ç¨‹ï¼Œä¸æ³„éœ²æ¨ç†ç»†èŠ‚ï¼ˆä¸è¾“å‡º action.logï¼‰ã€‚"""
    def __init__(self, yield_func):
        self.yield_func = yield_func
        self.current_tool = None
        self._tool_start_ts = {}
        self._run_id = str(uuid.uuid4())
        self._sent_events = set()  # ç”¨äºå»é‡

    def _send(self, etype: str, payload: Dict[str, Any]):
        # å¯¹äºtokenäº‹ä»¶ï¼Œä¸è¿›è¡Œå»é‡ï¼ˆå› ä¸ºæ¯ä¸ªtokenéƒ½ä¸åŒï¼‰
        if etype == "llm_token":
            # ç›´æ¥è·³è¿‡tokenäº‹ä»¶ï¼Œé¿å…å‰ç«¯æ˜¾ç¤ºè¿‡å¤šæ— ç”¨ä¿¡æ¯
            return
            
        # å¯¹äºå…¶ä»–äº‹ä»¶ï¼Œä½¿ç”¨äº‹ä»¶ç±»å‹+å…³é”®å†…å®¹è¿›è¡Œå»é‡
        if etype in ["run_start", "run_end"]:
            event_key = f"{etype}"  # è¿™äº›äº‹ä»¶æ¯æ¬¡åªåº”è¯¥å‘é€ä¸€æ¬¡
        else:
            event_key = f"{etype}:{str(payload)[:50]}"
        
        if event_key in self._sent_events:
            print(f"âš ï¸ è·³è¿‡é‡å¤äº‹ä»¶: {etype}")
            return
        
        self._sent_events.add(event_key)
        payload = {"runId": self._run_id, **payload}
        # æŒ‰ SSE æ ‡å‡†å†™æ³•è¾“å‡ºï¼ševent + data + ç©ºè¡Œ
        sse_message = f"event: {etype}\ndata: {json.dumps(payload, ensure_ascii=False)}\n\n"
        print(f"ğŸ”„ å›è°ƒå‘é€SSE: {etype} -> {str(payload)[:100]}...")
        self.yield_func(sse_message)

    # ä»»åŠ¡èµ·æ­¢
    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        self._send("run_start", {
            "chain": serialized.get("name", "chain"),
            "inputsPreview": str(inputs)[:300]
        })

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        self._send("run_end", {
            "outputsPreview": str(outputs)[:300]
        })

    # Agent å†³ç­–ï¼ˆä¸è¾“å‡ºæ€ç»´æ–‡æœ¬ï¼‰
    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        self._send("plan", {
            "nextTool": action.tool,
            "argsPreview": str(action.tool_input)[:300]
        })

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        self._send("plan_done", {
            "finalReturnPreview": str(finish.return_values)[:300]
        })

    # å·¥å…·è°ƒç”¨
    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
        tool_name = serialized.get("name", "unknown_tool")
        self.current_tool = tool_name
        self._tool_start_ts[tool_name] = time.time()
        self._send("tool_start", {
            "tool": tool_name,
            "inputPreview": input_str[:500]
        })

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        tool = self.current_tool or 'unknown_tool'
        latency = None
        if tool in self._tool_start_ts:
            latency = round((time.time() - self._tool_start_ts.pop(tool)), 3)
        self._send("tool_end", {
            "tool": tool,
            "latencySec": latency,
            "outputPreview": output[:800],
            "truncated": len(output) > 800
        })

    # LLM token æµ + ç”¨é‡
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        self._send("llm_start", {
            "model": serialized.get("name", "llm"),
            "promptPreview": (prompts[0][:300] if prompts else "")
        })

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self._send("llm_token", {"token": token})

    def on_llm_end(self, response, **kwargs: Any) -> None:
        usage = {}
        try:
            usage = getattr(response, "usage_metadata", None) or {}
            if not usage and hasattr(response, "generations"):
                gi = response.generations[0][0].generation_info or {}
                usage = gi.get("usage", gi)
        except Exception:
            usage = {}
        self._send("llm_end", {"usage": usage})

def initialize_agent():
    """åˆå§‹åŒ–LangChainä»£ç†"""
    global graph_agent, graph_agent_instance
    try:
        # åˆ›å»º GraphAnalysisAgent å®ä¾‹
        graph_agent_instance = GraphAnalysisAgent(use_multi_book=True)
        
        # è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬
        print("ğŸ“š æ­£åœ¨è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬...")
        
        # å®šä¹‰è¦åŠ è½½çš„ä¹¦æœ¬åˆ—è¡¨
        books_to_load = [
            ("å¹³å‡¡çš„ä¸–ç•Œ", "./book4/output"),
            ("ä¸‰ä½“", "./book5/output"), 
            ("ä¸‰ä½“2", "./book6/output"),
            ("è¶…æ–°æ˜Ÿçºªå…ƒ", "./cxx/output"),
            ("ç™½å¤œè¡Œ", "./rag_book2/ragtest/output"),
            ("å¼—å…°è‚¯æ–¯å¦", "./tencent/output"),
            ("æ²™ä¸˜", "./rag/output"),
            ("å«Œç–‘äººxçš„çŒ®èº«", "./book7/output"),
            ("æ–—ç½—å¤§é™†4", "./book8/output"),
            ("ä¸‰å›½æ¼”ä¹‰","./sanguo/output")
        ]
        
        loaded_books = []
        for book_name, book_path in books_to_load:
            try:
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                if os.path.exists(book_path):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
                    required_files = ["communities.parquet", "entities.parquet", "community_reports.parquet", "relationships.parquet", "text_units.parquet"]
                    missing_files = [f for f in required_files if not os.path.exists(os.path.join(book_path, f))]
                    
                    if not missing_files:
                        graph_agent_instance.add_book(book_name, book_path)
                        loaded_books.append(book_name)
                        print(f"âœ… æˆåŠŸåŠ è½½ä¹¦æœ¬: {book_name} -> {book_path}")
                    else:
                        print(f"âš ï¸ è·³è¿‡ {book_name}: ç¼ºå°‘å¿…è¦æ–‡ä»¶ {missing_files}")
                else:
                    print(f"âš ï¸ è·³è¿‡ {book_name}: è·¯å¾„ä¸å­˜åœ¨ {book_path}")
            except Exception as e:
                print(f"âŒ åŠ è½½ {book_name} å¤±è´¥: {e}")
        
        print(f"âœ… æ€»å…±åŠ è½½äº† {len(loaded_books)} æœ¬ä¹¦: {', '.join(loaded_books)}")
        
        # ä¸è‡ªåŠ¨é€‰æ‹©ä¹¦æœ¬ï¼Œè®©ç”¨æˆ·ä¸»åŠ¨é€‰æ‹©
        if loaded_books:
            print(f"ğŸ“š å·²åŠ è½½ä¹¦æœ¬ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©: {', '.join(loaded_books)}")
        else:
            print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•ä¹¦æœ¬ï¼Œè¯·æ‰‹åŠ¨æ·»åŠ ä¹¦æœ¬")
        
        # ä½¿ç”¨è¿™ä¸ªå®ä¾‹åˆ›å»º LangChain Agent
        from langchain_agent import create_graphrag_agent
        graph_agent = create_graphrag_agent(graph_agent_instance)
        graph_agent_instance = graph_agent_instance  # ä¿å­˜å¼•ç”¨
        
        print("âœ… ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ ä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return send_from_directory('frontend', 'index.html')

@app.route('/test')
def test_page():
    """æµ‹è¯•é¡µé¢"""
    return send_from_directory('frontend', 'test.html')

@app.route('/<path:filename>')
def serve_static(filename):
    """æä¾›é™æ€æ–‡ä»¶"""
    return send_from_directory('frontend', filename)

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©API - æ”¯æŒæµå¼å“åº”"""
    try:
        data = request.get_json()
        message = data.get('message', '')
        current_book = data.get('currentBook')
        history = data.get('history', [])
        stream = data.get('stream', False)  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
        
        if not graph_agent:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        # æ„å»ºå†å²å¯¹è¯
        chat_history = []
        for msg in history[-10:]:  # åªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
            if msg['type'] == 'user':
                chat_history.append(f"ç”¨æˆ·: {msg['content']}")
            elif msg['type'] == 'assistant':
                chat_history.append(f"åŠ©æ‰‹: {msg['content']}")
        
        if stream:
            # æµå¼å“åº”ï¼ˆSSEï¼‰
            def generate():
                try:
                    # åˆå§‹çŠ¶æ€
                    yield "event: status\n"
                    yield f"data: {json.dumps({'prompt':'æ­£åœ¨åˆ†æé—®é¢˜...'}, ensure_ascii=False)}\n\n"

                    # åˆ›å»ºæ–°çš„é˜Ÿåˆ—å®ä¾‹ï¼Œç¡®ä¿å®Œå…¨æ¸…ç©º
                    callback_queue = queue.Queue()
                    
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    def callback_yield(sse_chunk: str):
                        # è¿™é‡Œ sse_chunk å·²ç»æ˜¯ "event:...\n data:...\n\n" çš„å®Œæ•´ç‰‡æ®µ
                        callback_queue.put(sse_chunk)

                    # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºæ–°çš„å›è°ƒå¤„ç†å™¨å®ä¾‹
                    callback_handler = StreamingCallbackHandler(callback_yield)
                    print(f"ğŸ†• åˆ›å»ºæ–°çš„å›è°ƒå¤„ç†å™¨: {callback_handler._run_id}")

                    def run_agent():
                        try:
                            print(f"ğŸ¤– å¼€å§‹æ‰§è¡Œä»£ç†: {message}")
                            resp = loop.run_until_complete(
                                graph_agent.ainvoke(
                                    {"input": message, "chat_history": chat_history},
                                    config={"callbacks": [callback_handler]}
                                )
                            )
                            print(f"âœ… ä»£ç†æ‰§è¡Œå®Œæˆ: {type(resp)}")
                            callback_queue.put(("response", resp))
                        except Exception as e:
                            print(f"âŒ ä»£ç†æ‰§è¡Œå¤±è´¥: {e}")
                            callback_queue.put(("error", str(e)))

                    agent_thread = threading.Thread(target=run_agent, daemon=True)
                    agent_thread.start()

                    response = None
                    message_count = 0
                    while True:
                        try:
                            item = callback_queue.get(timeout=0.1)
                            message_count += 1
                            print(f"ğŸ“¨ æ”¶åˆ°é˜Ÿåˆ—æ¶ˆæ¯ #{message_count}: {type(item)}")
                            
                            if isinstance(item, tuple):
                                if item[0] == "response":
                                    print(f"ğŸ¯ æ”¶åˆ°æœ€ç»ˆå“åº”")
                                    response = item[1]
                                    break
                                elif item[0] == "error":
                                    print(f"ğŸ’¥ æ”¶åˆ°é”™è¯¯: {item[1]}")
                                    raise Exception(item[1])
                            else:
                                # ç”±å›è°ƒå™¨æ„é€ çš„ SSE äº‹ä»¶ï¼Œç›´æ¥é€ä¼ 
                                print(f"ğŸ“¡ è½¬å‘SSEæ¶ˆæ¯: {item[:100]}...")
                                yield item
                        except queue.Empty:
                            if not agent_thread.is_alive():
                                print(f"ğŸ”š ä»£ç†çº¿ç¨‹å·²ç»“æŸï¼Œé€€å‡ºå¾ªç¯")
                                break
                            continue

                    agent_thread.join()

                    # ç»“æŸå‘é€ final/done
                    if isinstance(response, dict) and 'output' in response:
                        output = response['output']
                    else:
                        output = str(response)

                    # å°è¯•è§„çº¦ä¸­é—´æ­¥éª¤ï¼ˆå·¥å…·å + å‚æ•°/è§‚æµ‹æ‘˜è¦ï¼‰
                    intermediate = []
                    if isinstance(response, dict):
                        for st in response.get("intermediate_steps", []):
                            try:
                                action, observation = st
                                intermediate.append({
                                    "tool": getattr(action, "tool", "unknown"),
                                    "argsPreview": str(getattr(action, "tool_input", ""))[:300],
                                    "observationPreview": str(observation)[:500]
                                })
                            except Exception:
                                pass

                    final_payload = {
                        'response': output,
                        'currentBook': graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else current_book,
                        'needBookSelection': ("éœ€è¦é€‰æ‹©ä¹¦æœ¬" in output) or ("è¯·å…ˆé€‰æ‹©" in output),
                        'intermediate': intermediate
                    }
                    yield "event: final\n"
                    yield f"data: {json.dumps(final_payload, ensure_ascii=False)}\n\n"
                    yield "event: done\n"
                    yield "data: {}\n\n"

                except Exception as e:
                    yield "event: error\n"
                    yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

            return app.response_class(
                generate(),
                mimetype='text/event-stream',  # <<< å…³é”®
                headers={
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'X-Accel-Buffering': 'no'  # Nginx ä¸‹å…³é—­ç¼“å†²
                }
            )
        else:
            # éæµå¼å“åº”ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                response = loop.run_until_complete(graph_agent.ainvoke({
                    "input": message,
                    "chat_history": chat_history
                }))
                steps = []
                if isinstance(response, dict):
                    for st in response.get("intermediate_steps", []):
                        try:
                            action, observation = st
                            steps.append({
                                "tool": getattr(action, "tool", "unknown"),
                                "argsPreview": str(getattr(action, "tool_input", ""))[:300],
                                "observationPreview": str(observation)[:500]
                            })
                        except Exception:
                            pass
                # è§£æå“åº”
                if isinstance(response, dict) and 'output' in response:
                    output = response['output']
                else:
                    output = str(response)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‰æ‹©ä¹¦æœ¬
                if "éœ€è¦é€‰æ‹©ä¹¦æœ¬" in output or "è¯·å…ˆé€‰æ‹©" in output:
                    return jsonify({
                        'response': output,
                        'needBookSelection': True,
                        'currentBook': current_book
                    })
                
                return jsonify({
                    'response': output,
                    'currentBook': graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else current_book,
                    'toolCalls': []
                })
                
            finally:
                loop.close()
            
    except Exception as e:
        print(f"èŠå¤©APIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/books', methods=['GET'])
def list_books():
    """è·å–ä¹¦æœ¬åˆ—è¡¨"""
    try:
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        book_names = graph_agent_instance.list_books() if hasattr(graph_agent_instance, 'list_books') else []
        current_book = graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else None
        
        # è¿”å›æ›´è¯¦ç»†çš„ä¹¦æœ¬ä¿¡æ¯
        books = []
        for name in book_names:
            books.append({
                'name': name,
                'isCurrent': name == current_book
            })
        
        print(f"ğŸ“š è¿”å›ä¹¦æœ¬åˆ—è¡¨: {books}")
        return jsonify(books)
        
    except Exception as e:
        print(f"è·å–ä¹¦æœ¬åˆ—è¡¨é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/switch-book', methods=['POST'])
def switch_book():
    """åˆ‡æ¢ä¹¦æœ¬"""
    try:
        data = request.get_json()
        book_name = data.get('bookName')
        
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        if hasattr(graph_agent_instance, 'switch_book'):
            graph_agent_instance.switch_book(book_name)
            return jsonify({'success': True, 'currentBook': book_name})
        else:
            return jsonify({'error': 'åˆ‡æ¢ä¹¦æœ¬åŠŸèƒ½ä¸å¯ç”¨'}), 500
            
    except Exception as e:
        print(f"åˆ‡æ¢ä¹¦æœ¬é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/add-book', methods=['POST'])
def add_book():
    """æ·»åŠ ä¹¦æœ¬"""
    try:
        data = request.get_json()
        path = data.get('path')
        name = data.get('name')
        
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        if hasattr(graph_agent_instance, 'add_book'):
            graph_agent_instance.add_book(name, path)
            return jsonify({'success': True})
        else:
            return jsonify({'error': 'æ·»åŠ ä¹¦æœ¬åŠŸèƒ½ä¸å¯ç”¨'}), 500
            
    except Exception as e:
        print(f"æ·»åŠ ä¹¦æœ¬é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/current-book', methods=['GET'])
def get_current_book():
    """è·å–å½“å‰ä¹¦æœ¬"""
    try:
        if not graph_agent_instance:
            return jsonify({'error': 'ä»£ç†æœªåˆå§‹åŒ–'}), 500
        
        current_book = graph_agent_instance.get_current_book() if hasattr(graph_agent_instance, 'get_current_book') else None
        return jsonify({'currentBook': current_book})
        
    except Exception as e:
        print(f"è·å–å½“å‰ä¹¦æœ¬é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'agent_initialized': graph_agent is not None
    })

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½åˆ›ä½œåŠ©æ‰‹...")
    
    # åˆå§‹åŒ–ä»£ç†
    if initialize_agent():
        print("ğŸŒ å¯åŠ¨WebæœåŠ¡å™¨...")
        app.run(host='0.0.0.0', port=5000, debug=True)
    else:
        print("âŒ æ— æ³•å¯åŠ¨æœåŠ¡å™¨ï¼Œä»£ç†åˆå§‹åŒ–å¤±è´¥")