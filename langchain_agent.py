
# import os
# import json
# import asyncio
# from typing import Dict, Any

# # ç¡®ä¿ä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“
# # pip install langchain langchain-openai

# # æ³¨æ„é…ç½®OPENAI_API_KEYä»¥åŠgraphragæ‰€åœ¨è·¯å¾„(ä»£ç ç¬¬172è¡Œ)

# from dotenv import load_dotenv

# load_dotenv()

# # ä¼˜å…ˆè¯»å– OPENAI_API_KEYï¼Œå…¶æ¬¡ AZURE_OPENAI_API_KEYï¼Œä¸è¦æŠŠå¯†é’¥å½“ä½œç¯å¢ƒå˜é‡å
# api_key =os.getenv("AZURE_OPENAI_API_KEY") or ""

# import tiktoken
# from langchain.agents import tool
# from langchain.agents import create_react_agent, AgentExecutor, create_tool_calling_agent
# from langchain import hub
# from langchain_openai import ChatOpenAI,AzureChatOpenAI
# from langchain.prompts import ChatPromptTemplate
# from langchain_core.messages import SystemMessage, HumanMessage
# from search.global_search import global_search as graphrag_global_search
# from search.global_search import global_retrieve
# from search.local_search import local_search
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# import prompt_utils
# import prompt
# class GraphAnalysisAgent:
#     def __init__(self):
#         self.global_search = graphrag_global_search
#         self.local_search = local_search
#     async def global_search_async(self, query: str) -> Dict[str, Any]:
#         """ç›´æ¥è°ƒç”¨ agent.py ä¸­çš„ global_searchï¼ˆGraphRAG GlobalSearchï¼‰ï¼Œè¿”å›ç²¾ç®€æ–‡æœ¬ã€‚"""
#         try:
#             res = await graphrag_global_search(query)
#             # agent.py å½“å‰å¯èƒ½è¿”å›ç»“æœå¯¹è±¡æˆ–æ–‡æœ¬ï¼Œè¿™é‡Œç»Ÿä¸€æŠ½å–æ–‡æœ¬
#             text = getattr(res, "response", res)
#             if not isinstance(text, str):
#                 text = str(text)
#             return {"method": "global", "query": query, "result": text, "success": True}
#         except Exception as e:
#             return {"method": "global", "query": query, "error": str(e), "success": False}
    
#     async def global_retrieve_async(self, query: str) -> Dict[str, Any]:
#         """ç›´æ¥è°ƒç”¨ agent.py ä¸­çš„ global_retrieveï¼ˆGraphRAG GlobalSearchï¼‰ï¼Œè¿”å›ç²¾ç®€æ–‡æœ¬ã€‚"""
#         try:
#             res = await global_retrieve(query)

#             text = getattr(res, "response", res)
#             if not isinstance(text, str):
#                 text = str(text)
#             # current_tokens = 0
#             # tokenizer = tiktoken.get_encoding("cl100k_base")
#             # selected_text = []
#             # for txt in text:
#             #     tokens = len(tokenizer.encode(txt))
#             #     if current_tokens + tokens > 3000:
#             #         break
#             #     current_tokens += tokens
#             #     selected_text.append(txt)
#             # context = "\n".join(selected_text)
#             return {"method": "global", "query": query, "result": text, "success": True}
#         except Exception as e:
#             return {"method": "global", "query": query, "error": str(e), "success": False}

#     async def local_search_async(self, query: str) -> Dict[str, Any]:
#         try:
#             res = await local_search(query)
#             # agent.py å½“å‰å¯èƒ½è¿”å›ç»“æœå¯¹è±¡æˆ–æ–‡æœ¬ï¼Œè¿™é‡Œç»Ÿä¸€æŠ½å–æ–‡æœ¬
#             text = getattr(res, "response", res)
#             if not isinstance(text, str):
#                 text = str(text)
#             return {"method": "local", "query": query, "result": text, "success": True}
#         except Exception as e:
#             return {"method": "local", "query": query, "error": str(e), "success": False}

#     async def get_characters_async(self) -> Dict[str, Any]:
#         return await self.global_search_async("åˆ—å‡ºæ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²")

#     async def get_relationships_async(self, p1: str, p2: str) -> Dict[str, Any]:
#         return await self.global_search_async(f"åˆ†æ{p1}å’Œ{p2}ä¹‹é—´çš„å…³ç³»")

#     async def get_important_locations_async(self) -> Dict[str, Any]:
#         return await self.global_search_async("åˆ†ææ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹å’Œåœºæ™¯")

#     async def background_knowledge_async(self) -> Dict[str, Any]:
#         return await self.global_search_async("åˆ†ææ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†")
    
#     async def get_worldview_async(self) -> Dict[str, Any]:
#         return await self.global_search_async("è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®š")

#     async def get_character_profile_async(self, character_name: str) -> Dict[str, Any]:
#         return await self.global_search_async(f"è·å–{character_name}çš„è¯¦ç»†ä¿¡æ¯")
    
#     async def get_significant_event_async(self, event_name:str) -> Dict[str, Any]:
#         return await self.global_search_async(f"è·å–äº‹ä»¶{event_name}çš„è¯¦ç»†ä¿¡æ¯")
    
#     async def get_main_theme_async(self) -> Dict[str, Any]:
#         return await self.global_retrieve_async("åˆ†ææ•…äº‹çš„ä¸»é¢˜")
#     async def mock_coversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
#         return await self.local_search_async(f"æ¨¡æ‹Ÿ{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
#     async def get_open_questions_async(self) -> Dict[str, Any]:
#         return await self.global_search_async("æœ¬ä¹¦æœ‰ä»€ä¹ˆæ‚¬å¿µæˆ–è€…æ²¡æœ‰è§£å†³çš„ä¼ç¬”ï¼Ÿ")
#     async def get_conflict_async(self) -> Dict[str, Any]:
#         return await self.global_search_async("ç½—åˆ—å‡ºæœ¬ä¹¦æœ€å¤§çš„å†²çªæ˜¯ä»€ä¹ˆ")
#     async def get_related_characters_async(self, event: str) -> Dict[str, Any]:
#         return await self.global_search_async(f"è·å–{event}äº‹ä»¶çš„å…³è”äººç‰©")
#     async def get_causal_chains_async(self, event: str) -> Dict[str, Any]:
#         return await self.local_search_async(f"è·å–{event}äº‹ä»¶çš„å› æœé“¾ï¼šå‰ç½®æ¡ä»¶â†’è§¦å‘â†’ç»“æœâ†’åæœ")
#     async def style_guardrails_async(self, persona: str) -> Dict[str, Any]:
#         return await self.global_search_async(f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚")
#     async def canon_alignment_async(self, text: str) -> Dict[str, Any]:
#         return await self.local_search_async(f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}")
#     async def contradiction_test_async(self, text: str) -> Dict[str, Any]:
#         return await self.local_search_async(f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}")
#     async def continue_story_async(self, brief: str, persona: str = "ä¿æŒä¸åŸè‘—ä¸€è‡´çš„å™è¿°è€…å£å»ä¸è§’è‰²å¯¹ç™½é£æ ¼", target_style: str = "ç´§å‡‘ã€å…·è±¡ç»†èŠ‚ã€å¯¹ç™½æ¨åŠ¨å‰§æƒ…", words_per_scene: int = 600, max_iters: int = 2) -> Dict[str, Any]:
#         return await self.local_search_async(f"ä¸ºä»¥ä¸‹å¤§çº²ç»­å†™ä¸€ä¸ªåœºæ™¯ï¼ˆä¸è¶…è¿‡{words_per_scene}è¯ï¼‰ï¼š{brief[:3000]}")
#     async def imagine_conversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
#         return await self.local_search_async(f"æƒ³è±¡{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
#     async def extract_quotes_async(self, name:str, n:int=8) -> Dict[str, Any]:
#         q = f"åˆ—å‡º{name}æœ€å…·ä»£è¡¨æ€§çš„å°è¯{n}æ¡ï¼ˆæ¯æ¡<=40å­—ï¼Œé™„ç« èŠ‚/æ®µè½ç¼–å·ï¼‰ï¼Œä¸¥æ ¼JSONæ•°ç»„ï¼š"
#         return await self.local_search_async(q)
#     async def narrative_pov_async(self) -> Dict[str, Any]:
#         q = """
#     åˆ†æå™äº‹è§†è§’ä¸å¯é æ€§ï¼šPOVç±»å‹ã€åˆ‡æ¢ç‚¹ã€å¯èƒ½åè§/è¯¯å¯¼çš„è¯æ®ã€‚ç”¨åˆ†ç‚¹åˆ—å‡ºï¼Œæ¯ç‚¹é™„<=40å­—çŸ­æ‘˜+ç« èŠ‚ã€‚
#     """
#         return await self.global_search_async(q)
#     async def get_motifs_symbols_async(self, max_items:int=20) -> Dict[str, Any]:
#         q = f"""
#     æŠ½å–æ„è±¡/æ¯é¢˜/è±¡å¾ï¼ˆæœ€å¤š{max_items}æ¡ï¼‰ï¼Œä¸¥æ ¼JSONï¼š
#     [{{"motif":"â€¦","meaning":"â€¦","linked_themes":["â€¦"],"chapters":["â€¦"],"evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}]
#     """
#         return await self.local_search_async(q)
#     async def build_story_outline_async(self, brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> Dict[str, Any]:
#         q = f"""
#     åŸºäºåŸè‘—çº¦æŸï¼Œä¸ºâ€œ{brief}â€ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚
#     é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚
#     """
#         return await self.global_search_async(q)
#     async def emotion_curve_async(self, scope:str="å…¨ä¹¦") -> Dict[str, Any]:
#         q = f"æå–{scope}çš„æƒ…æ„Ÿæ›²çº¿å…³é”®è½¬æŠ˜ï¼ˆå–œ/æ€’/å“€/æƒ§/æƒŠ/åŒ/ä¿¡ï¼‰ï¼Œåˆ—å‡ºè½¬æŠ˜ç‚¹ç« èŠ‚ä¸è§¦å‘äº‹ä»¶ï¼Œå„ç»™<=40å­—çŸ­æ‘˜ã€‚"
#         return await self.global_search_async(q)
#     async def compare_characters_async(self, a:str, b:str) -> Dict[str, Any]:
#         q = f"""
#     æ¯”è¾ƒ{a}ä¸{b}ï¼Œä¸¥æ ¼JSONï¼š
#     {{"values":["â€¦"],"goals":["â€¦"],"methods":["â€¦"],"red_lines":["â€¦"],"decision_style":"å†²åŠ¨|è°¨æ…|ç®—è®¡","evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}
#     """
#         return await self.global_search_async(q)

# # --- ç¬¬äºŒæ­¥ï¼šåˆ›å»º LangChain Agent ---
# def create_graphrag_agent(graphrag_agent_instance: GraphAnalysisAgent) -> AgentExecutor:
#     """
#     åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå¯ä»¥è°ƒç”¨ GraphRAG å‘½ä»¤è¡ŒåŠŸèƒ½çš„ LangChain Agentã€‚
#     """
#     # ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œå°† GraphAnalysisAgent çš„æ–¹æ³•åŒ…è£…æˆ LangChain å·¥å…·
#     # æ³¨æ„ï¼šè¿™é‡Œçš„å·¥å…·å‡½æ•°éœ€è¦èƒ½å¤Ÿè¢« Agent ç›´æ¥è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨é—­åŒ…æ¥ä¼ é€’å®ä¾‹
#     @tool
#     async def get_characters_tool() -> str:
#         """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²ã€‚"""
#         result = await graphrag_agent_instance.get_characters_async()
#         return json.dumps(result, ensure_ascii=False)

#     @tool
#     async def get_relationships_tool(p1: str, p2: str) -> str:
#         """è·å–ä¸¤ä¸ªç‰¹å®šäººç‰©ä¹‹é—´çš„å…³ç³»ã€‚è¾“å…¥å‚æ•°p1å’Œp2æ˜¯äººç‰©åç§°ã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸¤ä¸ªäººç‰©çš„å…³ç³»ï¼Œå¯ä»¥å°è¯•å•ç‹¬æŸ¥è¯¢ä¸¤ä¸ªäººç‰©çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå¹¶ä¸”å°è¯•æ‰¾åˆ°å’Œä»–ä»¬å…±åŒç›¸å…³çš„äººæ¥åˆ¤æ–­ä»–ä»¬ä¹‹é—´å¯èƒ½çš„å…³ç³»"""
#         result = await graphrag_agent_instance.get_relationships_async(p1, p2)
#         return json.dumps(result, ensure_ascii=False)

#     @tool
#     async def get_important_locations_tool() -> str:
#         """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹ã€‚"""
#         result = await graphrag_agent_instance.get_important_locations_async()
#         return json.dumps(result, ensure_ascii=False)

#     @tool
#     async def background_knowledge_tool() -> str:
#         """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†ã€‚"""
#         result = await graphrag_agent_instance.background_knowledge_async()
#         return json.dumps(result, ensure_ascii=False)
    
#     @tool
#     async def get_worldview_tool() -> str:
#         """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®šã€‚"""
#         result = await graphrag_agent_instance.get_worldview_async()
#         return json.dumps(result, ensure_ascii=False)

#     @tool
#     async def local_search_tool(query: str) -> str:
#         """ä½¿ç”¨ GraphRAG çš„å±€éƒ¨æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
#         result = await graphrag_agent_instance.local_search_async(query)
#         return json.dumps(result, ensure_ascii=False)

#     @tool
#     async def global_search_tool(query: str) -> str:
#         """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
#         result = await graphrag_agent_instance.global_search_async(query)
#         return json.dumps(result, ensure_ascii=False)
#     @tool 
#     async def get_character_profile_tool(character_name: str) -> str:
#         """è·å–ç‰¹å®šäººç‰©çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°character_nameæ˜¯äººç‰©åç§°ã€‚"""
#         result = await graphrag_agent_instance.get_character_profile_async(character_name)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def get_significant_event_tool(event_name: str) -> str:
#         """è·å–ç‰¹å®šäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°event_nameæ˜¯äº‹ä»¶åç§°ã€‚"""
#         result = await graphrag_agent_instance.get_significant_event_async(event_name)
#         return json.dumps(result, ensure_ascii=False)

#     @tool
#     async def get_main_theme_tool() -> str:
#         """è·å–æ•…äº‹çš„ä¸»é¢˜ã€‚"""
#         result = await graphrag_agent_instance.get_main_theme_async()
#         return json.dumps(result, ensure_ascii=False)
#     @tool 
#     async def get_open_questions_tool() -> str:
#         """è·å–æœ¬ä¹¦çš„æ‚¬å¿µæˆ–è€…æœªè§£å†³çš„ä¼ç¬”ã€‚"""
#         result = await graphrag_agent_instance.get_open_questions_async()
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def get_causal_chains_tool(event: str) -> str:
#         """è·å–ç»™å®šäº‹ä»¶çš„å› æœé“¾ã€‚å¯ä»¥çŸ¥é“æ˜¯ä»€ä¹ˆå¯¼è‡´çš„è¯¥äº‹ä»¶ï¼Œç„¶åè¯¥äº‹ä»¶å¯¼è‡´äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼Œæœ€åç»“æœåˆå¯¼è‡´äº†ä»€ä¹ˆæ ·çš„åæœ"""
#         result = await graphrag_agent_instance.get_causal_chains_async(event)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def style_guardrails_tool(persona: str) -> str:
#         """äº§å‡ºé£æ ¼æŠ¤æ ï¼šå…è®¸/ç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€è§†è§’ã€èŠ‚å¥ç­‰ï¼ˆä¾›ç»­å†™éµå®ˆï¼‰"""
#         q = f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚"
#         res = await graphrag_agent_instance.global_search_async(q)
#         return json.dumps(res, ensure_ascii=False)

#     @tool
#     async def canon_alignment_tool(text: str) -> str:
#         """è¯„ä¼°æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™ä¸€è‡´æ€§ï¼ˆè§’è‰²OOC/è®¾å®šè¿èƒŒ/å†å²è¿èƒŒï¼‰ï¼Œç»™è¦ç‚¹ä¸ä¾æ®"""
#         q = f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}"
#         res = await graphrag_agent_instance.local_search_async(q)
#         return json.dumps(res, ensure_ascii=False)

#     @tool
#     async def contradiction_test_tool(text: str) -> str:
#         """æ£€æµ‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼Œç»™å‡ºåŸæ–‡è¯æ®ç‰‡æ®µå®šä½"""
#         q = f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}"
#         res = await graphrag_agent_instance.local_search_async(q)
#         return json.dumps(res, ensure_ascii=False)
#     @tool
#     async def get_conflict_tool() -> str:
#         """è·å–æœ¬ä¹¦æœ€å¤§çš„å†²çªã€‚"""
#         result = await graphrag_agent_instance.get_conflict_async()
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def get_related_characters_tool(event: str) -> str:
#         """è·å–ç»™å®šäº‹ä»¶çš„å…³è”äººç‰©ã€‚"""
#         result = await graphrag_agent_instance.get_related_characters_async(event)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def imagine_conversation_tool(character1_name: str, character2_name: str) -> str:
#         """æƒ³è±¡ä¸¤ä¸ªè§’è‰²ä¹‹é—´çš„å¯¹è¯ã€‚"""
#         result = await graphrag_agent_instance.imagine_conversation_async(character1_name, character2_name)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def extract_quotes_tool(name:str, n:int=8) -> str:
#         """è·å–ç‰¹å®šäººç‰©çš„å°è¯ã€‚"""
#         result = await graphrag_agent_instance.extract_quotes_async(name, n)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def narrative_pov_tool() -> str:
#         """è·å–æœ¬ä¹¦çš„å™äº‹è§†è§’ã€‚"""
#         result = await graphrag_agent_instance.narrative_pov_async()
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def get_motifs_symbols_tool(max_items:int=20) -> str:
#         """è·å–æœ¬ä¹¦çš„æ„è±¡/æ¯é¢˜/è±¡å¾ã€‚"""
#         result = await graphrag_agent_instance.get_motifs_symbols_async(max_items)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def build_story_outline_tool(brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> str:
#         """åŸºäºåŸè‘—çº¦æŸï¼Œä¸ºâ€œ{brief}â€ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚"""
#         result = await graphrag_agent_instance.build_story_outline_async(brief, target_style)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def emotion_curve_tool(scope:str="å…¨ä¹¦") -> str:
#         """è·å–æœ¬ä¹¦çš„æƒ…æ„Ÿæ›²çº¿ã€‚"""
#         result = await graphrag_agent_instance.emotion_curve_async(scope)
#         return json.dumps(result, ensure_ascii=False)
#     @tool
#     async def compare_characters_tool(a:str, b:str) -> str:
#         """æ¯”è¾ƒä¸¤ä¸ªè§’è‰²ã€‚"""
#         result = await graphrag_agent_instance.compare_characters_async(a, b)
#         return json.dumps(result, ensure_ascii=False)
    
#     # @tool
#     # async def continue_story_tool(
#     #     brief: str,
#     #     persona: str = "ä¿æŒä¸åŸè‘—ä¸€è‡´çš„å™è¿°è€…å£å»ä¸è§’è‰²å¯¹ç™½é£æ ¼",
#     #     target_style: str = "ç´§å‡‘ã€å…·è±¡ç»†èŠ‚ã€å¯¹ç™½æ¨åŠ¨å‰§æƒ…",
#     #     words_per_scene: int = 600,
#     #     max_iters: int = 2
#     # ) -> str:
#     #     """
#     #     ä¸€é”®ç»­å†™ï¼šå¤§çº²->èŠ‚æ‹->å†™ä½œ->ä¸€è‡´æ€§ä¸å†²çªæ£€æŸ¥->å¿…è¦æ—¶ä¿®è®¢ã€‚è¿”å›æœ€ç»ˆåœºæ™¯æ–‡æœ¬å’Œæ ¡éªŒä¿¡æ¯ã€‚
#     #     - brief: ç”¨æˆ·çš„ç»­å†™æ„å›¾è¯´æ˜
#     #     - persona: äººè®¾ä¸å£å»çº¦æŸ
#     #     - target_style: æ–‡é£ç›®æ ‡
#     #     """
#     #     # 1) å–é£æ ¼æŠ¤æ ä¸ä¸–ç•Œè§„åˆ™
#     #     guard = await graphrag_agent_instance.global_search_async(
#     #         f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸/ç¦æ­¢çš„å¥å¼ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚"
#     #     )
#     #     world = await graphrag_agent_instance.global_search_async(
#     #         "æ€»ç»“ä¸–ç•Œè§‚ç¡¬æ€§è§„åˆ™ï¼ˆæ”¿æ²»/æ³•å¾‹/ç§‘æŠ€/å®—æ•™/é­”æ³•/ç»æµï¼‰ï¼Œè¿åçš„åæœï¼Œåˆ—è¡¨è¾“å‡ºã€‚"
#     #     )

#     #     # 2) åŸºäºåŸè‘—ç”Ÿæˆç»­å†™å¤§çº²ä¸èŠ‚æ‹ï¼ˆç”¨ GraphRAG ä¿å®ˆæŠ½çº²ï¼‰
#     #     outline = await graphrag_agent_instance.global_search_async(
#     #         f"åŸºäºåŸè‘—ä¿¡æ¯ï¼ŒæŒ‰ç…§ä¸‰å¹•å¼ç”Ÿæˆç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ï¼‰ï¼›é£æ ¼ï¼š{target_style}ï¼›ç”¨æˆ·æ„å›¾ï¼š{brief}"
#     #     )
#     #     beats = await graphrag_agent_instance.global_search_async(
#     #         f"æŠŠä»¥ä¸‹å¤§çº²æ‹†ä¸ºèŠ‚æ‹è¡¨ï¼ˆæ¯èŠ‚æ‹å«ï¼šç›®çš„ã€å†²çªã€è½¬æŠ˜ã€å…³é”®ä¿¡æ¯ã€æ¶‰åŠè§’è‰²ã€è¯æ®éœ€æ±‚ï¼‰ï¼Œç”¨ç´§å‡‘æ¸…å•ï¼š\n{outline.get('result','')[:2800]}"
#     #     )
#     #     # é€‰ç¬¬ä¸€æ¡èŠ‚æ‹å†™ä¸€ä¸ªåœºæ™¯ï¼ˆéœ€è¦æ›´å¤šå¯æ‹†å¾ªç¯ï¼‰
#     #     beat_first = "\n".join(beats.get("result","").split("\n")[:10])
#     #     # 2) åŸºäºåŸè‘—ç”Ÿæˆç»­å†™å¤§çº²ä¸èŠ‚æ‹ï¼ˆç”¨ GraphRAG ä¿å®ˆæŠ½çº²ï¼‰
#     #     outline = await graphrag_agent_instance.global_search_async(
#     #         f"åŸºäºåŸè‘—ä¿¡æ¯ï¼ŒæŒ‰ç…§ä¸‰å¹•å¼ç”Ÿæˆç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ï¼‰ï¼›é£æ ¼ï¼š{target_style}ï¼›ç”¨æˆ·æ„å›¾ï¼š{brief}"
#     #     )
#     #     beats = await graphrag_agent_instance.global_search_async(
#     #         f"æŠŠä»¥ä¸‹å¤§çº²æ‹†ä¸ºèŠ‚æ‹è¡¨ï¼ˆæ¯èŠ‚æ‹å«ï¼šç›®çš„ã€å†²çªã€è½¬æŠ˜ã€å…³é”®ä¿¡æ¯ã€æ¶‰åŠè§’è‰²ã€è¯æ®éœ€æ±‚ï¼‰ï¼Œç”¨ç´§å‡‘æ¸…å•ï¼š\n{outline.get('result','')[:2800]}"
#     #     )
#     #     # é€‰ç¬¬ä¸€æ¡èŠ‚æ‹å†™ä¸€ä¸ªåœºæ™¯ï¼ˆéœ€è¦æ›´å¤šå¯æ‹†å¾ªç¯ï¼‰
#     #     beat_first = "\n".join(beats.get("result","").split("\n")[:10])

#     #     # 3) å†™åœºæ™¯ï¼ˆç”¨ç”Ÿæˆå‹ LLMï¼‰
#     #     sys = SystemMessage(content=(
#     #         "ä½ æ˜¯ä¸€åä¸¥è°¨çš„ç»­å†™ä½œè€…ï¼Œå¿…é¡»éµå®ˆåŸè‘—ä¸–ç•Œè§„åˆ™ä¸è§’è‰²æ€§æ ¼ã€‚"
#     #         "ç”Ÿæˆæ–‡æœ¬è¦å¯ç›´æ¥å‘å¸ƒï¼Œé¿å…æ–¹å‘æ€§æè¿°ã€‚"
#     #         f"ã€é£æ ¼æŠ¤æ ã€‘{guard.get('result','')}\nã€ä¸–ç•Œè§„åˆ™ã€‘{world.get('result','')}"
#     #     ))
#     #     user = HumanMessage(content=(
#     #         f"è¯·å†™ä¸€ä¸ªå®Œæ•´åœºæ™¯ï¼ˆä¸è¶…è¿‡{words_per_scene}è¯ï¼‰ã€‚"
#     #         f"è¦æ±‚ï¼šéµå®ˆäººç‰©å£å»ä¸è®¾å®šã€ç”¨å¯¹ç™½æ¨åŠ¨å‰§æƒ…ã€ç»†èŠ‚å…·è±¡ã€é¿å…ä¸åŸè‘—å†²çªã€‚\n"
#     #         f"ã€èŠ‚æ‹ã€‘\n{beat_first}\n\nã€ç”¨æˆ·æ„å›¾ã€‘\n{brief}"
#     #     ))
#     #     gen = await llm_gen.ainvoke([sys, user])
#     #     scene = gen.content if hasattr(gen, "content") else str(gen)
#     #     # 3) å†™åœºæ™¯ï¼ˆç”¨ç”Ÿæˆå‹ LLMï¼‰
#     #     sys = SystemMessage(content=(
#     #         "ä½ æ˜¯ä¸€åä¸¥è°¨çš„ç»­å†™ä½œè€…ï¼Œå¿…é¡»éµå®ˆåŸè‘—ä¸–ç•Œè§„åˆ™ä¸è§’è‰²æ€§æ ¼ã€‚"
#     #         "ç”Ÿæˆæ–‡æœ¬è¦å¯ç›´æ¥å‘å¸ƒï¼Œé¿å…æ–¹å‘æ€§æè¿°ã€‚"
#     #         f"ã€é£æ ¼æŠ¤æ ã€‘{guard.get('result','')}\nã€ä¸–ç•Œè§„åˆ™ã€‘{world.get('result','')}"
#     #     ))
#     #     user = HumanMessage(content=(
#     #         f"è¯·å†™ä¸€ä¸ªå®Œæ•´åœºæ™¯ï¼ˆä¸è¶…è¿‡{words_per_scene}è¯ï¼‰ã€‚"
#     #         f"è¦æ±‚ï¼šéµå®ˆäººç‰©å£å»ä¸è®¾å®šã€ç”¨å¯¹ç™½æ¨åŠ¨å‰§æƒ…ã€ç»†èŠ‚å…·è±¡ã€é¿å…ä¸åŸè‘—å†²çªã€‚\n"
#     #         f"ã€èŠ‚æ‹ã€‘\n{beat_first}\n\nã€ç”¨æˆ·æ„å›¾ã€‘\n{brief}"
#     #     ))
#     #     gen = await llm_gen.ainvoke([sys, user])
#     #     scene = gen.content if hasattr(gen, "content") else str(gen)

#     #     # 4) æ ¡éªŒ & å¯èƒ½ä¿®è®¢ï¼ˆæœ€å¤š max_iters è½®ï¼‰
#     #     issues = []
#     #     for _ in range(max_iters+1):
#     #         # ä¸€è‡´æ€§ä¸å†²çªæ£€æŸ¥ï¼ˆç”¨ GraphRAG åšè¯æ®å¯¹é½ï¼‰
#     #         canon = await graphrag_agent_instance.local_search_async(
#     #             f"è¯„ä¼°æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™ä¸€è‡´æ€§ï¼ˆè§’è‰²OOC/è®¾å®šè¿èƒŒ/å†å²è¿èƒŒå„ç»™è¦ç‚¹ä¸ä¾æ®ï¼‰ï¼š{scene[:3000]}"
#     #         )
#     #         contra = await graphrag_agent_instance.local_search_async(
#     #             f"æ‰¾å‡ºæ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{scene[:3000]}"
#     #         )
#     #         hard_fail = ("è¿èƒŒ" in canon.get("result","")) or ("å†²çª" in contra.get("result",""))
#     #     # 4) æ ¡éªŒ & å¯èƒ½ä¿®è®¢ï¼ˆæœ€å¤š max_iters è½®ï¼‰
#     #     issues = []
#     #     for _ in range(max_iters+1):
#     #         # ä¸€è‡´æ€§ä¸å†²çªæ£€æŸ¥ï¼ˆç”¨ GraphRAG åšè¯æ®å¯¹é½ï¼‰
#     #         canon = await graphrag_agent_instance.local_search_async(
#     #             f"è¯„ä¼°æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™ä¸€è‡´æ€§ï¼ˆè§’è‰²OOC/è®¾å®šè¿èƒŒ/å†å²è¿èƒŒå„ç»™è¦ç‚¹ä¸ä¾æ®ï¼‰ï¼š{scene[:3000]}"
#     #         )
#     #         contra = await graphrag_agent_instance.local_search_async(
#     #             f"æ‰¾å‡ºæ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{scene[:3000]}"
#     #         )
#     #         hard_fail = ("è¿èƒŒ" in canon.get("result","")) or ("å†²çª" in contra.get("result",""))

#     #         if not hard_fail:
#     #             # æ”¶é›†å…³é”®æ–­è¨€è¯æ®ï¼ˆå¯é€‰ï¼‰
#     #             ev = await graphrag_agent_instance.local_search_async(
#     #                 "ä¸ºä¸Šè¿°ç»­å†™ä¸­å…³é”®è®¾å®šä¸è§’è‰²åŠ¨æœºæ‰¾å‡ºæœ€æœ‰åŠ›çš„è¯æ®ç‰‡æ®µï¼ˆåˆ—ç« èŠ‚/æ®µè½ID+çŸ­æ‘˜ï¼‰ï¼Œæœ€å¤š5æ¡ã€‚"
#     #             )
#     #             return json.dumps({
#     #                 "status": "DONE",
#     #                 "outline": outline,
#     #                 "beats": beats,
#     #                 "final_text": scene,
#     #                 "evidence": ev,
#     #                 "issues": issues
#     #             }, ensure_ascii=False)
#     #         if not hard_fail:
#     #             # æ”¶é›†å…³é”®æ–­è¨€è¯æ®ï¼ˆå¯é€‰ï¼‰
#     #             ev = await graphrag_agent_instance.local_search_async(
#     #                 "ä¸ºä¸Šè¿°ç»­å†™ä¸­å…³é”®è®¾å®šä¸è§’è‰²åŠ¨æœºæ‰¾å‡ºæœ€æœ‰åŠ›çš„è¯æ®ç‰‡æ®µï¼ˆåˆ—ç« èŠ‚/æ®µè½ID+çŸ­æ‘˜ï¼‰ï¼Œæœ€å¤š5æ¡ã€‚"
#     #             )
#     #             return json.dumps({
#     #                 "status": "DONE",
#     #                 "outline": outline,
#     #                 "beats": beats,
#     #                 "final_text": scene,
#     #                 "evidence": ev,
#     #                 "issues": issues
#     #             }, ensure_ascii=False)

#     #         issues.append({"canon": canon.get("result",""), "conflict": contra.get("result","")})
#     #         issues.append({"canon": canon.get("result",""), "conflict": contra.get("result","")})

#     #         # 5) ä¿®è®¢æŒ‡ä»¤ï¼ˆå†å–‚å›ç”Ÿæˆ LLMï¼‰
#     #         sys2 = SystemMessage(content=(
#     #             "æ ¹æ®è¯„å®¡æ„è§ä¿®è®¢æ–‡æœ¬ï¼ŒåŠ¡å¿…æ¶ˆé™¤OOCä¸è®¾å®š/å†å²å†²çªï¼Œä¿ç•™èŠ‚æ‹ç›®æ ‡ä¸é£æ ¼æŠ¤æ ã€‚"
#     #             f"ã€é£æ ¼æŠ¤æ ã€‘{guard.get('result','')}\nã€è¯„å®¡ã€‘{json.dumps(issues[-1], ensure_ascii=False)[:1500]}"
#     #         ))
#     #         user2 = HumanMessage(content=(
#     #             f"è¯·åœ¨ä¸è¶…è¿‡{words_per_scene}è¯å†…é‡å†™è¯¥åœºæ™¯ï¼š\n{scene[:2000]}"
#     #         ))
#     #         rev = await llm_gen.ainvoke([sys2, user2])
#     #         scene = rev.content if hasattr(rev, "content") else str(rev)
#     #         # 5) ä¿®è®¢æŒ‡ä»¤ï¼ˆå†å–‚å›ç”Ÿæˆ LLMï¼‰
#     #         sys2 = SystemMessage(content=(
#     #             "æ ¹æ®è¯„å®¡æ„è§ä¿®è®¢æ–‡æœ¬ï¼ŒåŠ¡å¿…æ¶ˆé™¤OOCä¸è®¾å®š/å†å²å†²çªï¼Œä¿ç•™èŠ‚æ‹ç›®æ ‡ä¸é£æ ¼æŠ¤æ ã€‚"
#     #             f"ã€é£æ ¼æŠ¤æ ã€‘{guard.get('result','')}\nã€è¯„å®¡ã€‘{json.dumps(issues[-1], ensure_ascii=False)[:1500]}"
#     #         ))
#     #         user2 = HumanMessage(content=(
#     #             f"è¯·åœ¨ä¸è¶…è¿‡{words_per_scene}è¯å†…é‡å†™è¯¥åœºæ™¯ï¼š\n{scene[:2000]}"
#     #         ))
#     #         rev = await llm_gen.ainvoke([sys2, user2])
#     #         scene = rev.content if hasattr(rev, "content") else str(rev)

#     #     # è¾¾åˆ°è¿­ä»£ä¸Šé™ä»æœ‰é—®é¢˜
#     #     return json.dumps({
#     #         "status": "BUDGET_EXCEEDED",
#     #         "outline": outline,
#     #         "beats": beats,
#     #         "final_text": scene,
#     #         "issues": issues
#     #     }, ensure_ascii=False)
#     #     # è¾¾åˆ°è¿­ä»£ä¸Šé™ä»æœ‰é—®é¢˜
#     #     return json.dumps({
#     #         "status": "BUDGET_EXCEEDED",
#     #         "outline": outline,
#     #         "beats": beats,
#     #         "final_text": scene,
#     #         "issues": issues
#     #     }, ensure_ascii=False)
#     tools = [
#         get_characters_tool,
#         get_relationships_tool,
#         get_important_locations_tool,
#         get_significant_event_tool,
#         background_knowledge_tool,
#         get_worldview_tool,
#         local_search_tool,
#         global_search_tool,
#         get_character_profile_tool,
#         get_main_theme_tool,
#         get_open_questions_tool,
#         get_causal_chains_tool,
#         style_guardrails_tool,
#         canon_alignment_tool,
#         contradiction_test_tool,
#         get_conflict_tool,
#         get_related_characters_tool,
#         imagine_conversation_tool,
#         extract_quotes_tool,
#         narrative_pov_tool,
#         get_motifs_symbols_tool, 
#         build_story_outline_tool,
#         emotion_curve_tool,
#         compare_characters_tool,
#         # continue_story_tool
#     ]

#     # åˆå§‹åŒ– LLM
#     # ç¡®ä¿ä½ å·²ç»è®¾ç½®äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡
#     llm = AzureChatOpenAI(
#         openai_api_version="2024-12-01-preview",
#         azure_deployment="gpt-4o",
#         model_name="gpt-4o",
#         azure_endpoint="https://tcamp.openai.azure.com/",
#         openai_api_key=api_key,
#         temperature=0.3,
#         streaming=True,
#         callbacks=[StreamingStdOutCallbackHandler()]
#     )
#     llm_gen = AzureChatOpenAI(
#         openai_api_version="2024-12-01-preview",
#         azure_deployment="gpt-4o",
#         model_name="gpt-4o",
#         azure_endpoint="https://tcamp.openai.azure.com/",
#         openai_api_key=api_key,
#         temperature=0.85,   # æ›´é«˜åˆ›é€ æ€§
#         max_tokens=1400     # ç»­å†™é•¿åº¦é¢„ç®—ï¼ˆæŒ‰éœ€è°ƒï¼‰
#     )


#     prompt = f"""
#     You are a helpful assistant that can answer questions about the data in the tables provided. Your tasks mainly consist of two parts: 1. extract and summarize the information about the book; 2. derivative work based on the book.

#     ---Goal---
# ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆ›ä½œåŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œä¿¡æ¯åˆ†æå’Œæ¢ç´¢ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒæŸ¥æ¥å®Œæˆå¤æ‚çš„åˆ›ä½œä»»åŠ¡ã€‚
# ## å†å²å¯¹è¯
# {{history}}
# ## ç”¨æˆ·é—®é¢˜
# {{input}}
# ## è°ƒæŸ¥å‘¨æœŸ (Investigation Cycle)
# ä½ æŒ‰ç…§ä¸€ä¸ªæŒç»­çš„å‘¨æœŸè¿ä½œï¼š
# 1. ä»å¤šä¸ªç»´åº¦ç†è§£ç”¨æˆ·è¯‰æ±‚ï¼Œæ‹†è§£ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç”¨æˆ·æ„å›¾
# 2. æ ¹æ®å†å²å¯¹è¯ï¼Œæ•´åˆæœ‰ç”¨ä¿¡æ¯ä»¥ç†è§£ä»»åŠ¡ç›®æ ‡
# 3. æ ¹æ®å·²æŒæ¡çš„çº¿ç´¢å’Œä¿¡æ¯ç¼ºå£ï¼Œé¿å…å’Œå†å²å¯¹è¯ä¸­å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨ï¼ˆå·¥å…·å‚æ•°ä¸€è‡´ï¼‰ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„å·¥å…·ï¼Œå†³å®šæ¥ä¸‹æ¥è¦è°ƒç”¨å“ªä¸ªå·¥å…·
# 4. å½“ä½ è®¤ä¸ºæ²¡å®Œæˆä»»åŠ¡æ—¶æˆ–ç°æœ‰ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "IN_PROGRES"ï¼Œæ­¤æ—¶ä½ å¿…é¡»é€‰æ‹©ä¸€ä¸ªå·¥å…·ã€‚
# 5. å½“ä½ è®¤ä¸ºå†å²å¯¹è¯çš„ä¿¡æ¯è¶³å¤Ÿä½ å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "DONE"
# ## å¯ç”¨å·¥å…· (Available Tools)
# {{functions}}
# ## å·¥å…·ä½¿ç”¨å‡†åˆ™ (Tool Usage Guidelines)
# {{guidelines}}
# ## æ³¨æ„äº‹é¡¹
# {{requirements}}
# å“åº”æ ¼å¼ (Response Format)
# {{response_format}}
#     """

#     prompt = ChatPromptTemplate.from_messages([
#         ("system", prompt),
#         ("user", "{input}\n\n{agent_scratchpad}"),
#     ])

#     # åˆ›å»º Agent
#     agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)

#     # åˆ›å»º Agent æ‰§è¡Œå™¨
#     return AgentExecutor(agent=agent, tools=tools, verbose=True)

# # --- ä¸»ç¨‹åºå…¥å£ ---
# async def main() -> None:
#     graph_agent = GraphAnalysisAgent()

#     # ä½¿ç”¨è¿™ä¸ªå®ä¾‹åˆ›å»º LangChain Agent
#     agent_executor = create_graphrag_agent(graph_agent)

#     print("LangChain Agent with GraphRAG (Python API) tools is ready. Type 'exit' to quit.")

#     history = []

#     while True:
#         user_query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
#         history.append({"role": "user", "content": user_query})
#         recent_history = history[-4:]  # åªä¿ç•™æœ€è¿‘çš„4æ¡å†å²è®°å½•
#         history_text = ""
#         for msg in recent_history:
#             prefix = "ç”¨æˆ·ï¼š" if msg["role"] == "user" else "åŠ©æ‰‹ï¼š"
#             history_text += f"{prefix}{msg['content']}\n"
#         if user_query.lower() == 'exit':
#             break

#         try:
#             # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ŒåŒ¹é…å¼‚æ­¥å·¥å…·
#             # response = await agent_executor.ainvoke({"input": user_query, "guidelines": prompt_utils.build_guidelines(), "functions": agent_executor.tools, "requirements": prompt_utils.build_requirements(), "response_format": prompt_utils.build_response_format()})
#             response = await agent_executor.ainvoke({"input": user_query, "guidelines": prompt.build_guidelines(), "functions": agent_executor.tools, "requirements": prompt.build_requirements(), "response_format": prompt.build_response_format(), "history": history_text})
#             # print("\n--- Agent å›ç­” ---")
#             # print(response.get("output"))
#             # print("--------------------\n")
#             # history.append({"role": "assistant", "content": response.get("output")})
#         except Exception as e:
#             print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
#             break


# if __name__ == "__main__":
#     asyncio.run(main())


import os
import json
import asyncio
from typing import Dict, Any

# ç¡®ä¿ä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“
# pip install langchain langchain-openai

# æ³¨æ„é…ç½®OPENAI_API_KEYä»¥åŠgraphragæ‰€åœ¨è·¯å¾„(ä»£ç ç¬¬172è¡Œ)

from dotenv import load_dotenv

load_dotenv()

# ä¼˜å…ˆè¯»å– OPENAI_API_KEYï¼Œå…¶æ¬¡ AZURE_OPENAI_API_KEYï¼Œä¸è¦æŠŠå¯†é’¥å½“ä½œç¯å¢ƒå˜é‡å
api_key =os.getenv("AZURE_OPENAI_API_KEY") or ""


from langchain.agents import tool
from langchain.agents import create_react_agent, AgentExecutor, create_tool_calling_agent
from langchain import hub
from langchain_openai import ChatOpenAI,AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from search.rag_engine import rag_engine
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import prompt_utils
import prompt

class GraphAnalysisAgent:
    def __init__(self):
        self.rag_engine = rag_engine
        
    async def global_search_retrieve_async(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        return await self.rag_engine.global_search_retrieve(query)
    
    async def global_search_generate_async(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        return await self.rag_engine.global_search_generate(query, retrieved_context)
    
    async def global_search_full_async(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        return await self.rag_engine.global_search_full(query)
    
    async def local_search_retrieve_async(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        return await self.rag_engine.local_search_retrieve(query)
    
    async def local_search_generate_async(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        return await self.rag_engine.local_search_generate(query, retrieved_context)
    
    async def local_search_full_async(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        return await self.rag_engine.local_search_full(query)

    async def get_characters_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ—å‡ºæ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²")

    async def get_relationships_async(self, p1: str, p2: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"åˆ†æ{p1}å’Œ{p2}ä¹‹é—´çš„å…³ç³»")

    async def get_important_locations_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹å’Œåœºæ™¯")

    async def background_knowledge_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†")
    
    async def get_worldview_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®š")

    async def get_character_profile_async(self, character_name: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–{character_name}çš„è¯¦ç»†ä¿¡æ¯")
    
    async def get_significant_event_async(self, event_name:str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–äº‹ä»¶{event_name}çš„è¯¦ç»†ä¿¡æ¯")
    
    async def get_main_theme_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹çš„ä¸»é¢˜")
    async def mock_coversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æ¨¡æ‹Ÿ{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
    async def get_open_questions_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("æœ¬ä¹¦æœ‰ä»€ä¹ˆæ‚¬å¿µæˆ–è€…æ²¡æœ‰è§£å†³çš„ä¼ç¬”ï¼Ÿ")
    async def get_conflict_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("ç½—åˆ—å‡ºæœ¬ä¹¦æœ€å¤§çš„å†²çªæ˜¯ä»€ä¹ˆ")
    async def get_related_characters_async(self, event: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–{event}äº‹ä»¶çš„å…³è”äººç‰©")
    async def get_causal_chains_async(self, event: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è·å–{event}äº‹ä»¶çš„å› æœé“¾ï¼šå‰ç½®æ¡ä»¶â†’è§¦å‘â†’ç»“æœâ†’åæœ")
    async def style_guardrails_async(self, persona: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚")
    async def canon_alignment_async(self, text: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}")
    async def contradiction_test_async(self, text: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}")
    async def continue_story_async(self, brief: str, persona: str = "ä¿æŒä¸åŸè‘—ä¸€è‡´çš„å™è¿°è€…å£å»ä¸è§’è‰²å¯¹ç™½é£æ ¼", target_style: str = "ç´§å‡‘ã€å…·è±¡ç»†èŠ‚ã€å¯¹ç™½æ¨åŠ¨å‰§æƒ…", words_per_scene: int = 600, max_iters: int = 2) -> Dict[str, Any]:
        return await self.local_search_full_async(f"ä¸ºä»¥ä¸‹å¤§çº²ç»­å†™ä¸€ä¸ªåœºæ™¯ï¼ˆä¸è¶…è¿‡{words_per_scene}è¯ï¼‰ï¼š{brief[:3000]}")
    async def imagine_conversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æƒ³è±¡{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
    async def extract_quotes_async(self, name:str, n:int=8) -> Dict[str, Any]:
        q = f"åˆ—å‡º{name}æœ€å…·ä»£è¡¨æ€§çš„å°è¯{n}æ¡ï¼ˆæ¯æ¡<=40å­—ï¼Œé™„ç« èŠ‚/æ®µè½ç¼–å·ï¼‰ï¼Œä¸¥æ ¼JSONæ•°ç»„ï¼š"
        return await self.local_search_full_async(q)
    async def narrative_pov_async(self) -> Dict[str, Any]:
        q = """
    åˆ†æå™äº‹è§†è§’ä¸å¯é æ€§ï¼šPOVç±»å‹ã€åˆ‡æ¢ç‚¹ã€å¯èƒ½åè§/è¯¯å¯¼çš„è¯æ®ã€‚ç”¨åˆ†ç‚¹åˆ—å‡ºï¼Œæ¯ç‚¹é™„<=40å­—çŸ­æ‘˜+ç« èŠ‚ã€‚
    """
        return await self.global_search_full_async(q)
    async def get_motifs_symbols_async(self, max_items:int=20) -> Dict[str, Any]:
        q = f"""
    æŠ½å–æ„è±¡/æ¯é¢˜/è±¡å¾ï¼ˆæœ€å¤š{max_items}æ¡ï¼‰ï¼Œä¸¥æ ¼JSONï¼š
    [{{"motif":"â€¦","meaning":"â€¦","linked_themes":["â€¦"],"chapters":["â€¦"],"evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}]
    """
        return await self.local_search_full_async(q)
    async def build_story_outline_async(self, brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> Dict[str, Any]:
        q = f"""
    åŸºäºåŸè‘—çº¦æŸï¼Œä¸º"{brief}"ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚
    é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚
    """
        return await self.global_search_full_async(q)
    async def emotion_curve_async(self, scope:str="å…¨ä¹¦") -> Dict[str, Any]:
        q = f"æå–{scope}çš„æƒ…æ„Ÿæ›²çº¿å…³é”®è½¬æŠ˜ï¼ˆå–œ/æ€’/å“€/æƒ§/æƒŠ/åŒ/ä¿¡ï¼‰ï¼Œåˆ—å‡ºè½¬æŠ˜ç‚¹ç« èŠ‚ä¸è§¦å‘äº‹ä»¶ï¼Œå„ç»™<=40å­—çŸ­æ‘˜ã€‚"
        return await self.global_search_full_async(q)
    async def compare_characters_async(self, a:str, b:str) -> Dict[str, Any]:
        q = f"""
    æ¯”è¾ƒ{a}ä¸{b}ï¼Œä¸¥æ ¼JSONï¼š
    {{"values":["â€¦"],"goals":["â€¦"],"methods":["â€¦"],"red_lines":["â€¦"],"decision_style":"å†²åŠ¨|è°¨æ…|ç®—è®¡","evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}
    """
        return await self.global_search_full_async(q)
    async def get_people_location_relation_async(self, people:str, location:str, relation:str) -> Dict[str, Any]:
        q = f"""
    åˆ†æ{people}å’Œ{location}ä¹‹é—´çš„å…³ç³»ï¼Œä¸¥æ ¼JSONï¼š
    {{"values":["â€¦"],"goals":["â€¦"],"methods":["â€¦"],"red_lines":["â€¦"],"decision_style":"å†²åŠ¨|è°¨æ…|ç®—è®¡","evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}
    """
        return await self.global_search_full_async(q)

# --- ç¬¬äºŒæ­¥ï¼šåˆ›å»º LangChain Agent ---
def create_graphrag_agent(graphrag_agent_instance: GraphAnalysisAgent) -> AgentExecutor:
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå¯ä»¥è°ƒç”¨ GraphRAG å‘½ä»¤è¡ŒåŠŸèƒ½çš„ LangChain Agentã€‚
    """
    # ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œå°† GraphAnalysisAgent çš„æ–¹æ³•åŒ…è£…æˆ LangChain å·¥å…·
    # æ³¨æ„ï¼šè¿™é‡Œçš„å·¥å…·å‡½æ•°éœ€è¦èƒ½å¤Ÿè¢« Agent ç›´æ¥è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨é—­åŒ…æ¥ä¼ é€’å®ä¾‹
    
    # === æ–°å¢ï¼šRAGæ£€ç´¢åˆ†ç¦»å·¥å…· ===
    @tool
    async def global_search_retrieve_tool(query: str) -> str:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›çš„å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°GraphRAGæ£€ç´¢åˆ°äº†å“ªäº›ç›¸å…³æ–‡æ¡£å’Œä¸Šä¸‹æ–‡ï¼Œä½†ä¸è¿›è¡ŒLLMç”Ÿæˆã€‚"""
        result = await graphrag_agent_instance.global_search_retrieve_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def global_search_generate_tool(query: str, retrieved_context: str) -> str:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆã€‚éœ€è¦å…ˆè°ƒç”¨global_search_retrieve_toolè·å–ä¸Šä¸‹æ–‡ã€‚"""
        try:
            print(f"ğŸ¤– [Agent LLM] æ­£åœ¨ä½¿ç”¨agentçš„LLMå·¥å…·ç”Ÿæˆå…¨å±€æœç´¢å›ç­”: {query}")
            
            # è§£ææ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ - å¢åŠ æ›´å¥å£®çš„é”™è¯¯å¤„ç†
            import json
            try:
                if isinstance(retrieved_context, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    context_data = json.loads(retrieved_context)
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    context_data = retrieved_context
            except json.JSONDecodeError as e:
                print(f"â„¹ï¸ [Agent LLM] æ£€æµ‹åˆ°éJSONæ ¼å¼æ•°æ®ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                context_text = str(retrieved_context)
            else:
                # JSONè§£ææˆåŠŸï¼Œæå–ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_text = context_data.get('retrieved_context', {}).get('context_text', '')
                if not context_text:
                    # å¦‚æœåµŒå¥—ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®
                    context_text = context_data.get('context_text', str(context_data))
            
            # æ„å»ºæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å…·ä½“è¯¦ç»†ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®
2. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
3. å¦‚æœæ¶‰åŠäººç‰©ï¼Œè¦è¯´æ˜ä»–ä»¬çš„è§’è‰²ã€ç‰¹ç‚¹å’Œé‡è¦æ€§
4. å¦‚æœæ¶‰åŠäº‹ä»¶ï¼Œè¦æè¿°å…¶èƒŒæ™¯ã€è¿‡ç¨‹å’Œå½±å“
5. å›ç­”é•¿åº¦åº”è¯¥åœ¨200-500å­—ä¹‹é—´ï¼Œç¡®ä¿ä¿¡æ¯å……åˆ†ä½†ä¸è¿‡äºå†—é•¿"""
            
            # ä½¿ç”¨agentçš„LLMå·¥å…·è¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [Agent LLM] å…¨å±€æœç´¢ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè€Œä¸æ˜¯JSONæ ¼å¼
            return result_text
            
        except Exception as e:
            print(f"âŒ [Agent LLM] å…¨å±€æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "agent_global_generate",
                "query": query,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_retrieve_tool(query: str) -> str:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›çš„å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°GraphRAGæ£€ç´¢åˆ°äº†å“ªäº›ç›¸å…³æ–‡æ¡£å’Œä¸Šä¸‹æ–‡ï¼Œä½†ä¸è¿›è¡ŒLLMç”Ÿæˆã€‚"""
        result = await graphrag_agent_instance.local_search_retrieve_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_generate_tool(query: str, retrieved_context: str) -> str:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆã€‚éœ€è¦å…ˆè°ƒç”¨local_search_retrieve_toolè·å–ä¸Šä¸‹æ–‡ã€‚"""
        try:
            print(f"ğŸ¤– [Agent LLM] æ­£åœ¨ä½¿ç”¨agentçš„LLMå·¥å…·ç”Ÿæˆå±€éƒ¨æœç´¢å›ç­”: {query}")
            
            # è§£ææ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ - å¢åŠ æ›´å¥å£®çš„é”™è¯¯å¤„ç†
            import json
            try:
                if isinstance(retrieved_context, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    context_data = json.loads(retrieved_context)
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    context_data = retrieved_context
            except json.JSONDecodeError as e:
                print(f"â„¹ï¸ [Agent LLM] æ£€æµ‹åˆ°éJSONæ ¼å¼æ•°æ®ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                context_text = str(retrieved_context)
            else:
                # JSONè§£ææˆåŠŸï¼Œæå–ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_text = context_data.get('retrieved_context', {}).get('context_text', '')
                if not context_text:
                    # å¦‚æœåµŒå¥—ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®
                    context_text = context_data.get('context_text', str(context_data))
            
            # æ„å»ºæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å…·ä½“è¯¦ç»†ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®
2. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
3. å¦‚æœæ¶‰åŠäººç‰©ï¼Œè¦è¯´æ˜ä»–ä»¬çš„è§’è‰²ã€ç‰¹ç‚¹å’Œé‡è¦æ€§
4. å¦‚æœæ¶‰åŠäº‹ä»¶ï¼Œè¦æè¿°å…¶èƒŒæ™¯ã€è¿‡ç¨‹å’Œå½±å“
5. å›ç­”é•¿åº¦åº”è¯¥åœ¨200-500å­—ä¹‹é—´ï¼Œç¡®ä¿ä¿¡æ¯å……åˆ†ä½†ä¸è¿‡äºå†—é•¿"""
            
            # ä½¿ç”¨agentçš„LLMå·¥å…·è¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [Agent LLM] å±€éƒ¨æœç´¢ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè€Œä¸æ˜¯JSONæ ¼å¼
            return result_text
            
        except Exception as e:
            print(f"âŒ [Agent LLM] å±€éƒ¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "agent_local_generate",
                "query": query,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    # === æ–°å¢ï¼šç‹¬ç«‹LLMè°ƒç”¨å·¥å…· ===
    @tool
    async def llm_generate_tool(prompt: str, context: str = "") -> str:
        """ç‹¬ç«‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚è¾“å…¥promptæ˜¯ç”Ÿæˆæç¤ºï¼Œcontextæ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°LLMæ­£åœ¨ç”Ÿæˆå†…å®¹ã€‚"""
        try:
            print(f"ğŸ¤– [ç‹¬ç«‹LLM] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
            print(f"æç¤º: {prompt[:100]}...")
            if context:
                print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # ä½¿ç”¨llm_genè¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = []
            if context:
                messages.append(SystemMessage(content=f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ï¼š\n\n{context}"))
            messages.append(HumanMessage(content=prompt))
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [ç‹¬ç«‹LLM] ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            return json.dumps({
                "method": "llm_generate",
                "prompt": prompt,
                "context_length": len(context) if context else 0,
                "response": result_text,
                "success": True
            }, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âŒ [ç‹¬ç«‹LLM] ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "llm_generate",
                "prompt": prompt,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def llm_analyze_tool(text: str, analysis_type: str = "general") -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬ã€‚è¾“å…¥textæ˜¯è¦åˆ†æçš„æ–‡æœ¬ï¼Œanalysis_typeæ˜¯åˆ†æç±»å‹ï¼ˆå¦‚'character', 'theme', 'plot'ç­‰ï¼‰ã€‚"""
        try:
            print(f"ğŸ¤– [LLMåˆ†æ] æ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬...")
            print(f"åˆ†æç±»å‹: {analysis_type}")
            print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # æ ¹æ®åˆ†æç±»å‹æ„å»ºæç¤º
            if analysis_type == "character":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­çš„äººç‰©ç‰¹å¾ã€æ€§æ ¼ã€åŠ¨æœºç­‰ï¼š\n\n{text}"
            elif analysis_type == "theme":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»é¢˜ã€è±¡å¾æ„ä¹‰ã€æ·±å±‚å«ä¹‰ï¼š\n\n{text}"
            elif analysis_type == "plot":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…èŠ‚å‘å±•ã€å†²çªã€è½¬æŠ˜ç‚¹ï¼š\n\n{text}"
            else:
                prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œ{analysis_type}åˆ†æï¼š\n\n{text}"
            
            from langchain_core.messages import HumanMessage
            response = await llm_gen.ainvoke([HumanMessage(content=prompt)])
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [LLMåˆ†æ] åˆ†æå®Œæˆï¼Œç»“æœé•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            return json.dumps({
                "method": "llm_analyze",
                "analysis_type": analysis_type,
                "text_length": len(text),
                "analysis_result": result_text,
                "success": True
            }, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âŒ [LLMåˆ†æ] åˆ†æå¤±è´¥: {e}")
            return json.dumps({
                "method": "llm_analyze",
                "analysis_type": analysis_type,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    # === åŸæœ‰å·¥å…· ===
    @tool
    async def get_characters_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²ã€‚"""
        result = await graphrag_agent_instance.get_characters_async()
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_relationships_tool(p1: str, p2: str) -> str:
        """è·å–ä¸¤ä¸ªç‰¹å®šäººç‰©ä¹‹é—´çš„å…³ç³»ã€‚è¾“å…¥å‚æ•°p1å’Œp2æ˜¯äººç‰©åç§°ã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸¤ä¸ªäººç‰©çš„å…³ç³»ï¼Œå¯ä»¥å°è¯•å•ç‹¬æŸ¥è¯¢ä¸¤ä¸ªäººç‰©çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå¹¶ä¸”å°è¯•æ‰¾åˆ°å’Œä»–ä»¬å…±åŒç›¸å…³çš„äººæ¥åˆ¤æ–­ä»–ä»¬ä¹‹é—´å¯èƒ½çš„å…³ç³»"""
        result = await graphrag_agent_instance.get_relationships_async(p1, p2)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_important_locations_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹ã€‚"""
        result = await graphrag_agent_instance.get_important_locations_async()
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def background_knowledge_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†ã€‚"""
        result = await graphrag_agent_instance.background_knowledge_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def get_worldview_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®šã€‚"""
        result = await graphrag_agent_instance.get_worldview_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_tool(query: str) -> str:
        """ä½¿ç”¨ GraphRAG çš„å±€éƒ¨æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
        result = await graphrag_agent_instance.local_search_full_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def global_search_tool(query: str) -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
        result = await graphrag_agent_instance.global_search_full_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool 
    async def get_character_profile_tool(character_name: str) -> str:
        """è·å–ç‰¹å®šäººç‰©çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°character_nameæ˜¯äººç‰©åç§°ã€‚"""
        result = await graphrag_agent_instance.get_character_profile_async(character_name)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_significant_event_tool(event_name: str) -> str:
        """è·å–ç‰¹å®šäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°event_nameæ˜¯äº‹ä»¶åç§°ã€‚"""
        result = await graphrag_agent_instance.get_significant_event_async(event_name)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_main_theme_tool() -> str:
        """è·å–æ•…äº‹çš„ä¸»é¢˜ã€‚"""
        result = await graphrag_agent_instance.get_main_theme_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool 
    async def get_open_questions_tool() -> str:
        """è·å–æœ¬ä¹¦çš„æ‚¬å¿µæˆ–è€…æœªè§£å†³çš„ä¼ç¬”ã€‚"""
        result = await graphrag_agent_instance.get_open_questions_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_causal_chains_tool(event: str) -> str:
        """è·å–ç»™å®šäº‹ä»¶çš„å› æœé“¾ã€‚å¯ä»¥çŸ¥é“æ˜¯ä»€ä¹ˆå¯¼è‡´çš„è¯¥äº‹ä»¶ï¼Œç„¶åè¯¥äº‹ä»¶å¯¼è‡´äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼Œæœ€åç»“æœåˆå¯¼è‡´äº†ä»€ä¹ˆæ ·çš„åæœ"""
        result = await graphrag_agent_instance.get_causal_chains_async(event)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def style_guardrails_tool(persona: str) -> str:
        """äº§å‡ºé£æ ¼æŠ¤æ ï¼šå…è®¸/ç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€è§†è§’ã€èŠ‚å¥ç­‰ï¼ˆä¾›ç»­å†™éµå®ˆï¼‰"""
        q = f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚"
        res = await graphrag_agent_instance.global_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)

    @tool
    async def canon_alignment_tool(text: str) -> str:
        """è¯„ä¼°æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™ä¸€è‡´æ€§ï¼ˆè§’è‰²OOC/è®¾å®šè¿èƒŒ/å†å²è¿èƒŒï¼‰ï¼Œç»™è¦ç‚¹ä¸ä¾æ®"""
        q = f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}"
        res = await graphrag_agent_instance.local_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)

    @tool
    async def contradiction_test_tool(text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼Œç»™å‡ºåŸæ–‡è¯æ®ç‰‡æ®µå®šä½"""
        q = f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}"
        res = await graphrag_agent_instance.local_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)
    @tool
    async def get_conflict_tool() -> str:
        """è·å–æœ¬ä¹¦æœ€å¤§çš„å†²çªã€‚"""
        result = await graphrag_agent_instance.get_conflict_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_related_characters_tool(event: str) -> str:
        """è·å–ç»™å®šäº‹ä»¶çš„å…³è”äººç‰©ã€‚"""
        result = await graphrag_agent_instance.get_related_characters_async(event)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def imagine_conversation_tool(character1_name: str, character2_name: str) -> str:
        """æƒ³è±¡ä¸¤ä¸ªè§’è‰²ä¹‹é—´çš„å¯¹è¯ã€‚"""
        result = await graphrag_agent_instance.imagine_conversation_async(character1_name, character2_name)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def extract_quotes_tool(name:str, n:int=8) -> str:
        """è·å–ç‰¹å®šäººç‰©çš„å°è¯ã€‚"""
        result = await graphrag_agent_instance.extract_quotes_async(name, n)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def narrative_pov_tool() -> str:
        """è·å–æœ¬ä¹¦çš„å™äº‹è§†è§’ã€‚"""
        result = await graphrag_agent_instance.narrative_pov_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_motifs_symbols_tool(max_items:int=20) -> str:
        """è·å–æœ¬ä¹¦çš„æ„è±¡/æ¯é¢˜/è±¡å¾ã€‚"""
        result = await graphrag_agent_instance.get_motifs_symbols_async(max_items)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def build_story_outline_tool(brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> str:
        """åŸºäºåŸè‘—çº¦æŸï¼Œä¸º"{brief}"ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚"""
        result = await graphrag_agent_instance.build_story_outline_async(brief, target_style)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def emotion_curve_tool(scope:str="å…¨ä¹¦") -> str:
        """è·å–æœ¬ä¹¦çš„æƒ…æ„Ÿæ›²çº¿ã€‚"""
        result = await graphrag_agent_instance.emotion_curve_async(scope)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def compare_characters_tool(a:str, b:str) -> str:
        """æ¯”è¾ƒä¸¤ä¸ªè§’è‰²ã€‚"""
        result = await graphrag_agent_instance.compare_characters_async(a, b)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def system_status_tool() -> str:
        """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯ç”¨å·¥å…·ã€å¤„ç†èƒ½åŠ›ç­‰"""
        return json.dumps({
            "system_status": "running",
            "available_tools": [
                "global_search_tool", "local_search_tool", "get_characters_tool",
                "get_relationships_tool", "background_knowledge_tool", "llm_generate_tool",
                "llm_analyze_tool", "get_character_profile_tool", "get_worldview_tool"
            ],
            "capabilities": [
                "äººç‰©åˆ†æ", "å…³ç³»åˆ†æ", "èƒŒæ™¯çŸ¥è¯†æŸ¥è¯¢", "æƒ…èŠ‚åˆ†æ", "æ–‡æœ¬ç”Ÿæˆ", "åˆ›æ„å†™ä½œ"
            ],
            "specialization": "ã€Šæ²™ä¸˜ã€‹(Dune)ç³»åˆ—å°è¯´åˆ†æ",
            "note": "ç³»ç»Ÿå·²ä¼˜åŒ–ï¼Œæ”¯æŒè¯¦ç»†å›ç­”å’Œç”¨æˆ·å‹å¥½çš„çŠ¶æ€æç¤º"
        }, ensure_ascii=False, default=str)
    @tool 
    async def get_people_location_relation_tool(people:str, location:str, relation:str) -> str:
        """è·å–ç‰¹å®šäººç‰©å’Œåœ°ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚"""
        result = await graphrag_agent_instance.get_people_location_relation_async(people, location, relation)
        return json.dumps(result, ensure_ascii=False, default=str)
    tools = [
        # === æ–°å¢çš„RAGæ£€ç´¢åˆ†ç¦»å·¥å…· ===
        global_search_retrieve_tool,
        global_search_generate_tool,
        local_search_retrieve_tool,
        local_search_generate_tool,
        
        # === æ–°å¢ï¼šç‹¬ç«‹LLMè°ƒç”¨å·¥å…· ===
        # llm_generate_tool,
        # llm_analyze_tool,
        
        # === åŸæœ‰å·¥å…· ===
        get_characters_tool,
        get_relationships_tool,
        get_important_locations_tool,
        get_significant_event_tool,
        background_knowledge_tool,
        get_worldview_tool,
        local_search_tool,
        global_search_tool,
        get_character_profile_tool,
        get_main_theme_tool,
        get_open_questions_tool,
        get_causal_chains_tool,
        style_guardrails_tool,
        canon_alignment_tool,
        contradiction_test_tool,
        get_conflict_tool,
        get_related_characters_tool,
        imagine_conversation_tool,
        extract_quotes_tool,
        narrative_pov_tool,
        get_motifs_symbols_tool, 
        build_story_outline_tool,
        emotion_curve_tool,
        compare_characters_tool,
        system_status_tool,
        get_people_location_relation_tool,
    ]

    # åˆå§‹åŒ– LLM
    # ç¡®ä¿ä½ å·²ç»è®¾ç½®äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡
    llm = AzureChatOpenAI(
        openai_api_version="2024-12-01-preview",
        azure_deployment="gpt-4o",
        model_name="gpt-4o",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.3,
        max_tokens=2000,  # ä»800å¢åŠ åˆ°2000
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    llm_gen = AzureChatOpenAI(
        openai_api_version="2024-12-01-preview",
        azure_deployment="gpt-4o",
        model_name="gpt-4o",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.85,   # æ›´é«˜åˆ›é€ æ€§
        max_tokens=2000     # ä»1000å¢åŠ åˆ°2000
    )


    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆ›ä½œåŠ©æ‰‹ï¼Œä¸“é—¨åˆ†æã€Šæ²™ä¸˜ã€‹(Dune)ç³»åˆ—å°è¯´ï¼Œå¯ä»¥è¿›è¡Œä¿¡æ¯åˆ†æå’Œæ¢ç´¢ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒæŸ¥æ¥å®Œæˆå¤æ‚çš„åˆ›ä½œä»»åŠ¡ã€‚

## é‡è¦è¯´æ˜ï¼šRAGæ£€ç´¢åˆ†ç¦»å·¥å…·å’Œç‹¬ç«‹LLMè°ƒç”¨
ç°åœ¨ä½ æœ‰æ–°çš„å·¥å…·å¯ä»¥åˆ†ç¦»RAGçš„æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥åŠç‹¬ç«‹çš„LLMè°ƒç”¨ï¼š

### RAGæ£€ç´¢åˆ†ç¦»å·¥å…·ï¼š
- **global_search_retrieve_tool**: ä»…è¿›è¡Œå…¨å±€æœç´¢çš„æ£€ç´¢ï¼Œå±•ç¤ºGraphRAGå¬å›çš„å†…å®¹
- **global_search_generate_tool**: ä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆ
- **local_search_retrieve_tool**: ä»…è¿›è¡Œå±€éƒ¨æœç´¢çš„æ£€ç´¢ï¼Œå±•ç¤ºGraphRAGå¬å›çš„å†…å®¹  
- **local_search_generate_tool**: ä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆ

### ç‹¬ç«‹LLMè°ƒç”¨å·¥å…·ï¼š
- **llm_generate_tool**: ç‹¬ç«‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œç”¨æˆ·å¯ä»¥æ¸…æ¥šçœ‹åˆ°LLMæ­£åœ¨ç”Ÿæˆå†…å®¹
- **llm_analyze_tool**: ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬ï¼Œæ”¯æŒä¸åŒç±»å‹çš„åˆ†æï¼ˆcharacter, theme, plotç­‰ï¼‰

### ä½¿ç”¨å»ºè®®ï¼š
1. **å±•ç¤ºRAGè¿‡ç¨‹**ï¼šå…ˆè°ƒç”¨ *_retrieve_tool å±•ç¤ºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Œå†è°ƒç”¨ *_generate_tool è¿›è¡ŒLLMç”Ÿæˆ
2. **ç‹¬ç«‹LLMè°ƒç”¨**ï¼šå½“éœ€è¦åˆ›é€ æ€§å†…å®¹æˆ–å¤æ‚åˆ†ææ—¶ï¼Œä½¿ç”¨ llm_generate_tool æˆ– llm_analyze_tool
3. **å®Œæ•´æµç¨‹**ï¼šæˆ–è€…ç›´æ¥ä½¿ç”¨åŸæœ‰çš„å®Œæ•´å·¥å…·ï¼ˆå¦‚ global_search_toolï¼‰

### ç”¨æˆ·å¯è§æ€§ï¼š
- ğŸ” [RAGæ£€ç´¢] è¡¨ç¤ºæ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯
- ğŸ¤– [LLMç”Ÿæˆ] è¡¨ç¤ºæ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹
- âœ… è¡¨ç¤ºæ“ä½œå®Œæˆ
- âŒ è¡¨ç¤ºæ“ä½œå¤±è´¥

### æ™ºèƒ½å†³ç­–æŒ‡å—ï¼š
- **äººç‰©ç›¸å…³é—®é¢˜**ï¼šä¼˜å…ˆä½¿ç”¨ get_character_profile_tool æˆ– get_characters_tool
- **å…³ç³»åˆ†æ**ï¼šä½¿ç”¨ get_relationships_tool åˆ†æäººç‰©å…³ç³»
- **èƒŒæ™¯çŸ¥è¯†**ï¼šä½¿ç”¨ background_knowledge_tool æˆ– get_worldview_tool
- **æƒ…èŠ‚åˆ†æ**ï¼šä½¿ç”¨ global_search_tool è¿›è¡Œå…¨å±€åˆ†æ
- **å…·ä½“ç»†èŠ‚**ï¼šä½¿ç”¨ local_search_tool è¿›è¡Œç²¾ç¡®æ£€ç´¢
- **åˆ›ä½œä»»åŠ¡**ï¼šä½¿ç”¨ llm_generate_tool è¿›è¡Œåˆ›é€ æ€§ç”Ÿæˆ

## å†å²å¯¹è¯
{{history}}
## ç”¨æˆ·é—®é¢˜
{{input}}
## è°ƒæŸ¥å‘¨æœŸ (Investigation Cycle)
ä½ æŒ‰ç…§ä¸€ä¸ªæŒç»­çš„å‘¨æœŸè¿ä½œï¼š
1. ä»å¤šä¸ªç»´åº¦ç†è§£ç”¨æˆ·è¯‰æ±‚ï¼Œæ‹†è§£ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç”¨æˆ·æ„å›¾
2. æ ¹æ®å†å²å¯¹è¯ï¼Œæ•´åˆæœ‰ç”¨ä¿¡æ¯ä»¥ç†è§£ä»»åŠ¡ç›®æ ‡
3. æ ¹æ®å·²æŒæ¡çš„çº¿ç´¢å’Œä¿¡æ¯ç¼ºå£ï¼Œé¿å…å’Œå†å²å¯¹è¯ä¸­å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨ï¼ˆå·¥å…·å‚æ•°ä¸€è‡´ï¼‰ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„å·¥å…·ï¼Œå†³å®šæ¥ä¸‹æ¥è¦è°ƒç”¨å“ªä¸ªå·¥å…·
4. å½“ä½ è®¤ä¸ºæ²¡å®Œæˆä»»åŠ¡æ—¶æˆ–ç°æœ‰ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "IN_PROGRES"ï¼Œæ­¤æ—¶ä½ å¿…é¡»é€‰æ‹©ä¸€ä¸ªå·¥å…·ã€‚
5. å½“ä½ è®¤ä¸ºå†å²å¯¹è¯çš„ä¿¡æ¯è¶³å¤Ÿä½ å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "DONE"
## å¯ç”¨å·¥å…· (Available Tools)
{{functions}}
## å·¥å…·ä½¿ç”¨å‡†åˆ™ (Tool Usage Guidelines)
{{guidelines}}
## æ³¨æ„äº‹é¡¹
{{requirements}}
å“åº”æ ¼å¼ (Response Format)
{{response_format}}
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt),
        ("user", "{input}\n\n{agent_scratchpad}"),
    ])

    # åˆ›å»º Agent
    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)

    # åˆ›å»º Agent æ‰§è¡Œå™¨
    return AgentExecutor(agent=agent, tools=tools, verbose=True)

# --- ä¸»ç¨‹åºå…¥å£ ---
async def main() -> None:
    graph_agent = GraphAnalysisAgent()

    # ä½¿ç”¨è¿™ä¸ªå®ä¾‹åˆ›å»º LangChain Agent
    agent_executor = create_graphrag_agent(graph_agent)

    print("=" * 60)
    print("ğŸ¤– ã€Šæ²™ä¸˜ã€‹æ™ºèƒ½åˆ†æåŠ©æ‰‹å·²å¯åŠ¨")
    print("=" * 60)
    print("ğŸ“š ä¸“ç²¾ï¼šã€Šæ²™ä¸˜ã€‹(Dune)ç³»åˆ—å°è¯´åˆ†æ")
    print("ğŸ”§ åŠŸèƒ½ï¼šäººç‰©åˆ†æã€å…³ç³»åˆ†æã€èƒŒæ™¯çŸ¥è¯†ã€æƒ…èŠ‚åˆ†æã€åˆ›æ„å†™ä½œ")
    print("ğŸ’¡ æç¤ºï¼šè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œè¾“å…¥ 'exit' é€€å‡º")
    print("=" * 60)
    history = []
    
    while True:
        user_query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
        history.append({"role": "user", "content": user_query})
        recent_history = history[-4:]  # åªä¿ç•™æœ€è¿‘çš„4æ¡å†å²è®°å½•
        history_text = ""
        for msg in recent_history:
            prefix = "ç”¨æˆ·ï¼š" if msg["role"] == "user" else "åŠ©æ‰‹ï¼š"
            history_text += f"{prefix}{msg['content']}\n"
        if user_query.lower() == 'exit':
            break
        elif user_query.lower() == 'help':
            print("\n" + "=" * 60)
            print("ğŸ“– ã€Šæ²™ä¸˜ã€‹æ™ºèƒ½åˆ†æåŠ©æ‰‹ - ä½¿ç”¨å¸®åŠ©")
            print("=" * 60)
            print("ğŸ¯ ä¸»è¦åŠŸèƒ½ï¼š")
            print("  â€¢ äººç‰©åˆ†æï¼šæŸ¥è¯¢è§’è‰²èƒŒæ™¯ã€æ€§æ ¼ã€åŠ¨æœº")
            print("  â€¢ å…³ç³»åˆ†æï¼šåˆ†æäººç‰©ä¹‹é—´çš„å…³ç³»")
            print("  â€¢ èƒŒæ™¯çŸ¥è¯†ï¼šäº†è§£ä¸–ç•Œè§‚ã€è®¾å®šã€å†å²")
            print("  â€¢ æƒ…èŠ‚åˆ†æï¼šåˆ†ææ•…äº‹å‘å±•ã€å†²çªã€è½¬æŠ˜")
            print("  â€¢ åˆ›æ„å†™ä½œï¼šåŸºäºåŸè‘—è¿›è¡Œç»­å†™ã€å¯¹è¯ç”Ÿæˆ")
            print("\nğŸ’¬ ç¤ºä¾‹é—®é¢˜ï¼š")
            print("  â€¢ 'ä¿ç½—Â·é˜¿ç‰¹é›·å¾·æ–¯çš„æ€§æ ¼ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ'")
            print("  â€¢ 'ä¿ç½—å’Œæ°è¥¿å¡çš„å…³ç³»å¦‚ä½•ï¼Ÿ'")
            print("  â€¢ 'é¦™æ–™åœ¨æ²™ä¸˜ä¸–ç•Œä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ'")
            print("  â€¢ 'Bene Gesseritå§å¦¹ä¼šçš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ'")
            print("  â€¢ 'è¯·åˆ†ææ²™ä¸˜çš„ä¸»è¦å†²çª'")
            print("\nğŸ”§ ç³»ç»ŸçŠ¶æ€ï¼š")
            print("  â€¢ è¾“å…¥ 'status' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
            print("  â€¢ è¾“å…¥ 'exit' é€€å‡ºç¨‹åº")
            print("=" * 60)
            continue
        elif user_query.lower() == 'status':
            print("\nğŸ”§ æ­£åœ¨è·å–ç³»ç»ŸçŠ¶æ€...")
            try:
                status_response = await agent_executor.ainvoke({"input": "è¯·è°ƒç”¨system_status_toolè·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"})
                if status_response and status_response.get("output"):
                    print(status_response.get("output"))
                else:
                    print("âŒ æ— æ³•è·å–ç³»ç»ŸçŠ¶æ€")
            except Exception as e:
                print(f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥ï¼š{e}")
            continue
        
        try:
            print(f"\nğŸ¤– [Agentå¤„ç†] æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...")
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ŒåŒ¹é…å¼‚æ­¥å·¥å…·
            response = await agent_executor.ainvoke({"input": user_query, "guidelines": prompt.build_guidelines(), "functions": agent_executor.tools, "requirements": prompt.build_requirements(), "response_format": prompt.build_response_format(), "history": history_text})
            
            # æ˜¾ç¤ºAgentçš„å›ç­”
            if response and response.get("output"):
                print(f"\nğŸ“ [Agentå›ç­”]")
                print("=" * 50)
                print(response.get("output"))
                print("=" * 50)
                history.append({"role": "assistant", "content": response.get("output")})
            else:
                print("âŒ [é”™è¯¯] Agentæ²¡æœ‰è¿”å›æœ‰æ•ˆå›ç­”")
                
        except Exception as e:
            print(f"âŒ [é”™è¯¯] å‘ç”Ÿé”™è¯¯ï¼š{e}")
            break


if __name__ == "__main__":
    asyncio.run(main())