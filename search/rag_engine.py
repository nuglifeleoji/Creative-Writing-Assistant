import os
import asyncio
import pandas as pd
import tiktoken
from typing import Dict, Any, List, Optional

from graphrag.config.enums import ModelType
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.language_model.manager import ModelManager
from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
    read_indexer_relationships,
    read_indexer_text_units,
    read_indexer_covariates,
)
from graphrag.query.structured_search.global_search.community_context import (
    GlobalCommunityContext,
)
from graphrag.query.structured_search.global_search.search import GlobalSearch
from graphrag.query.structured_search.local_search.mixed_context import (
    LocalSearchMixedContext,
)
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from dotenv import load_dotenv

load_dotenv()

class RAGEngine:
    """GraphRAGå¼•æ“ï¼Œå°†æ£€ç´¢å’ŒLLMè°ƒç”¨åˆ†ç¦»"""
    
    def __init__(self):
        self.api_key = os.getenv("AZURE_OPENAI_API_KEY")
        self.embedding_key = os.getenv("Embedding_key")
        
        # åˆå§‹åŒ–é…ç½®
        self._init_configs()
        self._init_data()
        self._init_engines()
        
    def _init_configs(self):
        """åˆå§‹åŒ–LLMé…ç½®"""
        self.chat_config = LanguageModelConfig(
            type=ModelType.AzureOpenAIChat,
            api_base="https://tcamp.openai.azure.com/",
            api_version="2025-01-01-preview",
            auth_type="api_key",
            api_key=self.api_key,
            model="gpt-4o",
            deployment_name="gpt-4o",
            model_supports_json=True,
            concurrent_requests=25,
            async_mode="threaded",
            retry_strategy="native",
            max_retries=10,
            tokens_per_minute=120000,
            requests_per_minute=100,
            encoding_model="cl100k_base",
        )
        
        self.embedding_config = LanguageModelConfig(
            api_key=self.embedding_key,
            type=ModelType.OpenAIEmbedding,
            api_base="https://open.bigmodel.cn/api/paas/v4",
            model="embedding-3",
            deployment_name="embedding-3",
            auth_type="api_key",
            max_retries=20,
            tokens_per_minute=120000000,
            requests_per_minute=100,
            encoding_model="cl100k_base",
        )
        
        self.token_encoder = tiktoken.get_encoding("cl100k_base")
        
    def _init_data(self):
        """åˆå§‹åŒ–æ•°æ®"""
        INPUT_DIR = "./rag/output"
        COMMUNITY_LEVEL = 2
        
        # è¯»å–æ•°æ®æ–‡ä»¶
        self.community_df = pd.read_parquet(f"{INPUT_DIR}/communities.parquet")
        self.entity_df = pd.read_parquet(f"{INPUT_DIR}/entities.parquet")
        self.report_df = pd.read_parquet(f"{INPUT_DIR}/community_reports.parquet")
        self.relationship_df = pd.read_parquet(f"{INPUT_DIR}/relationships.parquet")
        self.text_unit_df = pd.read_parquet(f"{INPUT_DIR}/text_units.parquet")
        
        # åˆå§‹åŒ–GraphRAGæ•°æ®ç»“æ„
        self.communities = read_indexer_communities(self.community_df, self.report_df)
        self.reports = read_indexer_reports(self.report_df, self.community_df, COMMUNITY_LEVEL)
        self.entities = read_indexer_entities(self.entity_df, self.community_df, COMMUNITY_LEVEL)
        self.relationships = read_indexer_relationships(self.relationship_df)
        self.text_units = read_indexer_text_units(self.text_unit_df)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.description_embedding_store = LanceDBVectorStore(
            collection_name="default-entity-description",
        )
        self.description_embedding_store.connect(db_uri=f"{INPUT_DIR}/lancedb")
        
    def _init_engines(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager()
        self.chat_model = self.model_manager.get_or_create_chat_model(
            name="rag_engine",
            model_type=ModelType.AzureOpenAIChat,
            config=self.chat_config,
        )
        self.text_embedder = self.model_manager.get_or_create_embedding_model(
            name="rag_engine_embedding",
            model_type=ModelType.OpenAIEmbedding,
            config=self.embedding_config,
        )
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„å»ºå™¨
        self._init_global_context()
        self._init_local_context()
        
    def _init_global_context(self):
        """åˆå§‹åŒ–å…¨å±€æœç´¢ä¸Šä¸‹æ–‡"""
        self.global_context_builder = GlobalCommunityContext(
            community_reports=self.reports,
            communities=self.communities,
            entities=self.entities,
            token_encoder=self.token_encoder,
        )
        
        # å¤§å¹…å‡å°‘max_tokensä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™
        self.global_context_params = {
            "use_community_summary": False,
            "shuffle_data": True,
            "include_community_rank": True,
            "min_community_rank": 0,
            "community_rank_name": "rank",
            "include_community_weight": True,
            "community_weight_name": "occurrence weight",
            "normalize_community_weight": True,
            "max_tokens": 10000,  # ä»4000å¢åŠ åˆ°10000
            "context_name": "Reports",
        }
        
        self.map_llm_params = {
            "max_tokens": 1200,  # ä»800å¢åŠ åˆ°1200
            "temperature": 0.0,
            # ç§»é™¤ "response_format": {"type": "json_object"},
        }
        
        self.reduce_llm_params = {
            "max_tokens": 1500,  # ä»2500å‡å°‘åˆ°1500
            "temperature": 0.0,
        }
        
        self.global_search_engine = GlobalSearch(
            model=self.chat_model,
            context_builder=self.global_context_builder,
            token_encoder=self.token_encoder,
            max_data_tokens=10000,  # ä»4000å¢åŠ åˆ°10000
            map_llm_params=self.map_llm_params,
            reduce_llm_params=self.reduce_llm_params,
            allow_general_knowledge=False,
            json_mode=False,  # å…³é—­JSONæ¨¡å¼ï¼Œé¿å…JSONè§£æé”™è¯¯
            context_builder_params=self.global_context_params,
            concurrent_coroutines=128,
            response_type="multiple paragraphs",
        )
        
    def _init_local_context(self):
        """åˆå§‹åŒ–å±€éƒ¨æœç´¢ä¸Šä¸‹æ–‡"""
        self.local_context_builder = LocalSearchMixedContext(
            community_reports=self.reports,
            text_units=self.text_units,
            entities=self.entities,
            relationships=self.relationships,
            covariates=None,
            entity_text_embeddings=self.description_embedding_store,
            embedding_vectorstore_key=EntityVectorStoreKey.ID,
            text_embedder=self.text_embedder,
            token_encoder=self.token_encoder,
        )

        # å¤§å¹…å‡å°‘max_tokensä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™
        self.local_context_params = {
            "text_unit_prop": 0.5,
            "community_prop": 0.1,
            "conversation_history_max_turns": 3,  # ä»5å‡å°‘åˆ°3
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 5,  # ä»10å‡å°‘åˆ°5
            "top_k_relationships": 5,  # ä»10å‡å°‘åˆ°5
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 10000,  # ä»4000å¢åŠ åˆ°10000
        }
        
        self.local_model_params = {
            "max_tokens": 1500,  # ä»2500å‡å°‘åˆ°1500
            "temperature": 0.0,
        }
        
        self.local_search_engine = LocalSearch(
            model=self.chat_model,
            context_builder=self.local_context_builder,
            token_encoder=self.token_encoder,
            model_params=self.local_model_params,
            context_builder_params=self.local_context_params,
            response_type="multiple paragraphs",
        )
    
    def _truncate_text(self, text: str, max_tokens: int = 2000) -> str:
        """æˆªæ–­æ–‡æœ¬ä»¥é¿å…tokenè¶…é™"""
        if not text:
            return text
        
        tokens = self.token_encoder.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        # æˆªæ–­åˆ°æŒ‡å®štokenæ•°
        truncated_tokens = tokens[:max_tokens]
        return self.token_encoder.decode(truncated_tokens)
    
    async def global_search_retrieve(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨æ£€ç´¢å…¨å±€ä¿¡æ¯: {query}")
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼‰
            # build_context æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦ await
            context = await self.global_context_builder.build_context(
                query=query,
                **self.global_context_params
            )
            
            # æˆªæ–­ä¸Šä¸‹æ–‡ä»¥é¿å…tokenè¶…é™
            # æ›´å®‰å…¨åœ°å¤„ç†contextå¯¹è±¡
            if hasattr(context, 'context_text'):
                context_text = context.context_text
            elif hasattr(context, 'text'):
                context_text = context.text
            elif hasattr(context, 'content'):
                context_text = context.content
            elif hasattr(context, 'response'):
                context_text = context.response
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                context_text = str(context)
            
            truncated_context = self._truncate_text(context_text, max_tokens=8000)  # ä»3000å¢åŠ åˆ°8000
            
            print(f" [RAGæ£€ç´¢] å…¨å±€æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(truncated_context)} å­—ç¬¦çš„ä¸Šä¸‹æ–‡")
            print(f"ğŸ¤– [Agentè°ƒç”¨] æ­£åœ¨ä½¿ç”¨Agentè°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            
            return {
                "method": "global_retrieve",
                "query": query,
                "retrieved_context": {
                    "context_text": truncated_context,
                    "context_length": len(truncated_context),
                    "context_summary": "GraphRAGå…¨å±€æœç´¢æ£€ç´¢åˆ°çš„ç¤¾åŒºæŠ¥å‘Šå’Œå®ä½“ä¿¡æ¯ï¼ˆå·²æˆªæ–­ï¼‰"
                },
                "success": True
            }
        except Exception as e:
            print(f"[RAGæ£€ç´¢] å…¨å±€æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "global_retrieve", 
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def global_search_generate(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        try:
            print(f" [LLMç”Ÿæˆ] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå…¨å±€æœç´¢å›ç­”: {query}")
            
            # ä¸ç›´æ¥è°ƒç”¨GraphRAGçš„searchæ–¹æ³•ï¼Œè€Œæ˜¯è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            # è®©agenté€šè¿‡ç‹¬ç«‹LLMå·¥å…·æ¥å¤„ç†ç”Ÿæˆ
            context_text = retrieved_context.get('context_text', '') if isinstance(retrieved_context, dict) else str(retrieved_context)
            
            print(f"[LLMç”Ÿæˆ] å‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context_text)} å­—ç¬¦")
            
            return {
                "method": "global_generate",
                "query": query,
                "retrieved_context": retrieved_context,
                "context_ready": True,
                "success": True,
                "note": "è¯·ä½¿ç”¨llm_generate_toolæ¥å¤„ç†ç”Ÿæˆ"
            }
        except Exception as e:
            print(f" [LLMç”Ÿæˆ] å…¨å±€æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "method": "global_generate",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def global_search_full(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        try:
            print(f"ğŸš€ [å®Œæ•´æµç¨‹] å¼€å§‹å…¨å±€æœç´¢: {query}")
            
            # 1. å…ˆæ£€ç´¢
            retrieve_result = await self.global_search_retrieve(query)
            if not retrieve_result['success']:
                return retrieve_result
            
            # 2. è¿”å›æ£€ç´¢ç»“æœï¼Œè®©agentå†³å®šå¦‚ä½•å¤„ç†
            print(f"âœ… [å®Œæ•´æµç¨‹] å…¨å±€æ£€ç´¢å®Œæˆï¼Œç­‰å¾…agentå¤„ç†")
            print(f"ğŸ¤– [Agentè°ƒç”¨] æ­£åœ¨ä½¿ç”¨Agentè°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            
            return {
                "method": "global_full",
                "query": query,
                "retrieved_context": retrieve_result['retrieved_context'],
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨llm_generate_toolè¿›è¡Œç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [å®Œæ•´æµç¨‹] å…¨å±€æœç´¢å¤±è´¥: {e}")
            return {
                "method": "global_full",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_retrieve(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨æ£€ç´¢å±€éƒ¨ä¿¡æ¯: {query}")
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼‰
            # LocalSearchMixedContext.build_context æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¸éœ€è¦ await
            context = self.local_context_builder.build_context(
                query=query,
                **self.local_context_params
            )
            
            # æˆªæ–­ä¸Šä¸‹æ–‡ä»¥é¿å…tokenè¶…é™
            # æ›´å®‰å…¨åœ°å¤„ç†contextå¯¹è±¡
            if hasattr(context, 'context_text'):
                context_text = context.context_text
            elif hasattr(context, 'text'):
                context_text = context.text
            elif hasattr(context, 'content'):
                context_text = context.content
            elif hasattr(context, 'response'):
                context_text = context.response
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                context_text = str(context)
            
            truncated_context = self._truncate_text(context_text, max_tokens=8000)  # ä»3000å¢åŠ åˆ°8000
            
            print(f" [RAGæ£€ç´¢] å±€éƒ¨æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(truncated_context)} å­—ç¬¦çš„ä¸Šä¸‹æ–‡")
            print(f"ğŸ¤– [Agentè°ƒç”¨] æ­£åœ¨ä½¿ç”¨Agentè°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            
            return {
                "method": "local_retrieve",
                "query": query,
                "retrieved_context": {
                    "context_text": truncated_context,
                    "context_length": len(truncated_context),
                    "context_summary": "GraphRAGå±€éƒ¨æœç´¢æ£€ç´¢åˆ°çš„æ–‡æœ¬å•å…ƒã€å®ä½“å’Œå…³ç³»ä¿¡æ¯ï¼ˆå·²æˆªæ–­ï¼‰"
                },
                "success": True
            }
        except Exception as e:
            print(f"âŒ [RAGæ£€ç´¢] å±€éƒ¨æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "local_retrieve",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_generate(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        try:
            print(f"ğŸ¤– [LLMç”Ÿæˆ] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå±€éƒ¨æœç´¢å›ç­”: {query}")
            
            # ä¸ç›´æ¥è°ƒç”¨GraphRAGçš„searchæ–¹æ³•ï¼Œè€Œæ˜¯è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            # è®©agenté€šè¿‡ç‹¬ç«‹LLMå·¥å…·æ¥å¤„ç†ç”Ÿæˆ
            context_text = retrieved_context.get('context_text', '') if isinstance(retrieved_context, dict) else str(retrieved_context)
            
            print(f"âœ… [LLMç”Ÿæˆ] å‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context_text)} å­—ç¬¦")
            
            return {
                "method": "local_generate",
                "query": query,
                "retrieved_context": retrieved_context,
                "context_ready": True,
                "success": True,
                "note": "è¯·ä½¿ç”¨llm_generate_toolæ¥å¤„ç†ç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [LLMç”Ÿæˆ] å±€éƒ¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "method": "local_generate",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_full(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        try:
            print(f"ğŸš€ [å®Œæ•´æµç¨‹] å¼€å§‹å±€éƒ¨æœç´¢: {query}")
            
            # 1. å…ˆæ£€ç´¢
            retrieve_result = await self.local_search_retrieve(query)
            if not retrieve_result['success']:
                return retrieve_result
            
            # 2. è¿”å›æ£€ç´¢ç»“æœï¼Œè®©agentå†³å®šå¦‚ä½•å¤„ç†
            print(f"âœ… [å®Œæ•´æµç¨‹] å±€éƒ¨æ£€ç´¢å®Œæˆï¼Œç­‰å¾…agentå¤„ç†")
            print(f"ğŸ¤– [Agentè°ƒç”¨] æ­£åœ¨ä½¿ç”¨Agentè°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            
            return {
                "method": "local_full",
                "query": query,
                "retrieved_context": retrieve_result['retrieved_context'],
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨llm_generate_toolè¿›è¡Œç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [å®Œæ•´æµç¨‹] å±€éƒ¨æœç´¢å¤±è´¥: {e}")
            return {
                "method": "local_full",
                "query": query,
                "error": str(e),
                "success": False
            }

# å…¨å±€å®ä¾‹
rag_engine = RAGEngine()