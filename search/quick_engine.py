import os
import asyncio
import pandas as pd
import tiktoken
from typing import Dict, Any, List, Optional

from graphrag.config.enums import ModelType
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.language_model.manager import ModelManager
from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
    read_indexer_relationships,
    read_indexer_text_units,
    read_indexer_covariates,
)
from graphrag.query.structured_search.global_search.community_context import (
    GlobalCommunityContext,
)
from graphrag.query.structured_search.global_search.search import GlobalSearch
from graphrag.query.structured_search.local_search.mixed_context import (
    LocalSearchMixedContext,
)
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from dotenv import load_dotenv

load_dotenv()

class RAGEngine:
    """GraphRAGå¼•æ“ï¼Œå°†æ£€ç´¢å’ŒLLMè°ƒç”¨åˆ†ç¦»ï¼Œæ”¯æŒå¤šä¹¦æœ¬"""
    
    def __init__(self, input_dir: str = "./rag/output"):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„ï¼ŒåŒ…å«GraphRAGå¤„ç†åçš„æ•°æ®æ–‡ä»¶
        """
        self.input_dir = input_dir
        self.api_key = os.getenv("AZURE_OPENAI_API_KEY")
        self.embedding_key = os.getenv("Embedding_key")
        
        # åˆå§‹åŒ–é…ç½®
        self._init_configs()
        self._init_data()
        self._init_engines()
        
    def _init_configs(self):
        """åˆå§‹åŒ–LLMé…ç½®"""
        self.chat_config = LanguageModelConfig(
            type=ModelType.AzureOpenAIChat,
            api_base="https://tcamp.openai.azure.com/",
            api_version="2025-01-01-preview",
            auth_type="api_key",
            api_key=self.api_key,
            model="gpt-4o",
            deployment_name="gpt-4o",
            model_supports_json=True,
            concurrent_requests=25,
            async_mode="threaded",
            retry_strategy="native",
            max_retries=10,
            tokens_per_minute=120000,
            requests_per_minute=100,
            encoding_model="cl100k_base",
        )


        
        # self.embedding_config = LanguageModelConfig(
        #     api_key=self.embedding_key,
        #     type=ModelType.OpenAIEmbedding,
        #     api_base="https://open.bigmodel.cn/api/paas/v4",
        #     model="embedding-3",
        #     deployment_name="embedding-3",
        #     auth_type="api_key",
        #     max_retries=20,
        #     tokens_per_minute=120000000,
        #     requests_per_minute=100,
        #     encoding_model="cl100k_base",
        # )

        self.embedding_config = LanguageModelConfig(
                        type=ModelType.AzureOpenAIChat,
            api_base="https://tcamp.openai.azure.com/",
            api_version="2023-05-15",
            auth_type="api_key",
            api_key=self.api_key,
            model="text-embedding-ada-002",
            deployment_name="text-embedding-ada-002",
            model_supports_json=True,
            concurrent_requests=25,
            async_mode="threaded",
            retry_strategy="native",
            max_retries=20,
            tokens_per_minute=120000000,
            requests_per_minute=100,
            encoding_model="cl100k_base",
        )
        
        self.token_encoder = tiktoken.get_encoding("cl100k_base")
        
    def _init_data(self):
        """åˆå§‹åŒ–æ•°æ®"""
        COMMUNITY_LEVEL = 2
        
        # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.input_dir):
            raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
        
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ä¹¦æœ¬æ•°æ®: {self.input_dir}")
        
        # è¯»å–æ•°æ®æ–‡ä»¶
        self.community_df = pd.read_parquet(f"{self.input_dir}/communities.parquet")
        self.entity_df = pd.read_parquet(f"{self.input_dir}/entities.parquet")
        self.report_df = pd.read_parquet(f"{self.input_dir}/community_reports.parquet")
        self.relationship_df = pd.read_parquet(f"{self.input_dir}/relationships.parquet")
        self.text_unit_df = pd.read_parquet(f"{self.input_dir}/text_units.parquet")
        
        # åˆå§‹åŒ–GraphRAGæ•°æ®ç»“æ„
        self.communities = read_indexer_communities(self.community_df, self.report_df)
        self.reports = read_indexer_reports(self.report_df, self.community_df, COMMUNITY_LEVEL)
        self.entities = read_indexer_entities(self.entity_df, self.community_df, COMMUNITY_LEVEL)
        self.relationships = read_indexer_relationships(self.relationship_df)
        self.text_units = read_indexer_text_units(self.text_unit_df)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.description_embedding_store = LanceDBVectorStore(
            collection_name="default-entity-description",
        )
        self.description_embedding_store.connect(db_uri=f"{self.input_dir}/lancedb")
        
        print(f"âœ… ä¹¦æœ¬æ•°æ®åŠ è½½å®Œæˆ: {self.input_dir}")
        
    def _init_engines(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager()
        self.chat_model = self.model_manager.get_or_create_chat_model(
            name="rag_engine",
            model_type=ModelType.AzureOpenAIChat,
            config=self.chat_config,
        )
        self.text_embedder = self.model_manager.get_or_create_embedding_model(
            name="rag_engine_embedding",
            model_type=ModelType.OpenAIEmbedding,
            config=self.embedding_config,
        )
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„å»ºå™¨
        self._init_global_context()
        self._init_local_context()
        
    def _init_global_context(self):
        """åˆå§‹åŒ–å…¨å±€æœç´¢ä¸Šä¸‹æ–‡"""
        self.global_context_builder = GlobalCommunityContext(
            community_reports=self.reports,
            communities=self.communities,
            entities=self.entities,
            token_encoder=self.token_encoder,
        )
        
        # å¤§å¹…å‡å°‘max_tokensä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™ï¼Œä¼˜åŒ–APIè°ƒç”¨æ•ˆç‡
        self.global_context_params = {
            "use_community_summary": False,
            "shuffle_data": True,
            "include_community_rank": True,
            "min_community_rank": 0,
            "community_rank_name": "rank",
            "include_community_weight": True,
            "community_weight_name": "occurrence weight",
            "normalize_community_weight": True,
            "max_tokens": 4000,  # å†æ¬¡æ”¶ç¼©ï¼Œé™ä½è¶…é•¿ä¸Šä¸‹æ–‡æ¦‚ç‡
            "context_name": "Reports",
        }
        
        self.map_llm_params = {
            "max_tokens": 1200,  # ä»800å¢åŠ åˆ°1200
            "temperature": 0.0,
            # ç§»é™¤ "response_format": {"type": "json_object"},
        }
        
        self.reduce_llm_params = {
            "max_tokens": 1500,  # ä»2500å‡å°‘åˆ°1500
            "temperature": 0.0,
        }
        
        self.global_search_engine = GlobalSearch(
            model=self.chat_model,
            context_builder=self.global_context_builder,
            token_encoder=self.token_encoder,
            max_data_tokens=6000,  # ä»10000å‡å°‘åˆ°6000ä»¥å‡å°‘æ£€ç´¢å†…å®¹
            map_llm_params=self.map_llm_params,
            reduce_llm_params=self.reduce_llm_params,
            allow_general_knowledge=False,
            json_mode=False,  # å…³é—­JSONæ¨¡å¼ï¼Œé¿å…JSONè§£æé”™è¯¯
            context_builder_params=self.global_context_params,
            concurrent_coroutines=128,
            response_type="multiple paragraphs",
        )
        
    def _init_local_context(self):
        """åˆå§‹åŒ–å±€éƒ¨æœç´¢ä¸Šä¸‹æ–‡"""
        self.local_context_builder = LocalSearchMixedContext(
            community_reports=self.reports,
            text_units=self.text_units,
            entities=self.entities,
            relationships=self.relationships,
            covariates=None,
            entity_text_embeddings=self.description_embedding_store,
            embedding_vectorstore_key=EntityVectorStoreKey.ID,
            text_embedder=self.text_embedder,
            token_encoder=self.token_encoder,
        )

        # å¤§å¹…å‡å°‘max_tokensä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™ï¼Œä¼˜åŒ–APIè°ƒç”¨æ•ˆç‡
        self.local_context_params = {
            "text_unit_prop": 0.4,  # ä»0.5å‡å°‘åˆ°0.4
            "community_prop": 0.05,  # ä»0.1å‡å°‘åˆ°0.05
            "conversation_history_max_turns": 2,  # ä»3å‡å°‘åˆ°2
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 3,  # ä»5å‡å°‘åˆ°3
            "top_k_relationships": 3,  # ä»5å‡å°‘åˆ°3
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 3500,  # å†æ¬¡æ”¶ç¼©
        }
        
        self.local_model_params = {
            "max_tokens": 1500,  # ä»2500å‡å°‘åˆ°1500
            "temperature": 0.0,
        }
        
        self.local_search_engine = LocalSearch(
            model=self.chat_model,
            context_builder=self.local_context_builder,
            token_encoder=self.token_encoder,
            model_params=self.local_model_params,
            context_builder_params=self.local_context_params,
            response_type="multiple paragraphs",
        )
    
    def _truncate_text(self, text: str, max_tokens: int = 2000) -> str:
        """æˆªæ–­æ–‡æœ¬ä»¥é¿å…tokenè¶…é™ - å·²å¼ƒç”¨ï¼Œä¿ç•™å‘åå…¼å®¹"""
        if not text:
            return text
        
        tokens = self.token_encoder.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        # æˆªæ–­åˆ°æŒ‡å®štokenæ•°
        truncated_tokens = tokens[:max_tokens]
        return self.token_encoder.decode(truncated_tokens)
    
    def _chunk_text(self, text: str, max_tokens_per_chunk: int = 6000, overlap_tokens: int = 300) -> List[Dict[str, Any]]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å—ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ï¼ˆæ™ºèƒ½åˆ†å—ç‰ˆæœ¬ï¼Œç¡®ä¿æ¯ä¸ªåˆ†å—éƒ½åœ¨LLMå¤„ç†èŒƒå›´å†…ï¼‰
        
        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            max_tokens_per_chunk: æ¯ä¸ªåˆ†å—çš„æœ€å¤§tokenæ•°ï¼ˆ8000ï¼Œä¸ºæç¤ºè¯ç•™å‡ºç©ºé—´ï¼‰
            overlap_tokens: åˆ†å—ä¹‹é—´çš„é‡å tokenæ•°
            
        Returns:
            åˆ†å—åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«æ–‡æœ¬ã€ä½ç½®ä¿¡æ¯ç­‰
        """
        if not text:
            return []
        
        tokens = self.token_encoder.encode(text)
        total_tokens = len(tokens)
        
        # å¦‚æœæ–‡æœ¬ä¸å¤ªé•¿ï¼Œç›´æ¥è¿”å›å•ä¸ªåˆ†å—
        if total_tokens <= max_tokens_per_chunk:
            return [{
                "chunk_id": 0,
                "text": text,
                "start_token": 0,
                "end_token": total_tokens,
                "total_tokens": total_tokens,
                "chunk_tokens": total_tokens,
                "is_complete": True,
                "safe_for_llm": True
            }]
        
        chunks = []
        chunk_id = 0
        start = 0
        
        # æ™ºèƒ½åˆ†å—ï¼šç¡®ä¿æ¯ä¸ªåˆ†å—éƒ½èƒ½è¢«LLMå¤„ç†
        # è€ƒè™‘æç¤ºè¯å¼€é”€ï¼ˆçº¦2000-3000 tokensï¼‰+ åˆ†å—å†…å®¹ï¼ˆ6000 tokensï¼‰â‰ˆ 8000-9000 tokens
        # æ›´ä¿å®ˆï¼Œé™ä½è¶…é™æ¦‚ç‡
        
        while start < total_tokens:
            end = min(start + max_tokens_per_chunk, total_tokens)
            
            # æå–å½“å‰åˆ†å—çš„token
            chunk_tokens = tokens[start:end]
            chunk_text = self.token_encoder.decode(chunk_tokens)
            
            chunks.append({
                "chunk_id": chunk_id,
                "text": chunk_text,
                "start_token": start,
                "end_token": end,
                "total_tokens": total_tokens,
                "chunk_tokens": len(chunk_tokens),
                "is_complete": (len(chunks) == 1 and end >= total_tokens),
                "safe_for_llm": True  # æ ‡è®°è¿™ä¸ªåˆ†å—æ˜¯LLMå®‰å…¨çš„
            })
            
            chunk_id += 1
            
            # ä¸‹ä¸€ä¸ªåˆ†å—çš„èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            if end >= total_tokens:
                break
            start = end - overlap_tokens
        
        print(f"ğŸ“Š [æ™ºèƒ½åˆ†å—] åŸå§‹ {total_tokens} tokens åˆ†ä¸º {len(chunks)} ä¸ªå®‰å…¨åˆ†å—ï¼ˆæ¯å—æœ€å¤š {max_tokens_per_chunk} tokensï¼Œç¡®ä¿LLMå¯å¤„ç†ï¼‰")
        return chunks
    
    async def naive_retrieve(self, query: str) -> Dict[str, Any]:
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨è¿›è¡Œæœ´ç´ æ£€ç´¢: {query}")
            # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æ£€ç´¢
            context = await self.global_context_builder.build_context(
                query=query,
                **self.global_context_params
            )
            return {
                "method": "naive_retrieve",
                "query": query,
                "retrieved_context": context,
                "context_ready": True,
                "success": True,
                "note": "æœ´ç´ æ£€ç´¢å®Œæˆ"
            }
        except Exception as e:
            print(f"âŒ [RAGæ£€ç´¢] æœ´ç´ æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "naive_retrieve",
                "query": query,
                "error": str(e),
                "success": False
            }

    async def global_search_retrieve(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œè¿”å›å®Œæ•´å¬å›å†…å®¹ç”¨äºåˆ†å—å¤„ç†"""
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨æ£€ç´¢å…¨å±€ä¿¡æ¯: {query}")
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼‰
            # build_context æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦ await
            context = await self.global_context_builder.build_context(
                query=query,
                **self.global_context_params
            )
            
            # å¤„ç†contextå¯¹è±¡ï¼Œè·å–åŸå§‹æ–‡æœ¬
            if hasattr(context, 'context_text'):
                context_text = context.context_text
            elif hasattr(context, 'text'):
                context_text = context.text
            elif hasattr(context, 'content'):
                context_text = context.content
            elif hasattr(context, 'response'):
                context_text = context.response
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                context_text = str(context)
            
            # è®¡ç®—åŸå§‹é•¿åº¦
            original_tokens = len(self.token_encoder.encode(context_text))
            
            # å¤§åˆ†å—å¤„ç†ï¼ˆå¤§å¹…å‡å°‘åˆ†å—æ•°é‡ä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
            chunks = self._chunk_text(context_text, max_tokens_per_chunk=100000, overlap_tokens=20000)
            
            print(f"âœ… [RAGæ£€ç´¢] å…¨å±€æ£€ç´¢å®Œæˆï¼ŒåŸå§‹å†…å®¹ {original_tokens} tokensï¼Œåˆ†ä¸º {len(chunks)} ä¸ªå¤§åˆ†å—")
            
            return {
                "method": "global_retrieve",
                "query": query,
                "retrieved_context": {
                    "full_text": context_text,
                    "original_length": len(context_text),
                    "original_tokens": original_tokens,
                    "chunks": chunks,
                    "total_chunks": len(chunks),
                    "context_summary": f"GraphRAGå…¨å±€æœç´¢æ£€ç´¢åˆ°çš„ç¤¾åŒºæŠ¥å‘Šå’Œå®ä½“ä¿¡æ¯ï¼ˆå®Œæ•´å†…å®¹ï¼Œ{len(chunks)}ä¸ªå¤§åˆ†å—ï¼Œå‡å°‘APIè°ƒç”¨ï¼‰"
                },
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆ"
            }
        except Exception as e:
            print(f"âŒ [RAGæ£€ç´¢] å…¨å±€æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "global_retrieve", 
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def global_search_generate(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        try:
            print(f" [LLMç”Ÿæˆ] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå…¨å±€æœç´¢å›ç­”: {query}")
            
            # ä¸ç›´æ¥è°ƒç”¨GraphRAGçš„searchæ–¹æ³•ï¼Œè€Œæ˜¯è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            # è®©agenté€šè¿‡ç‹¬ç«‹LLMå·¥å…·æ¥å¤„ç†ç”Ÿæˆ
            context_text = retrieved_context.get('context_text', '') if isinstance(retrieved_context, dict) else str(retrieved_context)
            
            print(f"[LLMç”Ÿæˆ] å‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context_text)} å­—ç¬¦")
            
            return {
                "method": "global_generate",
                "query": query,
                "retrieved_context": retrieved_context,
                "context_ready": True,
                "success": True,
                "note": "è¯·ä½¿ç”¨llm_generate_toolæ¥å¤„ç†ç”Ÿæˆ"
            }
        except Exception as e:
            print(f" [LLMç”Ÿæˆ] å…¨å±€æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "method": "global_generate",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def global_search_full(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+åˆ†å—å¹¶è¡Œåˆ†æ+ç»¼åˆï¼‰"""
        return await self.naive_retrieve(query)
        # try:
        #     print(f"ğŸš€ [å®Œæ•´æµç¨‹] å¼€å§‹å…¨å±€æœç´¢: {query}")
            
        #     # 1. å…ˆæ£€ç´¢
        #     retrieve_result = await self.naive_retrieve(query)
        #     if not retrieve_result['success']:
        #         return retrieve_result
            
#             # 2. è·å–æ£€ç´¢åˆ°çš„å†…å®¹å’Œåˆ†å—
#             retrieved_context = retrieve_result['retrieved_context']
#             full_text = retrieved_context['full_text']
#             chunks = retrieved_context['chunks']
            
#             print(f"ğŸ“Š [åˆ†å—å¤„ç†] å¼€å§‹å¯¹ {len(chunks)} ä¸ªåˆ†å—è¿›è¡Œå¹¶è¡ŒLLMåˆ†æ")
            
#             # 3. å¯¹æ¯ä¸ªåˆ†å—å¹¶è¡Œè°ƒç”¨LLMè¿›è¡Œåˆ†æ
#             async def analyze_chunk(chunk_info):
#                 chunk_id = chunk_info['chunk_id']
#                 chunk_text = chunk_info['text']
#                 chunk_tokens = chunk_info.get('chunk_tokens', 0)
                
#                 print(f"  ğŸ“ [åˆ†å— {chunk_id}] æ­£åœ¨åˆ†æ ({chunk_tokens} tokens)")
                
#                 # æ„å»ºåˆ†ææç¤ºï¼ˆä¼˜åŒ–ï¼šæ›´ç®€æ´èšç„¦ï¼‰
#                 analysis_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼Œè¯·åŸºäºè¾“å…¥çš„æ–‡æœ¬æå–ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼š{query}ã€‚è¾“å…¥å†…å®¹æ˜¯ä¸€ä¸ªæ•°æ®è¡¨æ ¼ï¼Œå®ƒä»£è¡¨äº†æŸæœ¬ä¹¦çš„ä¸€éƒ¨åˆ†æ•°æ®ä¿¡æ¯ä¸”æ˜¯åˆ†æ¡å¯¹å†…å®¹è¿›è¡Œè¡¨è¿°ï¼Œä½ å¯ä»¥é€šè¿‡è¿™ä¸ªæ•°æ®è¡¨æ ¼è·å–æœ‰ç”¨å†…å®¹ã€‚

# å†…å®¹ç‰‡æ®µ [{chunk_id + 1}]:
# {chunk_text}

# è¦æ±‚ï¼š
# # æ–‡æœ¬åˆ†æè¦æ±‚
# - æ ¹æ®ä¸Šé¢çš„æ–‡æœ¬ç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå°½å¯èƒ½è¯¦ç»†ï¼Œé€»è¾‘ä¸¥å¯†ã€‚ä¾‹å¦‚ï¼šç”¨æˆ·é—®é¢˜æœ‰å…³â€œæ€»ç»“ä¹¦ä¸­ä¸»è¦ä¿¡æ¯ã€ä¸»é¢˜â€ï¼Œé‚£ä¹ˆå°±å¯¹äºè¾“å…¥çš„æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ€»ç»“ï¼Œæå–å…¶ä¸­çš„å…³é”®éƒ¨åˆ†ï¼›å¦‚æœé—®é¢˜æœ‰å…³â€œä¹¦ä¸­çš„äººç‰©Aâ€ï¼Œé‚£ä¹ˆå°±æå–ä¸äººç‰©Aç›¸å…³çš„ä¿¡æ¯ï¼Œç­‰ç­‰ã€‚
# # è¾“å‡ºæ ¼å¼
# ä¸¥æ ¼æŒ‰ç…§ä¸­æ–‡è¾“å‡ºï¼Œå¯ä»¥è€ƒè™‘åˆ†æ¡å™è¿°ã€‚
# """
                
#                 try:
#                     # æ£€æŸ¥æç¤ºé•¿åº¦ï¼Œé¿å…tokenè¶…é™
#                     prompt_tokens = len(self.token_encoder.encode(analysis_prompt))
#                     if prompt_tokens > 1000000:  # é™ä½é™åˆ¶åˆ°10Kï¼Œä¸ºå“åº”ç•™ç©ºé—´
#                         print(f"    âš ï¸ [åˆ†å— {chunk_id}] å†…å®¹è¿‡é•¿ ({prompt_tokens} tokens)ï¼Œè¿›è¡Œå‹ç¼©")
#                         max_chunk_chars = 600000
#                         if len(chunk_text) > max_chunk_chars:
#                             compressed_text = chunk_text[:max_chunk_chars] + "\\n\\n[å†…å®¹å·²å‹ç¼©ä»¥é€‚é…LLMé™åˆ¶]"
#                             analysis_prompt = f"""è¯·åŸºäºä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

# ç”¨æˆ·é—®é¢˜ï¼š{query}

# å†…å®¹ç‰‡æ®µ [{chunk_id + 1}] (å·²å‹ç¼©):
# {compressed_text}

# è¦æ±‚ï¼š
# - ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„ç›¸å…³éƒ¨åˆ†
# - å¦‚æœå†…å®¹ä¸ç›¸å…³ï¼Œè¯´æ˜"æ­¤ç‰‡æ®µæ— ç›¸å…³ä¿¡æ¯"  
# - ç®€æ´å‡†ç¡®ï¼Œçªå‡ºé‡ç‚¹"""
                    
#                     # è°ƒç”¨GraphRAGçš„chat_modelè¿›è¡Œåˆ†æ
#                     response = await self.chat_model.achat(analysis_prompt)
                    
#                     # ç»Ÿè®¡responseçš„tokenæ•°
#                     response_tokens = len(self.token_encoder.encode(str(response.output)))
#                     print(f"    âœ… [åˆ†å— {chunk_id}] åˆ†æå®Œæˆï¼Œç”Ÿæˆå›ç­” {response_tokens} tokens")
#                     # print(response.output)
                    
#                     return {
#                         "chunk_id": chunk_id,
#                         "analysis": response.output,
#                         "success": True,
#                         "chunk_tokens": chunk_tokens
#                     }
                    
#                 except Exception as e:
#                     print(f"    âŒ [åˆ†å— {chunk_id}] åˆ†æå¤±è´¥: {e}")
#                     return {
#                         "chunk_id": chunk_id,
#                         "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
#                         "success": False,
#                         "error": str(e),
#                         "chunk_tokens": chunk_tokens
#                     }
            
#             # 4. å¹¶è¡Œå¤„ç†æ‰€æœ‰åˆ†å—ï¼ˆé™åˆ¶å¹¶å‘æ•°é¿å…APIé™åˆ¶ï¼‰
#             import asyncio
#             semaphore = asyncio.Semaphore(15)  # é™åˆ¶æœ€å¤š3ä¸ªå¹¶å‘
            
#             async def limited_analyze(chunk):
#                 async with semaphore:
#                     result = await analyze_chunk(chunk)
#                     # æ·»åŠ å°å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
#                     return result
            
#             chunk_results = await asyncio.gather(*[limited_analyze(chunk) for chunk in chunks])
            
#             # 5. ç»Ÿè®¡å’Œæ•´ç†ç»“æœ
#             successful_chunks = [r for r in chunk_results if r.get("success", False)]
#             failed_chunks = [r for r in chunk_results if not r.get("success", False)]
            
#             print(f"ğŸ“Š [åˆ†å—åˆ†æ] å®Œæˆï¼š{len(successful_chunks)}/{len(chunks)} ä¸ªåˆ†å—æˆåŠŸ")
            
#             # 6. å°†æ‰€æœ‰æˆåŠŸçš„åˆ†æç»“æœç»¼åˆæˆä¸€ä¸ªå®Œæ•´çš„ä¸Šä¸‹æ–‡
#             comprehensive_analysis = []
#             for result in successful_chunks:
#                 chunk_id = result['chunk_id']
#                 analysis = result['analysis']
#                 comprehensive_analysis.append(f"=== åˆ†å— {chunk_id} åˆ†æç»“æœ ===\\n{analysis}\\n")
            
            # 7. åˆ›å»ºç»¼åˆä¸Šä¸‹æ–‡
#             final_context = f"""åŸºäºGraphRAGå…¨å±€æœç´¢å’Œåˆ†å—åˆ†æï¼Œä»¥ä¸‹æ˜¯å…³äº"{query}"çš„ç»¼åˆä¿¡æ¯ï¼š

# {"".join(retrieve_result)}
# """
            
#             print(f"âœ… [å®Œæ•´æµç¨‹] å…¨å±€æœç´¢å’Œåˆ†å—åˆ†æå®Œæˆï¼Œç”Ÿæˆç»¼åˆä¸Šä¸‹æ–‡")
            
#             return {
#                 "method": "global_full_with_parallel_analysis",
#                 "query": query,
#                 "comprehensive_context": final_context,
#                 "context_ready": True,
#                 "success": True,
#                 "note": f"å·²å®Œæˆåˆ†å—å¹¶è¡Œåˆ†æï¼Œ{len(successful_chunks)}/{len(chunks)} ä¸ªåˆ†å—æˆåŠŸã€‚Agentå¯ç›´æ¥ä½¿ç”¨comprehensive_contextè¿›è¡Œæœ€ç»ˆå›ç­”ã€‚"
#             }
            
#         except Exception as e:
#             print(f"âŒ [å®Œæ•´æµç¨‹] å…¨å±€æœç´¢å¤±è´¥: {e}")
#             return {
#                 "method": "global_full_with_parallel_analysis",
#                 "query": query,
#                 "error": str(e),
#                 "success": False
#             }
    
    async def local_search_retrieve(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œè¿”å›å®Œæ•´å¬å›å†…å®¹ç”¨äºåˆ†å—å¤„ç†"""
        try:
            print(f"ğŸ” [RAGæ£€ç´¢] æ­£åœ¨æ£€ç´¢å±€éƒ¨ä¿¡æ¯: {query}")
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼‰
            # LocalSearchMixedContext.build_context æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¸éœ€è¦ await
            context = self.local_context_builder.build_context(
                query=query,
                **self.local_context_params
            )
            
            # å¤„ç†contextå¯¹è±¡ï¼Œè·å–åŸå§‹æ–‡æœ¬
            if hasattr(context, 'context_text'):
                context_text = context.context_text
            elif hasattr(context, 'text'):
                context_text = context.text
            elif hasattr(context, 'content'):
                context_text = context.content
            elif hasattr(context, 'response'):
                context_text = context.response
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                context_text = str(context)
            
            # è®¡ç®—åŸå§‹é•¿åº¦
            original_tokens = len(self.token_encoder.encode(context_text))
            
            # å¤§åˆ†å—å¤„ç†ï¼ˆå¤§å¹…å‡å°‘åˆ†å—æ•°é‡ä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
            chunks = self._chunk_text(context_text, max_tokens_per_chunk=20000, overlap_tokens=1500)
            
            print(f"âœ… [RAGæ£€ç´¢] å±€éƒ¨æ£€ç´¢å®Œæˆï¼ŒåŸå§‹å†…å®¹ {original_tokens} tokensï¼Œåˆ†ä¸º {len(chunks)} ä¸ªå¤§åˆ†å—")
            
            return {
                "method": "local_retrieve",
                "query": query,
                "retrieved_context": {
                    "full_text": context_text,
                    "original_length": len(context_text),
                    "original_tokens": original_tokens,
                    "chunks": chunks,
                    "total_chunks": len(chunks),
                    "context_summary": f"GraphRAGå±€éƒ¨æœç´¢æ£€ç´¢åˆ°çš„æ–‡æœ¬å•å…ƒã€å®ä½“å’Œå…³ç³»ä¿¡æ¯ï¼ˆå®Œæ•´å†…å®¹ï¼Œ{len(chunks)}ä¸ªå¤§åˆ†å—ï¼Œå‡å°‘APIè°ƒç”¨ï¼‰"
                },
                "context_ready": True,
                "success": True,
                "note": "æ£€ç´¢å®Œæˆï¼Œè¯·ä½¿ç”¨parallel_chunk_analysis_toolè¿›è¡Œåˆ†æ"
            }
        except Exception as e:
            print(f"âŒ [RAGæ£€ç´¢] å±€éƒ¨æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "local_retrieve",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_generate(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        try:
            print(f"ğŸ¤– [LLMç”Ÿæˆ] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå±€éƒ¨æœç´¢å›ç­”: {query}")
            
            # ä¸ç›´æ¥è°ƒç”¨GraphRAGçš„searchæ–¹æ³•ï¼Œè€Œæ˜¯è¿”å›æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            # è®©agenté€šè¿‡ç‹¬ç«‹LLMå·¥å…·æ¥å¤„ç†ç”Ÿæˆ
            context_text = retrieved_context.get('context_text', '') if isinstance(retrieved_context, dict) else str(retrieved_context)
            
            print(f"âœ… [LLMç”Ÿæˆ] å‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context_text)} å­—ç¬¦")
            
            return {
                "method": "local_generate",
                "query": query,
                "retrieved_context": retrieved_context,
                "context_ready": True,
                "success": True,
                "note": "è¯·ä½¿ç”¨llm_generate_toolæ¥å¤„ç†ç”Ÿæˆ"
            }
        except Exception as e:
            print(f"âŒ [LLMç”Ÿæˆ] å±€éƒ¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "method": "local_generate",
                "query": query,
                "error": str(e),
                "success": False
            }
    
    async def local_search_full(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+åˆ†å—å¹¶è¡Œåˆ†æ+ç»¼åˆï¼‰"""
        try:
            print(f"ğŸš€ [å®Œæ•´æµç¨‹] å¼€å§‹å±€éƒ¨æœç´¢: {query}")
            
            # 1. å…ˆæ£€ç´¢
            retrieve_result = await self.local_search_retrieve(query)
            if not retrieve_result['success']:
                return retrieve_result
            
            # 2. è·å–æ£€ç´¢åˆ°çš„å†…å®¹å’Œåˆ†å—
            retrieved_context = retrieve_result['retrieved_context']
            full_text = retrieved_context['full_text']
            chunks = retrieved_context['chunks']
            
            print(f"ğŸ“Š [åˆ†å—å¤„ç†] å¼€å§‹å¯¹ {len(chunks)} ä¸ªåˆ†å—è¿›è¡Œå¹¶è¡ŒLLMåˆ†æ")
            
            # 3. å¯¹æ¯ä¸ªåˆ†å—å¹¶è¡Œè°ƒç”¨LLMè¿›è¡Œåˆ†æ
            async def analyze_chunk(chunk_info):
                chunk_id = chunk_info['chunk_id']
                chunk_text = chunk_info['text']
                chunk_tokens = chunk_info.get('chunk_tokens', 0)
                
                print(f"  ğŸ“ [åˆ†å— {chunk_id}] æ­£åœ¨åˆ†æ ({chunk_tokens} tokens)")
                
                # æ„å»ºåˆ†ææç¤ºï¼ˆå±€éƒ¨æœç´¢ï¼šèšç„¦å…·ä½“ç»†èŠ‚ï¼‰
                analysis_prompt = f"""è¯·åŸºäºä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸“æ³¨äºå…·ä½“ç»†èŠ‚ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

å†…å®¹ç‰‡æ®µ [{chunk_id + 1}]:
{chunk_text}

è¦æ±‚ï¼š
- é‡ç‚¹æå–å…·ä½“äº‹å®ã€æ•°æ®ã€äººç‰©å…³ç³»
- å¼•ç”¨å…³é”®æ–‡æœ¬æ®µè½ä½œä¸ºè¯æ®
- å¦‚æœå†…å®¹ä¸ç›¸å…³ï¼Œè¯´æ˜"æ­¤ç‰‡æ®µæ— ç›¸å…³ä¿¡æ¯"
- ç®€æ´å‡†ç¡®ï¼Œçªå‡ºå…³é”®ç»†èŠ‚"""
                
                try:
                    # æ£€æŸ¥æç¤ºé•¿åº¦ï¼Œé¿å…tokenè¶…é™
                    prompt_tokens = len(self.token_encoder.encode(analysis_prompt))
                    if prompt_tokens > 10000:  # é™ä½é™åˆ¶åˆ°10Kï¼Œä¸ºå“åº”ç•™ç©ºé—´
                        print(f"    âš ï¸ [åˆ†å— {chunk_id}] å†…å®¹è¿‡é•¿ ({prompt_tokens} tokens)ï¼Œè¿›è¡Œå‹ç¼©")
                        max_chunk_chars = 6000
                        if len(chunk_text) > max_chunk_chars:
                            compressed_text = chunk_text[:max_chunk_chars] + "\\n\\n[å†…å®¹å·²å‹ç¼©ä»¥é€‚é…LLMé™åˆ¶]"
                            analysis_prompt = f"""è¯·åŸºäºä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸“æ³¨äºå…·ä½“ç»†èŠ‚ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

å†…å®¹ç‰‡æ®µ [{chunk_id + 1}] (å·²å‹ç¼©):
{compressed_text}

è¦æ±‚ï¼š
- é‡ç‚¹æå–å…·ä½“äº‹å®ã€æ•°æ®ã€äººç‰©å…³ç³»
- å¼•ç”¨å…³é”®æ–‡æœ¬æ®µè½ä½œä¸ºè¯æ®
- å¦‚æœå†…å®¹ä¸ç›¸å…³ï¼Œè¯´æ˜"æ­¤ç‰‡æ®µæ— ç›¸å…³ä¿¡æ¯"
- ç®€æ´å‡†ç¡®ï¼Œçªå‡ºå…³é”®ç»†èŠ‚"""
                    
                    # è°ƒç”¨GraphRAGçš„chat_modelè¿›è¡Œåˆ†æ
                    response = await self.chat_model.achat(analysis_prompt)
                    
                    print(f"    âœ… [åˆ†å— {chunk_id}] åˆ†æå®Œæˆ")
                    
                    return {
                        "chunk_id": chunk_id,
                        "analysis": response,
                        "success": True,
                        "chunk_tokens": chunk_tokens
                    }
                    
                except Exception as e:
                    print(f"    âŒ [åˆ†å— {chunk_id}] åˆ†æå¤±è´¥: {e}")
                    return {
                        "chunk_id": chunk_id,
                        "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                        "success": False,
                        "error": str(e),
                        "chunk_tokens": chunk_tokens
                    }
            
            # 4. å¹¶è¡Œå¤„ç†æ‰€æœ‰åˆ†å—ï¼ˆé™åˆ¶å¹¶å‘æ•°é¿å…APIé™åˆ¶ï¼‰
            import asyncio
            semaphore = asyncio.Semaphore(3)  # é™åˆ¶æœ€å¤š3ä¸ªå¹¶å‘
            
            async def limited_analyze(chunk):
                async with semaphore:
                    result = await analyze_chunk(chunk)
                    # æ·»åŠ å°å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
                    await asyncio.sleep(0.1)
                    return result
            
            chunk_results = await asyncio.gather(*[limited_analyze(chunk) for chunk in chunks])
            
            # 5. ç»Ÿè®¡å’Œæ•´ç†ç»“æœ
            successful_chunks = [r for r in chunk_results if r.get("success", False)]
            failed_chunks = [r for r in chunk_results if not r.get("success", False)]
            
            print(f"ğŸ“Š [åˆ†å—åˆ†æ] å®Œæˆï¼š{len(successful_chunks)}/{len(chunks)} ä¸ªåˆ†å—æˆåŠŸ")
            
            # 6. å°†æ‰€æœ‰æˆåŠŸçš„åˆ†æç»“æœç»¼åˆæˆä¸€ä¸ªå®Œæ•´çš„ä¸Šä¸‹æ–‡
            comprehensive_analysis = []
            for result in successful_chunks:
                chunk_id = result['chunk_id']
                analysis = result['analysis']
                comprehensive_analysis.append(f"=== åˆ†å— {chunk_id} åˆ†æç»“æœ ===\\n{analysis}\\n")
            
            # 7. åˆ›å»ºç»¼åˆä¸Šä¸‹æ–‡
            final_context = f"""åŸºäºGraphRAGå±€éƒ¨æœç´¢å’Œåˆ†å—åˆ†æï¼Œä»¥ä¸‹æ˜¯å…³äº"{query}"çš„ç»¼åˆä¿¡æ¯ï¼š

{"".join(comprehensive_analysis)}

=== ç»¼åˆä¿¡æ¯æ€»ç»“ ===
ä»¥ä¸Šæ˜¯åŸºäº {len(successful_chunks)} ä¸ªæ•°æ®åˆ†å—çš„è¯¦ç»†å±€éƒ¨åˆ†æç»“æœã€‚æ¯ä¸ªåˆ†å—éƒ½ç»è¿‡äº†ç‹¬ç«‹çš„LLMåˆ†æï¼Œé‡ç‚¹å…³æ³¨å…·ä½“ç»†èŠ‚å’Œç²¾ç¡®ä¿¡æ¯ã€‚

åŸå§‹æ£€ç´¢ä¿¡æ¯ï¼š
- æ€»tokenæ•°ï¼š{retrieved_context['original_tokens']}
- åˆ†å—æ•°é‡ï¼š{len(chunks)}
- æˆåŠŸåˆ†æï¼š{len(successful_chunks)}ä¸ªåˆ†å—
- å¤±è´¥åˆ†æï¼š{len(failed_chunks)}ä¸ªåˆ†å—"""
            
            print(f"âœ… [å®Œæ•´æµç¨‹] å±€éƒ¨æœç´¢å’Œåˆ†å—åˆ†æå®Œæˆï¼Œç”Ÿæˆç»¼åˆä¸Šä¸‹æ–‡")
            
            return {
                "method": "local_full_with_parallel_analysis",
                "query": query,
                "comprehensive_context": final_context,
                "total_chunks": len(chunks),
                "successful_chunks": len(successful_chunks),
                "failed_chunks": len(failed_chunks),
                "chunk_details": chunk_results,
                "context_ready": True,
                "success": True,
                "note": f"å·²å®Œæˆåˆ†å—å¹¶è¡Œåˆ†æï¼Œ{len(successful_chunks)}/{len(chunks)} ä¸ªåˆ†å—æˆåŠŸã€‚Agentå¯ç›´æ¥ä½¿ç”¨comprehensive_contextè¿›è¡Œæœ€ç»ˆå›ç­”ã€‚"
            }
            
        except Exception as e:
            print(f"âŒ [å®Œæ•´æµç¨‹] å±€éƒ¨æœç´¢å¤±è´¥: {e}")
            return {
                "method": "local_full_with_parallel_analysis",
                "query": query,
                "error": str(e),
                "success": False
            }


# å…¨å±€å®ä¾‹ - é»˜è®¤ä½¿ç”¨ ./rag/output
rag_engine = RAGEngine()

# å¤šä¹¦æœ¬ç®¡ç†å™¨
class MultiBookManager:
    """å¤šä¹¦æœ¬ç®¡ç†å™¨ï¼Œç”¨äºç®¡ç†å¤šä¸ªRAGå¼•æ“å®ä¾‹"""
    
    def __init__(self):
        self.engines = {}  # ç¼“å­˜å¼•æ“å®ä¾‹
        self.current_book = None
    
    def add_book(self, book_name: str, book_folder: str):
        """æ·»åŠ æ–°ä¹¦æœ¬"""
        if not os.path.exists(book_folder):
            raise ValueError(f"ä¹¦æœ¬æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {book_folder}")
        
        # åˆ›å»ºæ–°çš„RAGå¼•æ“å®ä¾‹
        self.engines[book_name] = RAGEngine(input_dir=book_folder)
        print(f"âœ… æ·»åŠ ä¹¦æœ¬: {book_name} -> {book_folder}")
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æœ¬ä¹¦ï¼Œè‡ªåŠ¨è®¾ç½®ä¸ºå½“å‰ä¹¦æœ¬
        if self.current_book is None:
            self.current_book = book_name
    
    def switch_book(self, book_name: str):
        """åˆ‡æ¢åˆ°æŒ‡å®šä¹¦æœ¬"""
        if book_name not in self.engines:
            raise ValueError(f"ä¹¦æœ¬ä¸å­˜åœ¨: {book_name}")
        
        if self.current_book == book_name:
            print(f"â„¹ï¸ å·²ç»åœ¨ä¹¦æœ¬: {book_name}")
            return
        
        print(f"ğŸ”„ åˆ‡æ¢åˆ°ä¹¦æœ¬: {book_name}")
        self.current_book = book_name
    
    def get_current_engine(self) -> RAGEngine:
        """è·å–å½“å‰ä¹¦æœ¬çš„å¼•æ“"""
        if self.current_book is None:
            raise ValueError("æ²¡æœ‰é€‰æ‹©ä»»ä½•ä¹¦æœ¬ï¼Œè¯·å…ˆä½¿ç”¨ add_book() æ·»åŠ ä¹¦æœ¬")
        
        return self.engines[self.current_book]
    
    def list_books(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬"""
        return list(self.engines.keys())
    
    def get_current_book(self) -> Optional[str]:
        """è·å–å½“å‰ä¹¦æœ¬åç§°"""
        return self.current_book
    
    def remove_book(self, book_name: str):
        """ç§»é™¤ä¹¦æœ¬"""
        if book_name in self.engines:
            del self.engines[book_name]
            print(f"âœ… ç§»é™¤ä¹¦æœ¬: {book_name}")
            
            # å¦‚æœç§»é™¤çš„æ˜¯å½“å‰ä¹¦æœ¬ï¼Œåˆ‡æ¢åˆ°å…¶ä»–ä¹¦æœ¬
            if self.current_book == book_name:
                if self.engines:
                    first_book = list(self.engines.keys())[0]
                    self.current_book = first_book
                else:
                    self.current_book = None


# å…¨å±€å¤šä¹¦æœ¬ç®¡ç†å™¨å®ä¾‹
multi_book_manager = MultiBookManager()