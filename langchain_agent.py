import os
import json
import asyncio
from typing import Dict, Any

# ç¡®ä¿ä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“
# pip install langchain langchain-openai

# æ³¨æ„é…ç½®OPENAI_API_KEYä»¥åŠgraphragæ‰€åœ¨è·¯å¾„(ä»£ç ç¬¬172è¡Œ)

from dotenv import load_dotenv

load_dotenv("./.env")

# ä¼˜å…ˆè¯»å– OPENAI_API_KEYï¼Œå…¶æ¬¡ AZURE_OPENAI_API_KEYï¼Œä¸è¦æŠŠå¯†é’¥å½“ä½œç¯å¢ƒå˜é‡å
api_key =os.getenv("AZURE_OPENAI_API_KEY") or ""

import tiktoken
from langchain.agents import tool
from langchain.agents import create_react_agent, AgentExecutor, create_tool_calling_agent
from langchain import hub
from langchain_openai import ChatOpenAI,AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from search.rag_engine import rag_engine, multi_book_manager, RAGEngine
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import prompt_utils
import prompt

from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# 1. åˆå§‹åŒ–ä¸€ä¸ªç”¨äºæ€»ç»“çš„LLM
# å¯ä»¥ç”¨ä¸€ä¸ªä¾¿å®œã€å¿«é€Ÿçš„æ¨¡å‹æ¥åšæ€»ç»“ï¼Œä¹Ÿå¯ä»¥ç”¨ä¸»æ¨¡å‹
llm_hist = AzureChatOpenAI(
        openai_api_version="2024-12-01-preview",
        azure_deployment="gpt-4o",
        model_name="gpt-4o",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.1,   # æ›´é«˜åˆ›é€ æ€§
        max_tokens=2000     # ä»1000å¢åŠ åˆ°2000
)

# 2. åˆ›å»ºå¸¦æ€»ç»“åŠŸèƒ½çš„Memory
# å½“tokenè¶…è¿‡1000æ—¶ï¼Œå¼€å§‹å°†æ—§æ¶ˆæ¯æ€»ç»“
memory = ConversationSummaryBufferMemory(
    llm=llm_hist,
    max_token_limit=1500,  # è®¾ç½®tokené™åˆ¶
    memory_key="chat_history", # ä¸promptä¸­çš„keyå¯¹åº”
    return_messages=True,
)


class GraphAnalysisAgent:
    def __init__(self, use_multi_book=True):
        self.use_multi_book = use_multi_book
        if use_multi_book:
            self.rag_engine = multi_book_manager
            self.current_engine = None
        else:
            self.rag_engine = rag_engine
            self.current_engine = rag_engine
        
    def add_book(self, book_name: str, book_folder: str):
        """æ·»åŠ æ–°ä¹¦æœ¬"""
        if self.use_multi_book:
            self.rag_engine.add_book(book_name, book_folder)
        else:
            print("å½“å‰ä½¿ç”¨çš„æ˜¯å•ä¹¦æœ¬å¼•æ“ï¼Œè¯·è®¾ç½® use_multi_book=True æ¥å¯ç”¨å¤šä¹¦æœ¬åŠŸèƒ½")
    
    def switch_book(self, book_name: str):
        """åˆ‡æ¢åˆ°æŒ‡å®šä¹¦æœ¬"""
        if self.use_multi_book:
            self.rag_engine.switch_book(book_name)
            self.current_engine = self.rag_engine.get_current_engine()
        else:
            print("å½“å‰ä½¿ç”¨çš„æ˜¯å•ä¹¦æœ¬å¼•æ“ï¼Œè¯·è®¾ç½® use_multi_book=True æ¥å¯ç”¨å¤šä¹¦æœ¬åŠŸèƒ½")
    
    def list_books(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬"""
        if self.use_multi_book:
            return self.rag_engine.list_books()
        else:
            return ["default_book"]
    
    def get_current_book(self):
        """è·å–å½“å‰ä¹¦æœ¬åç§°"""
        if self.use_multi_book:
            return self.rag_engine.get_current_book()
        else:
            return "default_book"
    
    def _get_engine(self):
        """è·å–å½“å‰å¼•æ“"""
        if self.use_multi_book:
            if self.current_engine is None:
                # å¦‚æœè¿˜æ²¡æœ‰é€‰æ‹©ä¹¦æœ¬ï¼Œè¿”å›Noneè®©agentæç¤ºç”¨æˆ·é€‰æ‹©
                return None
            return self.current_engine
        else:
            return self.rag_engine
        
    async def global_search_retrieve_async(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "global_retrieve",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.global_search_retrieve(query)
    
    async def global_search_generate_async(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "global_generate",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.global_search_generate(query, retrieved_context)
    
    async def global_search_full_async(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "global_full",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.global_search_full(query)
    
    async def local_search_retrieve_async(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "local_retrieve",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.local_search_retrieve(query)
    
    async def local_search_generate_async(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "local_generate",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.local_search_generate(query, retrieved_context)
    
    async def local_search_full_async(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "local_full",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.local_search_full(query)

    async def get_characters_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ—å‡ºæ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²")

    async def get_relationships_async(self, p1: str, p2: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"åˆ†æ{p1}å’Œ{p2}ä¹‹é—´çš„å…³ç³»")

    async def get_important_locations_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹å’Œåœºæ™¯")

    async def background_knowledge_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†")
    
    async def get_worldview_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®š")

    async def get_character_profile_async(self, character_name: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–{character_name}çš„è¯¦ç»†ä¿¡æ¯")
    
    async def get_significant_event_async(self, event_name:str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–äº‹ä»¶{event_name}çš„è¯¦ç»†ä¿¡æ¯")
    
    async def get_main_theme_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹çš„ä¸»é¢˜")
    async def mock_coversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æ¨¡æ‹Ÿ{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
    async def get_open_questions_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("æœ¬ä¹¦æœ‰ä»€ä¹ˆæ‚¬å¿µæˆ–è€…æ²¡æœ‰è§£å†³çš„ä¼ç¬”ï¼Ÿ")
    async def get_conflict_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("ç½—åˆ—å‡ºæœ¬ä¹¦æœ€å¤§çš„å†²çªæ˜¯ä»€ä¹ˆ")
    async def get_related_characters_async(self, event: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–{event}äº‹ä»¶çš„å…³è”äººç‰©")
    async def get_causal_chains_async(self, event: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è·å–{event}äº‹ä»¶çš„å› æœé“¾ï¼šå‰ç½®æ¡ä»¶â†’è§¦å‘â†’ç»“æœâ†’åæœ")
    async def style_guardrails_async(self, persona: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚")
    async def canon_alignment_async(self, text: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}")
    async def contradiction_test_async(self, text: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}")
    async def continue_story_async(self, brief: str, persona: str = "ä¿æŒä¸åŸè‘—ä¸€è‡´çš„å™è¿°è€…å£å»ä¸è§’è‰²å¯¹ç™½é£æ ¼", target_style: str = "ç´§å‡‘ã€å…·è±¡ç»†èŠ‚ã€å¯¹ç™½æ¨åŠ¨å‰§æƒ…", words_per_scene: int = 600, max_iters: int = 2) -> Dict[str, Any]:
        return await self.local_search_full_async(f"ä¸ºä»¥ä¸‹å¤§çº²ç»­å†™ä¸€ä¸ªåœºæ™¯ï¼ˆä¸è¶…è¿‡{words_per_scene}è¯ï¼‰ï¼š{brief[:3000]}")
    async def imagine_conversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æƒ³è±¡{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
    async def extract_quotes_async(self, name:str, n:int=8) -> Dict[str, Any]:
        q = f"åˆ—å‡º{name}æœ€å…·ä»£è¡¨æ€§çš„å°è¯{n}æ¡ï¼ˆæ¯æ¡<=40å­—ï¼Œé™„ç« èŠ‚/æ®µè½ç¼–å·ï¼‰ï¼Œä¸¥æ ¼JSONæ•°ç»„ï¼š"
        return await self.local_search_full_async(q)
    async def narrative_pov_async(self) -> Dict[str, Any]:
        q = """
    åˆ†æå™äº‹è§†è§’ä¸å¯é æ€§ï¼šPOVç±»å‹ã€åˆ‡æ¢ç‚¹ã€å¯èƒ½åè§/è¯¯å¯¼çš„è¯æ®ã€‚ç”¨åˆ†ç‚¹åˆ—å‡ºï¼Œæ¯ç‚¹é™„<=40å­—çŸ­æ‘˜+ç« èŠ‚ã€‚
    """
        return await self.global_search_full_async(q)
    async def get_motifs_symbols_async(self, max_items:int=20) -> Dict[str, Any]:
        q = f"""
    æŠ½å–æ„è±¡/æ¯é¢˜/è±¡å¾ï¼ˆæœ€å¤š{max_items}æ¡ï¼‰ï¼Œä¸¥æ ¼JSONï¼š
    [{{"motif":"â€¦","meaning":"â€¦","linked_themes":["â€¦"],"chapters":["â€¦"],"evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}]
    """
        return await self.local_search_full_async(q)
    async def build_story_outline_async(self, brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> Dict[str, Any]:
        q = f"""
    åŸºäºåŸè‘—çº¦æŸï¼Œä¸º"{brief}"ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚
    é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚
    """
        return await self.global_search_full_async(q)
    async def emotion_curve_async(self, scope:str="å…¨ä¹¦") -> Dict[str, Any]:
        q = f"æå–{scope}çš„æƒ…æ„Ÿæ›²çº¿å…³é”®è½¬æŠ˜ï¼ˆå–œ/æ€’/å“€/æƒ§/æƒŠ/åŒ/ä¿¡ï¼‰ï¼Œåˆ—å‡ºè½¬æŠ˜ç‚¹ç« èŠ‚ä¸è§¦å‘äº‹ä»¶ï¼Œå„ç»™<=40å­—çŸ­æ‘˜ã€‚"
        return await self.global_search_full_async(q)
    async def compare_characters_async(self, a:str, b:str) -> Dict[str, Any]:
        q = f"""
    æ¯”è¾ƒ{a}ä¸{b}ï¼Œä¸¥æ ¼JSONï¼š
    {{"values":["â€¦"],"goals":["â€¦"],"methods":["â€¦"],"red_lines":["â€¦"],"decision_style":"å†²åŠ¨|è°¨æ…|ç®—è®¡","evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}
    """
        return await self.global_search_full_async(q)

# --- ç¬¬äºŒæ­¥ï¼šåˆ›å»º LangChain Agent ---
def create_graphrag_agent(graphrag_agent_instance: GraphAnalysisAgent) -> AgentExecutor:
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå¯ä»¥è°ƒç”¨ GraphRAG å‘½ä»¤è¡ŒåŠŸèƒ½çš„ LangChain Agentã€‚
    """
    # ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œå°† GraphAnalysisAgent çš„æ–¹æ³•åŒ…è£…æˆ LangChain å·¥å…·
    # æ³¨æ„ï¼šè¿™é‡Œçš„å·¥å…·å‡½æ•°éœ€è¦èƒ½å¤Ÿè¢« Agent ç›´æ¥è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨é—­åŒ…æ¥ä¼ é€’å®ä¾‹
    
    @tool
    async def list_available_books_tool() -> str:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬ã€‚å¦‚æœè¿˜æ²¡æœ‰æ·»åŠ ä»»ä½•ä¹¦æœ¬ï¼Œä¼šæç¤ºç”¨æˆ·æ·»åŠ ä¹¦æœ¬ã€‚"""
        books = graphrag_agent_instance.list_books()
        if not books:
            return json.dumps({
                "message": "è¿˜æ²¡æœ‰æ·»åŠ ä»»ä½•ä¹¦æœ¬ã€‚è¯·ä½¿ç”¨ add_book_tool æ·»åŠ ä¹¦æœ¬ã€‚",
                "books": [],
                "success": False
            }, ensure_ascii=False)
        
        current_book = graphrag_agent_instance.get_current_book()
        return json.dumps({
            "message": f"å¯ç”¨çš„ä¹¦æœ¬ï¼š{', '.join(books)}ã€‚å½“å‰é€‰æ‹©çš„ä¹¦æœ¬ï¼š{current_book}",
            "books": books,
            "current_book": current_book,
            "success": True
        }, ensure_ascii=False)
    
    @tool
    async def add_book_tool(book_name: str, book_folder: str) -> str:
        """æ·»åŠ æ–°ä¹¦æœ¬åˆ°ç³»ç»Ÿä¸­ã€‚book_nameæ˜¯ä¹¦æœ¬çš„æ˜¾ç¤ºåç§°ï¼Œbook_folderæ˜¯ä¹¦æœ¬æ•°æ®æ–‡ä»¶å¤¹çš„è·¯å¾„ã€‚"""
        try:
            graphrag_agent_instance.add_book(book_name, book_folder)
            return json.dumps({
                "message": f"æˆåŠŸæ·»åŠ ä¹¦æœ¬ï¼š{book_name} -> {book_folder}",
                "success": True
            }, ensure_ascii=False)
        except Exception as e:
            return json.dumps({
                "message": f"æ·»åŠ ä¹¦æœ¬å¤±è´¥ï¼š{str(e)}",
                "success": False
            }, ensure_ascii=False)
    
    @tool
    async def switch_book_tool(book_name: str) -> str:
        """åˆ‡æ¢åˆ°æŒ‡å®šçš„ä¹¦æœ¬ã€‚book_nameæ˜¯è¦åˆ‡æ¢åˆ°çš„ä¹¦æœ¬åç§°ã€‚"""
        try:
            graphrag_agent_instance.switch_book(book_name)
            return json.dumps({
                "message": f"æˆåŠŸåˆ‡æ¢åˆ°ä¹¦æœ¬ï¼š{book_name}",
                "current_book": book_name,
                "success": True
            }, ensure_ascii=False)
        except Exception as e:
            return json.dumps({
                "message": f"åˆ‡æ¢ä¹¦æœ¬å¤±è´¥ï¼š{str(e)}",
                "success": False
            }, ensure_ascii=False)
    
    @tool
    async def get_current_book_tool() -> str:
        """è·å–å½“å‰é€‰æ‹©çš„ä¹¦æœ¬åç§°ã€‚"""
        current_book = graphrag_agent_instance.get_current_book()
        return json.dumps({
            "message": f"å½“å‰é€‰æ‹©çš„ä¹¦æœ¬ï¼š{current_book}",
            "current_book": current_book,
            "success": True
        }, ensure_ascii=False)

    # === æ–°å¢ï¼šRAGæ£€ç´¢åˆ†ç¦»å·¥å…· ===
    @tool
    async def global_search_retrieve_tool(query: str) -> str:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›çš„å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°GraphRAGæ£€ç´¢åˆ°äº†å“ªäº›ç›¸å…³æ–‡æ¡£å’Œä¸Šä¸‹æ–‡ï¼Œä½†ä¸è¿›è¡ŒLLMç”Ÿæˆã€‚"""
        result = await graphrag_agent_instance.global_search_retrieve_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def global_search_generate_tool(query: str, retrieved_context: str) -> str:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆã€‚éœ€è¦å…ˆè°ƒç”¨global_search_retrieve_toolè·å–ä¸Šä¸‹æ–‡ã€‚"""
        try:
            print(f"ğŸ¤– [Agent LLM] æ­£åœ¨ä½¿ç”¨agentçš„LLMå·¥å…·ç”Ÿæˆå…¨å±€æœç´¢å›ç­”: {query}")
            
            # è§£ææ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ - å¢åŠ æ›´å¥å£®çš„é”™è¯¯å¤„ç†
            import json
            try:
                if isinstance(retrieved_context, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    context_data = json.loads(retrieved_context)
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    context_data = retrieved_context
            except json.JSONDecodeError as e:
                print(f"â„¹ï¸ [Agent LLM] æ£€æµ‹åˆ°éJSONæ ¼å¼æ•°æ®ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                context_text = str(retrieved_context)
            else:
                # JSONè§£ææˆåŠŸï¼Œæå–ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_text = context_data.get('retrieved_context', {}).get('context_text', '')
                if not context_text:
                    # å¦‚æœåµŒå¥—ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®
                    context_text = context_data.get('context_text', str(context_data))
            
            # æ„å»ºæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å…·ä½“è¯¦ç»†ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®
2. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
3. å¦‚æœæ¶‰åŠäººç‰©ï¼Œè¦è¯´æ˜ä»–ä»¬çš„è§’è‰²ã€ç‰¹ç‚¹å’Œé‡è¦æ€§
4. å¦‚æœæ¶‰åŠäº‹ä»¶ï¼Œè¦æè¿°å…¶èƒŒæ™¯ã€è¿‡ç¨‹å’Œå½±å“
5. å›ç­”é•¿åº¦åº”è¯¥åœ¨200-500å­—ä¹‹é—´ï¼Œç¡®ä¿ä¿¡æ¯å……åˆ†ä½†ä¸è¿‡äºå†—é•¿"""
            
            # ä½¿ç”¨agentçš„LLMå·¥å…·è¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [Agent LLM] å…¨å±€æœç´¢ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè€Œä¸æ˜¯JSONæ ¼å¼
            return result_text
            
        except Exception as e:
            print(f"âŒ [Agent LLM] å…¨å±€æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "agent_global_generate",
                "query": query,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_retrieve_tool(query: str) -> str:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›çš„å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°GraphRAGæ£€ç´¢åˆ°äº†å“ªäº›ç›¸å…³æ–‡æ¡£å’Œä¸Šä¸‹æ–‡ï¼Œä½†ä¸è¿›è¡ŒLLMç”Ÿæˆã€‚"""
        result = await graphrag_agent_instance.local_search_retrieve_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_generate_tool(query: str, retrieved_context: str) -> str:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆã€‚éœ€è¦å…ˆè°ƒç”¨local_search_retrieve_toolè·å–ä¸Šä¸‹æ–‡ã€‚"""
        try:
            print(f"ğŸ¤– [Agent LLM] æ­£åœ¨ä½¿ç”¨agentçš„LLMå·¥å…·ç”Ÿæˆå±€éƒ¨æœç´¢å›ç­”: {query}")
            
            # è§£ææ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ - å¢åŠ æ›´å¥å£®çš„é”™è¯¯å¤„ç†
            import json
            try:
                if isinstance(retrieved_context, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    context_data = json.loads(retrieved_context)
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    context_data = retrieved_context
            except json.JSONDecodeError as e:
                print(f"â„¹ï¸ [Agent LLM] æ£€æµ‹åˆ°éJSONæ ¼å¼æ•°æ®ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                context_text = str(retrieved_context)
            else:
                # JSONè§£ææˆåŠŸï¼Œæå–ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_text = context_data.get('retrieved_context', {}).get('context_text', '')
                if not context_text:
                    # å¦‚æœåµŒå¥—ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®
                    context_text = context_data.get('context_text', str(context_data))
            
            # æ„å»ºæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å…·ä½“è¯¦ç»†ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®
2. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
3. å¦‚æœæ¶‰åŠäººç‰©ï¼Œè¦è¯´æ˜ä»–ä»¬çš„è§’è‰²ã€ç‰¹ç‚¹å’Œé‡è¦æ€§
4. å¦‚æœæ¶‰åŠäº‹ä»¶ï¼Œè¦æè¿°å…¶èƒŒæ™¯ã€è¿‡ç¨‹å’Œå½±å“
5. å›ç­”é•¿åº¦åº”è¯¥åœ¨200-500å­—ä¹‹é—´ï¼Œç¡®ä¿ä¿¡æ¯å……åˆ†ä½†ä¸è¿‡äºå†—é•¿"""
            
            # ä½¿ç”¨agentçš„LLMå·¥å…·è¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [Agent LLM] å±€éƒ¨æœç´¢ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè€Œä¸æ˜¯JSONæ ¼å¼
            return result_text
            
        except Exception as e:
            print(f"âŒ [Agent LLM] å±€éƒ¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "agent_local_generate",
                "query": query,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    # === æ–°å¢ï¼šç‹¬ç«‹LLMè°ƒç”¨å·¥å…· ===
    @tool
    async def llm_generate_tool(prompt: str, context: str = "") -> str:
        """ç‹¬ç«‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚è¾“å…¥promptæ˜¯ç”Ÿæˆæç¤ºï¼Œcontextæ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°LLMæ­£åœ¨ç”Ÿæˆå†…å®¹ã€‚"""
        try:
            print(f"ğŸ¤– [ç‹¬ç«‹LLM] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
            print(f"æç¤º: {prompt[:100]}...")
            if context:
                print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # ä½¿ç”¨llm_genè¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = []
            if context:
                messages.append(SystemMessage(content=f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ï¼š\n\n{context}"))
            messages.append(HumanMessage(content=prompt))
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [ç‹¬ç«‹LLM] ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            return json.dumps({
                "method": "llm_generate",
                "prompt": prompt,
                "context_length": len(context) if context else 0,
                "response": result_text,
                "success": True
            }, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âŒ [ç‹¬ç«‹LLM] ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "llm_generate",
                "prompt": prompt,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def llm_analyze_tool(text: str, analysis_type: str = "general") -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬ã€‚è¾“å…¥textæ˜¯è¦åˆ†æçš„æ–‡æœ¬ï¼Œanalysis_typeæ˜¯åˆ†æç±»å‹ï¼ˆå¦‚'character', 'theme', 'plot'ç­‰ï¼‰ã€‚"""
        try:
            print(f"ğŸ¤– [LLMåˆ†æ] æ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬...")
            print(f"åˆ†æç±»å‹: {analysis_type}")
            print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # æ ¹æ®åˆ†æç±»å‹æ„å»ºæç¤º
            if analysis_type == "character":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­çš„äººç‰©ç‰¹å¾ã€æ€§æ ¼ã€åŠ¨æœºç­‰ï¼š\n\n{text}"
            elif analysis_type == "theme":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»é¢˜ã€è±¡å¾æ„ä¹‰ã€æ·±å±‚å«ä¹‰ï¼š\n\n{text}"
            elif analysis_type == "plot":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…èŠ‚å‘å±•ã€å†²çªã€è½¬æŠ˜ç‚¹ï¼š\n\n{text}"
            else:
                prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œ{analysis_type}åˆ†æï¼š\n\n{text}"
            
            from langchain_core.messages import HumanMessage
            response = await llm_gen.ainvoke([HumanMessage(content=prompt)])
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [LLMåˆ†æ] åˆ†æå®Œæˆï¼Œç»“æœé•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            return json.dumps({
                "method": "llm_analyze",
                "analysis_type": analysis_type,
                "text_length": len(text),
                "analysis_result": result_text,
                "success": True
            }, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âŒ [LLMåˆ†æ] åˆ†æå¤±è´¥: {e}")
            return json.dumps({
                "method": "llm_analyze",
                "analysis_type": analysis_type,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    # === åŸæœ‰å·¥å…· ===
    @tool
    async def get_characters_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²ã€‚"""
        result = await graphrag_agent_instance.get_characters_async()
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_relationships_tool(p1: str, p2: str) -> str:
        """è·å–ä¸¤ä¸ªç‰¹å®šäººç‰©ä¹‹é—´çš„å…³ç³»ã€‚è¾“å…¥å‚æ•°p1å’Œp2æ˜¯äººç‰©åç§°ã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸¤ä¸ªäººç‰©çš„å…³ç³»ï¼Œå¯ä»¥å°è¯•å•ç‹¬æŸ¥è¯¢ä¸¤ä¸ªäººç‰©çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå¹¶ä¸”å°è¯•æ‰¾åˆ°å’Œä»–ä»¬å…±åŒç›¸å…³çš„äººæ¥åˆ¤æ–­ä»–ä»¬ä¹‹é—´å¯èƒ½çš„å…³ç³»"""
        result = await graphrag_agent_instance.get_relationships_async(p1, p2)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_important_locations_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹ã€‚"""
        result = await graphrag_agent_instance.get_important_locations_async()
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def background_knowledge_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†ã€‚"""
        result = await graphrag_agent_instance.background_knowledge_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def get_worldview_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®šã€‚"""
        result = await graphrag_agent_instance.get_worldview_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_tool(query: str) -> str:
        """ä½¿ç”¨ GraphRAG çš„å±€éƒ¨æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
        result = await graphrag_agent_instance.local_search_full_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def global_search_tool(query: str) -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
        result = await graphrag_agent_instance.global_search_full_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool 
    async def get_character_profile_tool(character_name: str) -> str:
        """è·å–ç‰¹å®šäººç‰©çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°character_nameæ˜¯äººç‰©åç§°ã€‚"""
        result = await graphrag_agent_instance.get_character_profile_async(character_name)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_significant_event_tool(event_name: str) -> str:
        """è·å–ç‰¹å®šäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°event_nameæ˜¯äº‹ä»¶åç§°ã€‚"""
        result = await graphrag_agent_instance.get_significant_event_async(event_name)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_main_theme_tool() -> str:
        """è·å–æ•…äº‹çš„ä¸»é¢˜ã€‚"""
        result = await graphrag_agent_instance.get_main_theme_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool 
    async def get_open_questions_tool() -> str:
        """è·å–æœ¬ä¹¦çš„æ‚¬å¿µæˆ–è€…æœªè§£å†³çš„ä¼ç¬”ã€‚"""
        result = await graphrag_agent_instance.get_open_questions_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_causal_chains_tool(event: str) -> str:
        """è·å–ç»™å®šäº‹ä»¶çš„å› æœé“¾ã€‚å¯ä»¥çŸ¥é“æ˜¯ä»€ä¹ˆå¯¼è‡´çš„è¯¥äº‹ä»¶ï¼Œç„¶åè¯¥äº‹ä»¶å¯¼è‡´äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼Œæœ€åç»“æœåˆå¯¼è‡´äº†ä»€ä¹ˆæ ·çš„åæœ"""
        result = await graphrag_agent_instance.get_causal_chains_async(event)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def style_guardrails_tool(persona: str) -> str:
        """äº§å‡ºé£æ ¼æŠ¤æ ï¼šå…è®¸/ç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€è§†è§’ã€èŠ‚å¥ç­‰ï¼ˆä¾›ç»­å†™éµå®ˆï¼‰"""
        q = f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚"
        res = await graphrag_agent_instance.global_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)

    @tool
    async def canon_alignment_tool(text: str) -> str:
        """è¯„ä¼°æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™ä¸€è‡´æ€§ï¼ˆè§’è‰²OOC/è®¾å®šè¿èƒŒ/å†å²è¿èƒŒï¼‰ï¼Œç»™è¦ç‚¹ä¸ä¾æ®"""
        q = f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}"
        res = await graphrag_agent_instance.local_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)

    @tool
    async def contradiction_test_tool(text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼Œç»™å‡ºåŸæ–‡è¯æ®ç‰‡æ®µå®šä½"""
        q = f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}"
        res = await graphrag_agent_instance.local_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)
    @tool
    async def get_conflict_tool() -> str:
        """è·å–æœ¬ä¹¦æœ€å¤§çš„å†²çªã€‚"""
        result = await graphrag_agent_instance.get_conflict_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_related_characters_tool(event: str) -> str:
        """è·å–ç»™å®šäº‹ä»¶çš„å…³è”äººç‰©ã€‚"""
        result = await graphrag_agent_instance.get_related_characters_async(event)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def imagine_conversation_tool(character1_name: str, character2_name: str) -> str:
        """æƒ³è±¡ä¸¤ä¸ªè§’è‰²ä¹‹é—´çš„å¯¹è¯ã€‚"""
        result = await graphrag_agent_instance.imagine_conversation_async(character1_name, character2_name)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def extract_quotes_tool(name:str, n:int=8) -> str:
        """è·å–ç‰¹å®šäººç‰©çš„å°è¯ã€‚"""
        result = await graphrag_agent_instance.extract_quotes_async(name, n)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def narrative_pov_tool() -> str:
        """è·å–æœ¬ä¹¦çš„å™äº‹è§†è§’ã€‚"""
        result = await graphrag_agent_instance.narrative_pov_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_motifs_symbols_tool(max_items:int=20) -> str:
        """è·å–æœ¬ä¹¦çš„æ„è±¡/æ¯é¢˜/è±¡å¾ã€‚"""
        result = await graphrag_agent_instance.get_motifs_symbols_async(max_items)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def build_story_outline_tool(brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> str:
        """åŸºäºåŸè‘—çº¦æŸï¼Œä¸º"{brief}"ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚"""
        result = await graphrag_agent_instance.build_story_outline_async(brief, target_style)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def emotion_curve_tool(scope:str="å…¨ä¹¦") -> str:
        """è·å–æœ¬ä¹¦çš„æƒ…æ„Ÿæ›²çº¿ã€‚"""
        result = await graphrag_agent_instance.emotion_curve_async(scope)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def compare_characters_tool(a:str, b:str) -> str:
        """æ¯”è¾ƒä¸¤ä¸ªè§’è‰²ã€‚"""
        result = await graphrag_agent_instance.compare_characters_async(a, b)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def system_status_tool() -> str:
        """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯ç”¨å·¥å…·ã€å¤„ç†èƒ½åŠ›ç­‰"""
        return json.dumps({
            "system_status": "running",
            "available_tools": [
                "global_search_tool", "local_search_tool", "get_characters_tool",
                "get_relationships_tool", "background_knowledge_tool", "llm_generate_tool",
                "llm_analyze_tool", "get_character_profile_tool", "get_worldview_tool"
            ],
            "capabilities": [
                "äººç‰©åˆ†æ", "å…³ç³»åˆ†æ", "èƒŒæ™¯çŸ¥è¯†æŸ¥è¯¢", "æƒ…èŠ‚åˆ†æ", "æ–‡æœ¬ç”Ÿæˆ", "åˆ›æ„å†™ä½œ"
            ],
            "specialization": "ã€Šæ²™ä¸˜ã€‹(Dune)ç³»åˆ—å°è¯´åˆ†æ",
            "note": "ç³»ç»Ÿå·²ä¼˜åŒ–ï¼Œæ”¯æŒè¯¦ç»†å›ç­”å’Œç”¨æˆ·å‹å¥½çš„çŠ¶æ€æç¤º"
        }, ensure_ascii=False, default=str)
    @tool 
    async def get_people_location_relation_tool(people:str, location:str, relation:str) -> str:
        """è·å–ç‰¹å®šäººç‰©å’Œåœ°ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚"""
        result = await graphrag_agent_instance.get_people_location_relation_async(people, location, relation)
        return json.dumps(result, ensure_ascii=False, default=str)
    tools = [
        # ä¹¦æœ¬ç®¡ç†å·¥å…·ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        list_available_books_tool,
        add_book_tool,
        switch_book_tool,
        get_current_book_tool,
        
        # === æ–°å¢çš„RAGæ£€ç´¢åˆ†ç¦»å·¥å…· ===
        # global_search_retrieve_tool,
        # # global_search_generate_tool,
        # local_search_retrieve_tool,
        # local_search_generate_tool,
        
        # === æ–°å¢ï¼šç‹¬ç«‹LLMè°ƒç”¨å·¥å…· ===
        # llm_generate_tool,
        # llm_analyze_tool,
        
        # === åŸæœ‰å·¥å…· ===
        get_characters_tool,
        get_relationships_tool,
        get_important_locations_tool,
        get_significant_event_tool,
        background_knowledge_tool,
        get_worldview_tool,
        local_search_tool,
        global_search_tool,
        get_character_profile_tool,
        get_main_theme_tool,
        get_open_questions_tool,
        get_causal_chains_tool,
        style_guardrails_tool,
        canon_alignment_tool,
        contradiction_test_tool,
        get_conflict_tool,
        get_related_characters_tool,
        imagine_conversation_tool,
        extract_quotes_tool,
        narrative_pov_tool,
        get_motifs_symbols_tool, 
        build_story_outline_tool,
        emotion_curve_tool,
        compare_characters_tool,
        system_status_tool,
        get_people_location_relation_tool,
    ]

    # åˆå§‹åŒ– LLM
    # ç¡®ä¿ä½ å·²ç»è®¾ç½®äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡
    llm = AzureChatOpenAI(
        openai_api_version="2024-12-01-preview",
        azure_deployment="gpt-4o",
        model_name="gpt-4o",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.3,
        max_tokens=2000,  # ä»800å¢åŠ åˆ°2000
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    llm_gen = AzureChatOpenAI(
        openai_api_version="2024-12-01-preview",
        azure_deployment="gpt-4o",
        model_name="gpt-4o",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.85,   # æ›´é«˜åˆ›é€ æ€§
        max_tokens=2000     # ä»1000å¢åŠ åˆ°2000
    )

# ### RAGæ£€ç´¢åˆ†ç¦»å·¥å…·ï¼š
# - **global_search_retrieve_tool**: ä»…è¿›è¡Œå…¨å±€æœç´¢çš„æ£€ç´¢ï¼Œå±•ç¤ºGraphRAGå¬å›çš„å†…å®¹
# - **global_search_generate_tool**: ä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆ
# - **local_search_retrieve_tool**: ä»…è¿›è¡Œå±€éƒ¨æœç´¢çš„æ£€ç´¢ï¼Œå±•ç¤ºGraphRAGå¬å›çš„å†…å®¹  
# - **local_search_generate_tool**: ä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆ

# ### ç‹¬ç«‹LLMè°ƒç”¨å·¥å…·ï¼š
# - **llm_generate_tool**: ç‹¬ç«‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œç”¨æˆ·å¯ä»¥æ¸…æ¥šçœ‹åˆ°LLMæ­£åœ¨ç”Ÿæˆå†…å®¹
# - **llm_analyze_tool**: ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬ï¼Œæ”¯æŒä¸åŒç±»å‹çš„åˆ†æï¼ˆcharacter, theme, plotç­‰ï¼‰


# ## é‡è¦è¯´æ˜ï¼šRAGæ£€ç´¢åˆ†ç¦»å·¥å…·å’Œç‹¬ç«‹LLMè°ƒç”¨
# ç°åœ¨ä½ æœ‰æ–°çš„å·¥å…·å¯ä»¥åˆ†ç¦»RAGçš„æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥åŠç‹¬ç«‹çš„LLMè°ƒç”¨ï¼š



# ### ä½¿ç”¨å»ºè®®ï¼š
# 1. **å±•ç¤ºRAGè¿‡ç¨‹**ï¼šå…ˆè°ƒç”¨ *_retrieve_tool å±•ç¤ºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Œå†è°ƒç”¨ *_generate_tool è¿›è¡ŒLLMç”Ÿæˆ
# 2. **ç‹¬ç«‹LLMè°ƒç”¨**ï¼šå½“éœ€è¦åˆ›é€ æ€§å†…å®¹æˆ–å¤æ‚åˆ†ææ—¶ï¼Œä½¿ç”¨ llm_generate_tool æˆ– llm_analyze_tool
# 3. **å®Œæ•´æµç¨‹**ï¼šæˆ–è€…ç›´æ¥ä½¿ç”¨åŸæœ‰çš„å®Œæ•´å·¥å…·ï¼ˆå¦‚ global_search_toolï¼‰

# ### ç”¨æˆ·å¯è§æ€§ï¼š
# - ğŸ” [RAGæ£€ç´¢] è¡¨ç¤ºæ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯
# - ğŸ¤– [LLMç”Ÿæˆ] è¡¨ç¤ºæ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹
# - âœ… è¡¨ç¤ºæ“ä½œå®Œæˆ
# - âŒ è¡¨ç¤ºæ“ä½œå¤±è´¥

# ### æ™ºèƒ½å†³ç­–æŒ‡å—ï¼š
# - **äººç‰©ç›¸å…³é—®é¢˜**ï¼šä¼˜å…ˆä½¿ç”¨ get_character_profile_tool æˆ– get_characters_tool
# - **å…³ç³»åˆ†æ**ï¼šä½¿ç”¨ get_relationships_tool åˆ†æäººç‰©å…³ç³»
# - **èƒŒæ™¯çŸ¥è¯†**ï¼šä½¿ç”¨ background_knowledge_tool æˆ– get_worldview_tool
# - **æƒ…èŠ‚åˆ†æ**ï¼šä½¿ç”¨ global_search_tool è¿›è¡Œå…¨å±€åˆ†æ
# - **å…·ä½“ç»†èŠ‚**ï¼šä½¿ç”¨ local_search_tool è¿›è¡Œç²¾ç¡®æ£€ç´¢
# - **åˆ›ä½œä»»åŠ¡**ï¼šä½¿ç”¨ llm_generate_tool è¿›è¡Œåˆ›é€ æ€§ç”Ÿæˆ

    # å°† prompt å­—ç¬¦ä¸²é‡å‘½åä¸º prompt_textï¼Œé¿å…ä¸ prompt æ¨¡å—å†²çª
    prompt_text = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆ›ä½œåŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œä¿¡æ¯åˆ†æå’Œæ¢ç´¢ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒæŸ¥æ¥å®Œæˆå¤æ‚çš„åˆ›ä½œä»»åŠ¡ã€‚

### ä¹¦æœ¬ç®¡ç†ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰ï¼š
- **list_available_books_tool**: åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬
- **add_book_tool**: æ·»åŠ æ–°ä¹¦æœ¬åˆ°ç³»ç»Ÿä¸­
- **switch_book_tool**: åˆ‡æ¢åˆ°æŒ‡å®šçš„ä¹¦æœ¬
- **get_current_book_tool**: è·å–å½“å‰é€‰æ‹©çš„ä¹¦æœ¬

### å†å²è®°å½•
{{chat_history}}

## è°ƒæŸ¥å‘¨æœŸ (Investigation Cycle)
ä½ æŒ‰ç…§ä¸€ä¸ªæŒç»­çš„å‘¨æœŸè¿ä½œï¼š
1. ä»å¤šä¸ªç»´åº¦ç†è§£ç”¨æˆ·è¯‰æ±‚ï¼Œæ‹†è§£ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç”¨æˆ·æ„å›¾
2. æ ¹æ®å†å²å¯¹è¯ï¼Œæ•´åˆæœ‰ç”¨ä¿¡æ¯ä»¥ç†è§£ä»»åŠ¡ç›®æ ‡
3. æ ¹æ®å·²æŒæ¡çš„çº¿ç´¢å’Œä¿¡æ¯ç¼ºå£ï¼Œé¿å…å’Œå†å²å¯¹è¯ä¸­å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨ï¼ˆå·¥å…·å‚æ•°ä¸€è‡´ï¼‰ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„å·¥å…·ï¼Œå†³å®šæ¥ä¸‹æ¥è¦è°ƒç”¨å“ªä¸ªå·¥å…·
4. å½“ä½ è®¤ä¸ºæ²¡å®Œæˆä»»åŠ¡æ—¶æˆ–ç°æœ‰ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "IN_PROGRES"ï¼Œæ­¤æ—¶ä½ å¿…é¡»é€‰æ‹©ä¸€ä¸ªå·¥å…·ã€‚
5. å½“ä½ è®¤ä¸ºå†å²å¯¹è¯çš„ä¿¡æ¯è¶³å¤Ÿä½ å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "DONE"
## å¯ç”¨å·¥å…· (Available Tools)
{{functions}}
## å·¥å…·ä½¿ç”¨å‡†åˆ™ (Tool Usage Guidelines)
{{guidelines}}
## æ³¨æ„äº‹é¡¹
{{requirements}}
å“åº”æ ¼å¼ (Response Format)
{{response_format}}
    """

    prompt_obj = ChatPromptTemplate.from_messages([
        ("system", prompt_text),  # ä½¿ç”¨ prompt_text è€Œä¸æ˜¯ prompt
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # ä½¿ç”¨ partial æ¥é¢„è®¾å˜é‡å€¼
    final_prompt = prompt_obj.partial(
        functions=tools,
        guidelines=prompt.build_guidelines(),  # ç°åœ¨å¯ä»¥æ­£ç¡®è°ƒç”¨ prompt æ¨¡å—
        requirements=prompt.build_requirements(),
        response_format=prompt.build_response_format(),
        history=""  # æ·»åŠ ç©ºçš„historyå˜é‡
    )

    # åˆ›å»º Agent
    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=final_prompt)

    # åˆ›å»º Agent æ‰§è¡Œå™¨
    return AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)

# --- ä¸»ç¨‹åºå…¥å£ ---
async def main() -> None:
    graph_agent = GraphAnalysisAgent(use_multi_book=True)

    # è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬
    print("ğŸ“š æ­£åœ¨è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬...")
    
    # å®šä¹‰è¦åŠ è½½çš„ä¹¦æœ¬åˆ—è¡¨
    books_to_load = [
        ("book4", "./book4/output"),
        ("book5", "./book5/output"), 
        ("book6", "./book6/output"),
        ("tencent", "./tencent/output"),
        ("default", "./rag/output")  # é»˜è®¤çš„rag/output
    ]
    
    loaded_books = []
    for book_name, book_path in books_to_load:
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if os.path.exists(book_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
                required_files = ["communities.parquet", "entities.parquet", "community_reports.parquet", "relationships.parquet", "text_units.parquet"]
                missing_files = [f for f in required_files if not os.path.exists(os.path.join(book_path, f))]
                
                if not missing_files:
                    graph_agent.add_book(book_name, book_path)
                    loaded_books.append(book_name)
                    print(f"âœ… æˆåŠŸåŠ è½½ä¹¦æœ¬: {book_name} -> {book_path}")
                else:
                    print(f"âš ï¸ è·³è¿‡ {book_name}: ç¼ºå°‘å¿…è¦æ–‡ä»¶ {missing_files}")
            else:
                print(f"âš ï¸ è·³è¿‡ {book_name}: è·¯å¾„ä¸å­˜åœ¨ {book_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {book_name} å¤±è´¥: {e}")
    
    print(f"âœ… æ€»å…±åŠ è½½äº† {len(loaded_books)} æœ¬ä¹¦: {', '.join(loaded_books)}")
    
    # å¦‚æœæœ‰ä¹¦æœ¬åŠ è½½æˆåŠŸï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€æœ¬
    if loaded_books:
        first_book = loaded_books[0]
        graph_agent.switch_book(first_book)
        print(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°ç¬¬ä¸€æœ¬ä¹¦: {first_book}")
    else:
        print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•ä¹¦æœ¬ï¼Œè¯·æ‰‹åŠ¨æ·»åŠ ä¹¦æœ¬")

    # ä½¿ç”¨è¿™ä¸ªå®ä¾‹åˆ›å»º LangChain Agent
    agent_executor = create_graphrag_agent(graph_agent)

    print("LangChain Agent with GraphRAG (Python API) tools is ready. Type 'exit' to quit.")
    
    while True:
        user_query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
        if user_query.lower() == 'exit':
            break

        try:
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ŒåŒ¹é…å¼‚æ­¥å·¥å…·
            response = await agent_executor.ainvoke({
                "input": user_query
            })
            
            # æ¢å¤è¾“å‡ºæ˜¾ç¤º
            print("\n--- Agent å›ç­” ---")
            print(response.get("output"))
            print("--------------------\n")
            
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
            break


if __name__ == "__main__":
    asyncio.run(main())
