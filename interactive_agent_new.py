import os
import json
import asyncio
import tiktoken
from typing import Dict, Any

# ç¡®ä¿ä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“
# pip install langchain langchain-openai

# æ³¨æ„é…ç½®OPENAI_API_KEYä»¥åŠgraphragæ‰€åœ¨è·¯å¾„(ä»£ç ç¬¬172è¡Œ)

from dotenv import load_dotenv

load_dotenv("./.env")

# ä¼˜å…ˆè¯»å– OPENAI_API_KEYï¼Œå…¶æ¬¡ AZURE_OPENAI_API_KEYï¼Œä¸è¦æŠŠå¯†é’¥å½“ä½œç¯å¢ƒå˜é‡å
api_key =os.getenv("AZURE_OPENAI_API_KEY") or ""

from langchain.agents import tool
from langchain.agents import create_react_agent, AgentExecutor, create_tool_calling_agent
from langchain import hub
from langchain_openai import ChatOpenAI,AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from search.rag_engine import rag_engine, multi_book_manager, RAGEngine
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import prompt_utils
import prompt

from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# 1. åˆå§‹åŒ–ä¸€ä¸ªç”¨äºæ€»ç»“çš„LLM
# å¯ä»¥ç”¨ä¸€ä¸ªä¾¿å®œã€å¿«é€Ÿçš„æ¨¡å‹æ¥åšæ€»ç»“ï¼Œä¹Ÿå¯ä»¥ç”¨ä¸»æ¨¡å‹
llm_hist = AzureChatOpenAI(
        openai_api_version="2024-12-01-preview",
        azure_deployment="gpt-4o",
        model_name="gpt-4o",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.1,   # æ›´é«˜åˆ›é€ æ€§
        max_tokens=2000     # ä»1000å¢åŠ åˆ°2000
)

# 2. åˆ›å»ºå¸¦æ€»ç»“åŠŸèƒ½çš„Memory
# å½“tokenè¶…è¿‡1000æ—¶ï¼Œå¼€å§‹å°†æ—§æ¶ˆæ¯æ€»ç»“
memory = ConversationSummaryBufferMemory(
    llm=llm_hist,
    max_token_limit=1500,  # è®¾ç½®tokené™åˆ¶
    memory_key="chat_history", # ä¸promptä¸­çš„keyå¯¹åº”
    return_messages=True,
)


class GraphAnalysisAgent:
    def __init__(self, use_multi_book=True):
        self.use_multi_book = use_multi_book
        if use_multi_book:
            self.rag_engine = multi_book_manager
            self.current_engine = None
        else:
            self.rag_engine = rag_engine
            self.current_engine = rag_engine
        
    def add_book(self, book_name: str, book_folder: str):
        """æ·»åŠ æ–°ä¹¦æœ¬"""
        if self.use_multi_book:
            self.rag_engine.add_book(book_name, book_folder)
        else:
            print("å½“å‰ä½¿ç”¨çš„æ˜¯å•ä¹¦æœ¬å¼•æ“ï¼Œè¯·è®¾ç½® use_multi_book=True æ¥å¯ç”¨å¤šä¹¦æœ¬åŠŸèƒ½")
    
    def switch_book(self, book_name: str):
        """åˆ‡æ¢åˆ°æŒ‡å®šä¹¦æœ¬"""
        if self.use_multi_book:
            self.rag_engine.switch_book(book_name)
            self.current_engine = self.rag_engine.get_current_engine()
        else:
            print("å½“å‰ä½¿ç”¨çš„æ˜¯å•ä¹¦æœ¬å¼•æ“ï¼Œè¯·è®¾ç½® use_multi_book=True æ¥å¯ç”¨å¤šä¹¦æœ¬åŠŸèƒ½")
    
    def list_books(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬"""
        if self.use_multi_book:
            return self.rag_engine.list_books()
        else:
            return ["default_book"]
    
    def get_current_book(self):
        """è·å–å½“å‰ä¹¦æœ¬åç§°"""
        if self.use_multi_book:
            return self.rag_engine.get_current_book()
        else:
            return "default_book"
    
    def _get_engine(self):
        """è·å–å½“å‰å¼•æ“"""
        if self.use_multi_book:
            if self.current_engine is None:
                # å¦‚æœè¿˜æ²¡æœ‰é€‰æ‹©ä¹¦æœ¬ï¼Œè¿”å›Noneè®©agentæç¤ºç”¨æˆ·é€‰æ‹©
                return None
            return self.current_engine
        else:
            return self.rag_engine
        
    async def global_search_retrieve_async(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "global_retrieve",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.global_search_retrieve(query)
    
    async def global_search_generate_async(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "global_generate",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.global_search_generate(query, retrieved_context)
    
    async def global_search_full_async(self, query: str) -> Dict[str, Any]:
        """å…¨å±€æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "global_full",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.global_search_full(query)
    
    async def local_search_retrieve_async(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›å†…å®¹"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "local_retrieve",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.local_search_retrieve(query)
    
    async def local_search_generate_async(self, query: str, retrieved_context: Any) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "local_generate",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.local_search_generate(query, retrieved_context)
    
    async def local_search_full_async(self, query: str) -> Dict[str, Any]:
        """å±€éƒ¨æœç´¢ - å®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        engine = self._get_engine()
        if engine is None:
            return {
                "method": "local_full",
                "query": query,
                "error": "è¯·å…ˆé€‰æ‹©ä¸€æœ¬ä¹¦æœ¬",
                "success": False,
                "need_book_selection": True
            }
        return await engine.local_search_full(query)

    async def get_characters_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ—å‡ºæ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²")

    async def get_relationships_async(self, p1: str, p2: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"åˆ†æ{p1}å’Œ{p2}ä¹‹é—´çš„å…³ç³»")

    async def get_important_locations_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹å’Œåœºæ™¯")

    async def background_knowledge_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†")
    
    async def get_worldview_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®š")

    async def get_character_profile_async(self, character_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è·å–{character_name}çš„è¯¦ç»†ä¿¡æ¯")
    
    async def get_significant_event_async(self, event_name:str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–äº‹ä»¶{event_name}çš„è¯¦ç»†ä¿¡æ¯")
    
    async def get_main_theme_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("åˆ†ææ•…äº‹çš„ä¸»é¢˜")
    async def mock_coversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æ¨¡æ‹Ÿ{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
    async def get_open_questions_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("æœ¬ä¹¦æœ‰ä»€ä¹ˆæ‚¬å¿µæˆ–è€…æ²¡æœ‰è§£å†³çš„ä¼ç¬”ï¼Ÿ")
    async def get_conflict_async(self) -> Dict[str, Any]:
        return await self.global_search_full_async("ç½—åˆ—å‡ºæœ¬ä¹¦æœ€å¤§çš„å†²çªæ˜¯ä»€ä¹ˆ")
    async def get_related_characters_async(self, event: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"è·å–{event}äº‹ä»¶çš„å…³è”äººç‰©")
    async def get_causal_chains_async(self, event: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è·å–{event}äº‹ä»¶çš„å› æœé“¾ï¼šå‰ç½®æ¡ä»¶â†’è§¦å‘â†’ç»“æœâ†’åæœ")
    async def style_guardrails_async(self, persona: str) -> Dict[str, Any]:
        return await self.global_search_full_async(f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚")
    async def canon_alignment_async(self, text: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}")
    async def contradiction_test_async(self, text: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}")
    async def continue_story_async(self, brief: str, persona: str = "ä¿æŒä¸åŸè‘—ä¸€è‡´çš„å™è¿°è€…å£å»ä¸è§’è‰²å¯¹ç™½é£æ ¼", target_style: str = "ç´§å‡‘ã€å…·è±¡ç»†èŠ‚ã€å¯¹ç™½æ¨åŠ¨å‰§æƒ…", words_per_scene: int = 600, max_iters: int = 2) -> Dict[str, Any]:
        return await self.local_search_full_async(f"ä¸ºä»¥ä¸‹å¤§çº²ç»­å†™ä¸€ä¸ªåœºæ™¯ï¼ˆä¸è¶…è¿‡{words_per_scene}è¯ï¼‰ï¼š{brief[:3000]}")
    async def imagine_conversation_async(self, character1_name: str, character2_name: str) -> Dict[str, Any]:
        return await self.local_search_full_async(f"æƒ³è±¡{character1_name}å’Œ{character2_name}çš„å¯¹è¯")
    async def extract_quotes_async(self, name:str, n:int=8) -> Dict[str, Any]:
        q = f"åˆ—å‡º{name}æœ€å…·ä»£è¡¨æ€§çš„å°è¯{n}æ¡ï¼ˆæ¯æ¡<=40å­—ï¼Œé™„ç« èŠ‚/æ®µè½ç¼–å·ï¼‰ï¼Œä¸¥æ ¼JSONæ•°ç»„ï¼š"
        return await self.local_search_full_async(q)
    async def narrative_pov_async(self) -> Dict[str, Any]:
        q = """
    åˆ†æå™äº‹è§†è§’ä¸å¯é æ€§ï¼šPOVç±»å‹ã€åˆ‡æ¢ç‚¹ã€å¯èƒ½åè§/è¯¯å¯¼çš„è¯æ®ã€‚ç”¨åˆ†ç‚¹åˆ—å‡ºï¼Œæ¯ç‚¹é™„<=40å­—çŸ­æ‘˜+ç« èŠ‚ã€‚
    """
        return await self.global_search_full_async(q)
    async def get_motifs_symbols_async(self, max_items:int=20) -> Dict[str, Any]:
        q = f"""
    æŠ½å–æ„è±¡/æ¯é¢˜/è±¡å¾ï¼ˆæœ€å¤š{max_items}æ¡ï¼‰ï¼Œä¸¥æ ¼JSONï¼š
    [{{"motif":"â€¦","meaning":"â€¦","linked_themes":["â€¦"],"chapters":["â€¦"],"evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}]
    """
        return await self.local_search_full_async(q)
    async def build_story_outline_async(self, brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> Dict[str, Any]:
        q = f"""
    åŸºäºåŸè‘—çº¦æŸï¼Œä¸º"{brief}"ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚
    é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚
    """
        return await self.global_search_full_async(q)
    async def emotion_curve_async(self, scope:str="å…¨ä¹¦") -> Dict[str, Any]:
        q = f"æå–{scope}çš„æƒ…æ„Ÿæ›²çº¿å…³é”®è½¬æŠ˜ï¼ˆå–œ/æ€’/å“€/æƒ§/æƒŠ/åŒ/ä¿¡ï¼‰ï¼Œåˆ—å‡ºè½¬æŠ˜ç‚¹ç« èŠ‚ä¸è§¦å‘äº‹ä»¶ï¼Œå„ç»™<=40å­—çŸ­æ‘˜ã€‚"
        return await self.global_search_full_async(q)
    async def compare_characters_async(self, a:str, b:str) -> Dict[str, Any]:
        q = f"""
    æ¯”è¾ƒ{a}ä¸{b}ï¼Œä¸¥æ ¼JSONï¼š
    {{"values":["â€¦"],"goals":["â€¦"],"methods":["â€¦"],"red_lines":["â€¦"],"decision_style":"å†²åŠ¨|è°¨æ…|ç®—è®¡","evidence":[{{"chapter":"â€¦","quote":"<=40å­—"}}]}}
    """
        return await self.global_search_full_async(q)

# --- ç¬¬äºŒæ­¥ï¼šåˆ›å»º LangChain Agent ---
def create_graphrag_agent(graphrag_agent_instance: GraphAnalysisAgent) -> AgentExecutor:
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå¯ä»¥è°ƒç”¨ GraphRAG å‘½ä»¤è¡ŒåŠŸèƒ½çš„ LangChain Agentã€‚
    """

    # åˆå§‹åŒ– LLMï¼ˆæå‰å®šä¹‰ï¼Œä¾›å·¥å…·ä½¿ç”¨ï¼‰
    # ç¡®ä¿ä½ å·²ç»è®¾ç½®äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡
    llm = AzureChatOpenAI(
        openai_api_version="2025-01-01-preview",
        azure_deployment="gpt-4.1",
        model_name="gpt-4.1",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.3,
        max_tokens=2000,  # ä»800å¢åŠ åˆ°2000
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    llm_gen = AzureChatOpenAI(
        openai_api_version="2025-01-01-preview",
        azure_deployment="gpt-4.1",
        model_name="gpt-4.1",
        azure_endpoint="https://tcamp.openai.azure.com/",
        openai_api_key=api_key,
        temperature=0.85,   # æ›´é«˜åˆ›é€ æ€§
        max_tokens=2000     # ä»1000å¢åŠ åˆ°2000
    )

    # ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œå°† GraphAnalysisAgent çš„æ–¹æ³•åŒ…è£…æˆ LangChain å·¥å…·
    # æ³¨æ„ï¼šè¿™é‡Œçš„å·¥å…·å‡½æ•°éœ€è¦èƒ½å¤Ÿè¢« Agent ç›´æ¥è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨é—­åŒ…æ¥ä¼ é€’å®ä¾‹
    
    @tool
    async def list_available_books_tool() -> str:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬ã€‚å¦‚æœè¿˜æ²¡æœ‰æ·»åŠ ä»»ä½•ä¹¦æœ¬ï¼Œä¼šæç¤ºç”¨æˆ·æ·»åŠ ä¹¦æœ¬ã€‚"""
        books = graphrag_agent_instance.list_books()
        if not books:
            return json.dumps({
                "message": "è¿˜æ²¡æœ‰æ·»åŠ ä»»ä½•ä¹¦æœ¬ã€‚è¯·ä½¿ç”¨ add_book_tool æ·»åŠ ä¹¦æœ¬ã€‚",
                "books": [],
                "success": False
            }, ensure_ascii=False)
        
        current_book = graphrag_agent_instance.get_current_book()
        return json.dumps({
            "message": f"å¯ç”¨çš„ä¹¦æœ¬ï¼š{', '.join(books)}ã€‚å½“å‰é€‰æ‹©çš„ä¹¦æœ¬ï¼š{current_book}",
            "books": books,
            "current_book": current_book,
            "success": True
        }, ensure_ascii=False)
    
    @tool
    async def add_book_tool(book_name: str, book_folder: str) -> str:
        """æ·»åŠ æ–°ä¹¦æœ¬åˆ°ç³»ç»Ÿä¸­ã€‚book_nameæ˜¯ä¹¦æœ¬çš„æ˜¾ç¤ºåç§°ï¼Œbook_folderæ˜¯ä¹¦æœ¬æ•°æ®æ–‡ä»¶å¤¹çš„è·¯å¾„ã€‚"""
        try:
            graphrag_agent_instance.add_book(book_name, book_folder)
            return json.dumps({
                "message": f"æˆåŠŸæ·»åŠ ä¹¦æœ¬ï¼š{book_name} -> {book_folder}",
                "success": True
            }, ensure_ascii=False)
        except Exception as e:
            return json.dumps({
                "message": f"æ·»åŠ ä¹¦æœ¬å¤±è´¥ï¼š{str(e)}",
                "success": False
            }, ensure_ascii=False)
    
    @tool
    async def switch_book_tool(book_name: str) -> str:
        """åˆ‡æ¢åˆ°æŒ‡å®šçš„ä¹¦æœ¬ã€‚book_nameæ˜¯è¦åˆ‡æ¢åˆ°çš„ä¹¦æœ¬åç§°ã€‚"""
        try:
            graphrag_agent_instance.switch_book(book_name)
            return json.dumps({
                "message": f"æˆåŠŸåˆ‡æ¢åˆ°ä¹¦æœ¬ï¼š{book_name}",
                "current_book": book_name,
                "success": True
            }, ensure_ascii=False)
        except Exception as e:
            return json.dumps({
                "message": f"åˆ‡æ¢ä¹¦æœ¬å¤±è´¥ï¼š{str(e)}",
                "success": False
            }, ensure_ascii=False)
    
    @tool
    async def get_current_book_tool() -> str:
        """è·å–å½“å‰é€‰æ‹©çš„ä¹¦æœ¬åç§°ã€‚"""
        current_book = graphrag_agent_instance.get_current_book()
        return json.dumps({
            "message": f"å½“å‰é€‰æ‹©çš„ä¹¦æœ¬ï¼š{current_book}",
            "current_book": current_book,
            "success": True
        }, ensure_ascii=False)

    # === æ–°å¢ï¼šRAGæ£€ç´¢åˆ†ç¦»å·¥å…· ===
    @tool
    async def global_search_retrieve_tool(query: str) -> str:
        """å…¨å±€æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›çš„å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°GraphRAGæ£€ç´¢åˆ°äº†å“ªäº›ç›¸å…³æ–‡æ¡£å’Œä¸Šä¸‹æ–‡ï¼Œä½†ä¸è¿›è¡ŒLLMç”Ÿæˆã€‚"""
        result = await graphrag_agent_instance.global_search_retrieve_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def global_search_generate_tool(query: str, retrieved_context: str) -> str:
        """å…¨å±€æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆã€‚éœ€è¦å…ˆè°ƒç”¨global_search_retrieve_toolè·å–ä¸Šä¸‹æ–‡ã€‚"""
        try:
            print(f"ğŸ¤– [Agent LLM] æ­£åœ¨ä½¿ç”¨agentçš„LLMå·¥å…·ç”Ÿæˆå…¨å±€æœç´¢å›ç­”: {query}")
            
            # è§£ææ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ - å¢åŠ æ›´å¥å£®çš„é”™è¯¯å¤„ç†
            import json
            try:
                if isinstance(retrieved_context, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    context_data = json.loads(retrieved_context)
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    context_data = retrieved_context
            except json.JSONDecodeError as e:
                print(f"â„¹ï¸ [Agent LLM] æ£€æµ‹åˆ°éJSONæ ¼å¼æ•°æ®ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                context_text = str(retrieved_context)
            else:
                # JSONè§£ææˆåŠŸï¼Œæå–ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_text = context_data.get('retrieved_context', {}).get('context_text', '')
                if not context_text:
                    # å¦‚æœåµŒå¥—ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®
                    context_text = context_data.get('context_text', str(context_data))
            
            # æ„å»ºæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
    

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å…·ä½“è¯¦ç»†ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®
2. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
3. å¦‚æœæ¶‰åŠäººç‰©ï¼Œè¦è¯´æ˜ä»–ä»¬çš„è§’è‰²ã€ç‰¹ç‚¹å’Œé‡è¦æ€§
4. å¦‚æœæ¶‰åŠäº‹ä»¶ï¼Œè¦æè¿°å…¶èƒŒæ™¯ã€è¿‡ç¨‹å’Œå½±å“
5. å›ç­”é•¿åº¦åº”è¯¥åœ¨200-500å­—ä¹‹é—´ï¼Œç¡®ä¿ä¿¡æ¯å……åˆ†ä½†ä¸è¿‡äºå†—é•¿"""
            
            # ä½¿ç”¨agentçš„LLMå·¥å…·è¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [Agent LLM] å…¨å±€æœç´¢ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè€Œä¸æ˜¯JSONæ ¼å¼
            return result_text
            
        except Exception as e:
            print(f"âŒ [Agent LLM] å…¨å±€æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "agent_global_generate",
                "query": query,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_retrieve_tool(query: str) -> str:
        """å±€éƒ¨æœç´¢ - ä»…æ£€ç´¢é˜¶æ®µï¼Œå±•ç¤ºRAGå¬å›çš„å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°GraphRAGæ£€ç´¢åˆ°äº†å“ªäº›ç›¸å…³æ–‡æ¡£å’Œä¸Šä¸‹æ–‡ï¼Œä½†ä¸è¿›è¡ŒLLMç”Ÿæˆã€‚"""
        result = await graphrag_agent_instance.local_search_retrieve_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_generate_tool(query: str, retrieved_context: str) -> str:
        """å±€éƒ¨æœç´¢ - ä»…ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆã€‚éœ€è¦å…ˆè°ƒç”¨local_search_retrieve_toolè·å–ä¸Šä¸‹æ–‡ã€‚"""
        try:
            print(f"ğŸ¤– [Agent LLM] æ­£åœ¨ä½¿ç”¨agentçš„LLMå·¥å…·ç”Ÿæˆå±€éƒ¨æœç´¢å›ç­”: {query}")
            
            # è§£ææ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ - å¢åŠ æ›´å¥å£®çš„é”™è¯¯å¤„ç†
            import json
            try:
                if isinstance(retrieved_context, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    context_data = json.loads(retrieved_context)
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    context_data = retrieved_context
            except json.JSONDecodeError as e:
                print(f"â„¹ï¸ [Agent LLM] æ£€æµ‹åˆ°éJSONæ ¼å¼æ•°æ®ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼...")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                context_text = str(retrieved_context)
            else:
                # JSONè§£ææˆåŠŸï¼Œæå–ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_text = context_data.get('retrieved_context', {}).get('context_text', '')
                if not context_text:
                    # å¦‚æœåµŒå¥—ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®
                    context_text = context_data.get('context_text', str(context_data))
            
            # æ„å»ºæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å…·ä½“è¯¦ç»†ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®
2. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
3. å¦‚æœæ¶‰åŠäººç‰©ï¼Œè¦è¯´æ˜ä»–ä»¬çš„è§’è‰²ã€ç‰¹ç‚¹å’Œé‡è¦æ€§
4. å¦‚æœæ¶‰åŠäº‹ä»¶ï¼Œè¦æè¿°å…¶èƒŒæ™¯ã€è¿‡ç¨‹å’Œå½±å“
5. å›ç­”é•¿åº¦åº”è¯¥åœ¨200-500å­—ä¹‹é—´ï¼Œç¡®ä¿ä¿¡æ¯å……åˆ†ä½†ä¸è¿‡äºå†—é•¿"""
            
            # ä½¿ç”¨agentçš„LLMå·¥å…·è¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [Agent LLM] å±€éƒ¨æœç´¢ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè€Œä¸æ˜¯JSONæ ¼å¼
            return result_text
            
        except Exception as e:
            print(f"âŒ [Agent LLM] å±€éƒ¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "agent_local_generate",
                "query": query,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    # === æ–°å¢ï¼šç‹¬ç«‹LLMè°ƒç”¨å·¥å…· ===
    @tool
    async def llm_generate_tool(prompt: str, context: str = "") -> str:
        """ç‹¬ç«‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚è¾“å…¥promptæ˜¯ç”Ÿæˆæç¤ºï¼Œcontextæ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰ã€‚ç”¨æˆ·å¯ä»¥çœ‹åˆ°LLMæ­£åœ¨ç”Ÿæˆå†…å®¹ã€‚"""
        try:
            print(f"ğŸ¤– [ç‹¬ç«‹LLM] æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
            print(f"æç¤º: {prompt[:100]}...")
            if context:
                print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # ä½¿ç”¨llm_genè¿›è¡Œç”Ÿæˆ
            from langchain_core.messages import SystemMessage, HumanMessage
            
            messages = []
            if context:
                messages.append(SystemMessage(content=f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·æä¾›è¯¦ç»†ã€å…·ä½“çš„å›ç­”ï¼ŒåŒ…å«å……åˆ†çš„è§£é‡Šå’Œè¯æ®ï¼š\n\n{context}"))
            messages.append(HumanMessage(content=prompt))
            
            response = await llm_gen.ainvoke(messages)
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [ç‹¬ç«‹LLM] ç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            return json.dumps({
                "method": "llm_generate",
                "prompt": prompt,
                "context_length": len(context) if context else 0,
                "response": result_text,
                "success": True
            }, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âŒ [ç‹¬ç«‹LLM] ç”Ÿæˆå¤±è´¥: {e}")
            return json.dumps({
                "method": "llm_generate",
                "prompt": prompt,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def llm_analyze_tool(text: str, analysis_type: str = "general") -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬ã€‚è¾“å…¥textæ˜¯è¦åˆ†æçš„æ–‡æœ¬ï¼Œanalysis_typeæ˜¯åˆ†æç±»å‹ï¼ˆå¦‚'character', 'theme', 'plot'ç­‰ï¼‰ã€‚"""
        try:
            print(f"ğŸ¤– [LLMåˆ†æ] æ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬...")
            print(f"åˆ†æç±»å‹: {analysis_type}")
            print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # æ ¹æ®åˆ†æç±»å‹æ„å»ºæç¤º
            if analysis_type == "character":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­çš„äººç‰©ç‰¹å¾ã€æ€§æ ¼ã€åŠ¨æœºç­‰ï¼š\n\n{text}"
            elif analysis_type == "theme":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»é¢˜ã€è±¡å¾æ„ä¹‰ã€æ·±å±‚å«ä¹‰ï¼š\n\n{text}"
            elif analysis_type == "plot":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…èŠ‚å‘å±•ã€å†²çªã€è½¬æŠ˜ç‚¹ï¼š\n\n{text}"
            else:
                prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œ{analysis_type}åˆ†æï¼š\n\n{text}"
            
            from langchain_core.messages import HumanMessage
            response = await llm_gen.ainvoke([HumanMessage(content=prompt)])
            result_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"âœ… [LLMåˆ†æ] åˆ†æå®Œæˆï¼Œç»“æœé•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            return json.dumps({
                "method": "llm_analyze",
                "analysis_type": analysis_type,
                "text_length": len(text),
                "analysis_result": result_text,
                "success": True
            }, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âŒ [LLMåˆ†æ] åˆ†æå¤±è´¥: {e}")
            return json.dumps({
                "method": "llm_analyze",
                "analysis_type": analysis_type,
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    # === åŸæœ‰å·¥å…· ===
    @tool
    async def get_characters_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„æ‰€æœ‰äººç‰©è§’è‰²ã€‚"""
        result = await graphrag_agent_instance.get_characters_async()
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_relationships_tool(p1: str, p2: str) -> str:
        """è·å–ä¸¤ä¸ªç‰¹å®šäººç‰©ä¹‹é—´çš„å…³ç³»ã€‚è¾“å…¥å‚æ•°p1å’Œp2æ˜¯äººç‰©åç§°ã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸¤ä¸ªäººç‰©çš„å…³ç³»ï¼Œå¯ä»¥å°è¯•å•ç‹¬æŸ¥è¯¢ä¸¤ä¸ªäººç‰©çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå¹¶ä¸”å°è¯•æ‰¾åˆ°å’Œä»–ä»¬å…±åŒç›¸å…³çš„äººæ¥åˆ¤æ–­ä»–ä»¬ä¹‹é—´å¯èƒ½çš„å…³ç³»"""
        result = await graphrag_agent_instance.get_relationships_async(p1, p2)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_important_locations_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹ä¸­çš„é‡è¦åœ°ç‚¹ã€‚"""
        result = await graphrag_agent_instance.get_important_locations_async()
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def background_knowledge_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„èƒŒæ™¯çŸ¥è¯†ã€‚"""
        result = await graphrag_agent_instance.background_knowledge_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def get_worldview_tool() -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è·å–æ•…äº‹çš„ä¸–ç•Œè§‚å’ŒåŸºæœ¬è®¾å®šã€‚"""
        result = await graphrag_agent_instance.get_worldview_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def local_search_tool(query: str) -> str:
        """ä½¿ç”¨ GraphRAG çš„å±€éƒ¨æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
        result = await graphrag_agent_instance.local_search_full_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def global_search_tool(query: str) -> str:
        """ä½¿ç”¨ GraphRAG çš„å…¨å±€æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œè‡ªå®šä¹‰æœç´¢ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„æŸ¥è¯¢ã€‚"""
        result = await graphrag_agent_instance.global_search_full_async(query)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool 
    async def get_character_profile_tool(character_name: str) -> str:
        """è·å–ç‰¹å®šäººç‰©çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°character_nameæ˜¯äººç‰©åç§°ã€‚"""
        result = await graphrag_agent_instance.get_character_profile_async(character_name)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_significant_event_tool(event_name: str) -> str:
        """è·å–ç‰¹å®šäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°event_nameæ˜¯äº‹ä»¶åç§°ã€‚"""
        result = await graphrag_agent_instance.get_significant_event_async(event_name)
        return json.dumps(result, ensure_ascii=False, default=str)

    @tool
    async def get_main_theme_tool() -> str:
        """è·å–æ•…äº‹çš„ä¸»é¢˜ã€‚"""
        result = await graphrag_agent_instance.get_main_theme_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool 
    async def get_open_questions_tool() -> str:
        """è·å–æœ¬ä¹¦çš„æ‚¬å¿µæˆ–è€…æœªè§£å†³çš„ä¼ç¬”ã€‚"""
        result = await graphrag_agent_instance.get_open_questions_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_causal_chains_tool(event: str) -> str:
        """è·å–ç»™å®šäº‹ä»¶çš„å› æœé“¾ã€‚å¯ä»¥çŸ¥é“æ˜¯ä»€ä¹ˆå¯¼è‡´çš„è¯¥äº‹ä»¶ï¼Œç„¶åè¯¥äº‹ä»¶å¯¼è‡´äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼Œæœ€åç»“æœåˆå¯¼è‡´äº†ä»€ä¹ˆæ ·çš„åæœ"""
        result = await graphrag_agent_instance.get_causal_chains_async(event)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def style_guardrails_tool(persona: str) -> str:
        """äº§å‡ºé£æ ¼æŠ¤æ ï¼šå…è®¸/ç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€è§†è§’ã€èŠ‚å¥ç­‰ï¼ˆä¾›ç»­å†™éµå®ˆï¼‰"""
        q = f"æ€»ç»“{persona}çš„å™äº‹é£æ ¼ï¼šå…è®¸å’Œç¦æ­¢çš„å¥å¼ã€è¯æ±‡ã€å¸¸è§ä¿®è¾ã€è§†è§’é™åˆ¶ã€èŠ‚å¥å»ºè®®ï¼Œåˆ—è¡¨è¾“å‡ºã€‚"
        res = await graphrag_agent_instance.global_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)

    @tool
    async def canon_alignment_tool(text: str) -> str:
        """è¯„ä¼°æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™ä¸€è‡´æ€§ï¼ˆè§’è‰²OOC/è®¾å®šè¿èƒŒ/å†å²è¿èƒŒï¼‰ï¼Œç»™è¦ç‚¹ä¸ä¾æ®"""
        q = f"è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬ä¸æ­£å²/ä¸–ç•Œè§„åˆ™çš„ä¸€è‡´æ€§ï¼ˆè§’è‰²OOCã€è®¾å®šè¿èƒŒã€å†å²è¿èƒŒå„ç»™è¦ç‚¹è¯„ä»·ä¸ä¾æ®ï¼‰ï¼š{text[:3000]}"
        res = await graphrag_agent_instance.local_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)

    @tool
    async def contradiction_test_tool(text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼Œç»™å‡ºåŸæ–‡è¯æ®ç‰‡æ®µå®šä½"""
        q = f"æ‰¾å‡ºä»¥ä¸‹æ–‡æœ¬ä¸åŸè‘—å™è¿°çš„å†²çªç‚¹ï¼ˆé€æ¡åˆ—å‡ºå†²çªã€å¯¹åº”åŸè‘—è¯æ®ID/çŸ­æ‘˜ï¼‰ï¼š{text[:3000]}"
        res = await graphrag_agent_instance.local_search_full_async(q)
        return json.dumps(res, ensure_ascii=False, default=str)
    @tool
    async def get_conflict_tool() -> str:
        """è·å–æœ¬ä¹¦æœ€å¤§çš„å†²çªã€‚"""
        result = await graphrag_agent_instance.get_conflict_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_related_characters_tool(event: str) -> str:
        """è·å–ç»™å®šäº‹ä»¶çš„å…³è”äººç‰©ã€‚"""
        result = await graphrag_agent_instance.get_related_characters_async(event)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def imagine_conversation_tool(character1_name: str, character2_name: str) -> str:
        """æƒ³è±¡ä¸¤ä¸ªè§’è‰²ä¹‹é—´çš„å¯¹è¯ã€‚"""
        result = await graphrag_agent_instance.imagine_conversation_async(character1_name, character2_name)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def extract_quotes_tool(name:str, n:int=8) -> str:
        """è·å–ç‰¹å®šäººç‰©çš„å°è¯ã€‚"""
        result = await graphrag_agent_instance.extract_quotes_async(name, n)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def narrative_pov_tool() -> str:
        """è·å–æœ¬ä¹¦çš„å™äº‹è§†è§’ã€‚"""
        result = await graphrag_agent_instance.narrative_pov_async()
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def get_motifs_symbols_tool(max_items:int=20) -> str:
        """è·å–æœ¬ä¹¦çš„æ„è±¡/æ¯é¢˜/è±¡å¾ã€‚"""
        result = await graphrag_agent_instance.get_motifs_symbols_async(max_items)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def build_story_outline_tool(brief:str, target_style:str="ç´§å‡‘å…·è±¡") -> str:
        """åŸºäºåŸè‘—çº¦æŸï¼Œä¸º"{brief}"ç”Ÿæˆä¸‰å¹•å¼ç»­å†™å¤§çº²ï¼ˆæ¯å¹•3-5è¦ç‚¹ï¼‰ï¼Œæ ‡æ³¨æ¶‰åŠäººç‰©/åœ°ç‚¹/å†²çª/ç›®æ ‡ã€‚æ¡ç›®å¼è¾“å‡ºã€‚é£æ ¼ï¼š{target_style}ã€‚ä¸¥ç¦è¿åæ—¢æœ‰è®¾å®šã€‚"""
        result = await graphrag_agent_instance.build_story_outline_async(brief, target_style)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def emotion_curve_tool(scope:str="å…¨ä¹¦") -> str:
        """è·å–æœ¬ä¹¦çš„æƒ…æ„Ÿæ›²çº¿ã€‚"""
        result = await graphrag_agent_instance.emotion_curve_async(scope)
        return json.dumps(result, ensure_ascii=False, default=str)
    @tool
    async def compare_characters_tool(a:str, b:str) -> str:
        """æ¯”è¾ƒä¸¤ä¸ªè§’è‰²ã€‚"""
        result = await graphrag_agent_instance.compare_characters_async(a, b)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    @tool
    async def system_status_tool() -> str:
        """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯ç”¨å·¥å…·ã€å¤„ç†èƒ½åŠ›ç­‰"""
        return json.dumps({
            "system_status": "running",
            "available_tools": [
                "global_search_tool", "local_search_tool", "get_characters_tool",
                "get_relationships_tool", "background_knowledge_tool", "llm_generate_tool",
                "llm_analyze_tool", "get_character_profile_tool", "get_worldview_tool"
            ],
            "capabilities": [
                "äººç‰©åˆ†æ", "å…³ç³»åˆ†æ", "èƒŒæ™¯çŸ¥è¯†æŸ¥è¯¢", "æƒ…èŠ‚åˆ†æ", "æ–‡æœ¬ç”Ÿæˆ", "åˆ›æ„å†™ä½œ"
            ],
            "specialization": "ã€Šæ²™ä¸˜ã€‹(Dune)ç³»åˆ—å°è¯´åˆ†æ",
            "note": "ç³»ç»Ÿå·²ä¼˜åŒ–ï¼Œæ”¯æŒè¯¦ç»†å›ç­”å’Œç”¨æˆ·å‹å¥½çš„çŠ¶æ€æç¤º"
        }, ensure_ascii=False, default=str)
    @tool 
    async def get_people_location_relation_tool(people:str, location:str, relation:str) -> str:
        """è·å–ç‰¹å®šäººç‰©å’Œåœ°ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚"""
        result = await graphrag_agent_instance.get_people_location_relation_async(people, location, relation)
        return json.dumps(result, ensure_ascii=False, default=str)
    
    # === æ–°å¢ï¼šåˆ†å—å¤„ç†å·¥å…· ===
    @tool
    async def parallel_chunk_analysis_tool(retrieved_context: str, query: str, analysis_type: str = "general") -> str:
        """
        å¯¹RAGæ£€ç´¢åˆ°çš„å†…å®¹è¿›è¡Œåˆ†å—å¹¶è¡Œåˆ†æ
        
        Args:
            retrieved_context: RAGæ£€ç´¢åˆ°çš„å®Œæ•´å†…å®¹ï¼ˆJSONæ ¼å¼ï¼‰
            query: åŸå§‹æŸ¥è¯¢
            analysis_type: åˆ†æç±»å‹ (general, character, theme, plot, relationshipç­‰)
        """
        try:
            print(f"ğŸ”„ [åˆ†å—åˆ†æ] å¼€å§‹å¯¹æ£€ç´¢å†…å®¹è¿›è¡Œåˆ†å—å¹¶è¡Œåˆ†æ")
            
            # è§£ææ£€ç´¢ä¸Šä¸‹æ–‡
            if isinstance(retrieved_context, str):
                try:
                    context_data = json.loads(retrieved_context)
                except:
                    context_data = {"full_text": retrieved_context}
            else:
                context_data = retrieved_context
            
            # è·å–åˆ†å—ä¿¡æ¯
            chunks = context_data.get("chunks", [])
            if not chunks:
                # å¦‚æœæ²¡æœ‰åˆ†å—ï¼Œä½¿ç”¨å®Œæ•´æ–‡æœ¬
                full_text = context_data.get("full_text", str(context_data))
                # æ‰‹åŠ¨åˆ†å—
                token_encoder = tiktoken.get_encoding("cl100k_base")
                tokens = token_encoder.encode(full_text)
                
                chunk_size = 20000
                overlap = 2000
                chunks = []
                
                for i in range(0, len(tokens), chunk_size - overlap):
                    chunk_tokens = tokens[i:i + chunk_size]
                    chunk_text = token_encoder.decode(chunk_tokens)
                    chunks.append({
                        "chunk_id": len(chunks),
                        "text": chunk_text,
                        "chunk_tokens": len(chunk_tokens)
                    })
            
            print(f"ğŸ“Š [åˆ†å—åˆ†æ] å°†å¤„ç† {len(chunks)} ä¸ªåˆ†å—")
            
            # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©æç¤ºè¯
            analysis_prompts = {
                "general": f"åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š{query}\\n\\nè¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„åˆ†æã€‚",
                "character": f"ä»äººç‰©è§’åº¦åˆ†æä»¥ä¸‹å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š{query}\\n\\nè¯·è¯†åˆ«ç›¸å…³äººç‰©ã€æ€§æ ¼ç‰¹ç‚¹ã€è¡Œä¸ºåŠ¨æœºç­‰ã€‚",
                "theme": f"ä»ä¸»é¢˜è§’åº¦åˆ†æä»¥ä¸‹å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š{query}\\n\\nè¯·è¯†åˆ«ä¸»è¦ä¸»é¢˜ã€è±¡å¾æ„ä¹‰ã€æ·±å±‚å«ä¹‰ç­‰ã€‚",
                "plot": f"ä»æƒ…èŠ‚è§’åº¦åˆ†æä»¥ä¸‹å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š{query}\\n\\nè¯·è¯†åˆ«å…³é”®äº‹ä»¶ã€å› æœå…³ç³»ã€æƒ…èŠ‚å‘å±•ç­‰ã€‚",
                "relationship": f"ä»å…³ç³»è§’åº¦åˆ†æä»¥ä¸‹å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š{query}\\n\\nè¯·è¯†åˆ«äººç‰©å…³ç³»ã€äº¤äº’æ¨¡å¼ã€å…³ç³»å‘å±•ç­‰ã€‚"
            }
            
            base_prompt = analysis_prompts.get(analysis_type, analysis_prompts["general"])
            
            # å¹¶è¡Œå¤„ç†æ¯ä¸ªåˆ†å—
            async def analyze_chunk(chunk):
                chunk_id = chunk.get("chunk_id", "unknown")
                chunk_text = chunk.get("text", "")
                
                print(f"  ğŸ“ [åˆ†å— {chunk_id}] æ­£åœ¨åˆ†æ ({chunk.get('chunk_tokens', 0)} tokens)")
                
                prompt = f"{base_prompt}\\n\\n=== å†…å®¹åˆ†å— {chunk_id} ===\\n{chunk_text}"
                
                try:
                    # ä½¿ç”¨ llm_gen è¿›è¡Œåˆ†æ
                    response = await llm_gen.ainvoke([HumanMessage(content=prompt)])
                    
                    return {
                        "chunk_id": chunk_id,
                        "analysis": response.content,
                        "success": True,
                        "chunk_tokens": chunk.get("chunk_tokens", 0)
                    }
                except Exception as e:
                    print(f"    âŒ [åˆ†å— {chunk_id}] åˆ†æå¤±è´¥: {e}")
                    return {
                        "chunk_id": chunk_id,
                        "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                        "success": False,
                        "error": str(e)
                    }
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†å—åˆ†æ
            import asyncio
            chunk_results = await asyncio.gather(*[analyze_chunk(chunk) for chunk in chunks])
            
            # ç»Ÿè®¡ç»“æœ
            successful_chunks = [r for r in chunk_results if r.get("success", False)]
            failed_chunks = [r for r in chunk_results if not r.get("success", False)]
            
            print(f"âœ… [åˆ†å—åˆ†æ] å®Œæˆï¼š{len(successful_chunks)}/{len(chunks)} ä¸ªåˆ†å—æˆåŠŸ")
            
            # ä¸ºäº†é¿å…è¿”å›çš„JSONè¿‡å¤§ï¼Œå¯¹åˆ†æç»“æœè¿›è¡Œä¸¥æ ¼è£å‰ª
            processed_results = []
            for result in chunk_results:
                if result.get("success", False):
                    analysis_content = result.get("analysis", "")
                    # æ›´ä¸¥æ ¼çš„è£å‰ªï¼šæœ€å¤šä¿ç•™2000å­—ç¬¦
                    max_analysis_length = 2000
                    if len(analysis_content) > max_analysis_length:
                        truncated_analysis = analysis_content[:max_analysis_length-150] + f"\\n\\n[æ³¨ï¼šæ­¤åˆ†æç»“æœå·²è£å‰ªï¼ŒåŸé•¿åº¦ {len(analysis_content)} å­—ç¬¦ï¼Œå·²å‹ç¼©ä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™]"
                    else:
                        truncated_analysis = analysis_content
                    
                    processed_results.append({
                        "chunk_id": result.get("chunk_id"),
                        "analysis": truncated_analysis,
                        "success": True,
                        "chunk_tokens": result.get("chunk_tokens", 0),
                        "original_length": len(analysis_content),
                        "compressed": len(analysis_content) > max_analysis_length
                    })
                else:
                    processed_results.append(result)
            
            # ç»Ÿè®¡å‹ç¼©æƒ…å†µ
            compressed_count = sum(1 for r in processed_results if r.get("compressed", False))
            if compressed_count > 0:
                print(f"ğŸ“‰ [ç»“æœå‹ç¼©] {compressed_count}/{len(processed_results)} ä¸ªåˆ†æç»“æœå·²å‹ç¼©")
            
            result = {
                "method": "parallel_chunk_analysis",
                "query": query,
                "analysis_type": analysis_type,
                "total_chunks": len(chunks),
                "successful_chunks": len(successful_chunks),
                "failed_chunks": len(failed_chunks),
                "chunk_analyses": processed_results,
                "ready_for_summary": True,
                # "note": "åˆ†å—åˆ†æå®Œæˆï¼Œç»“æœå·²ä¸¥æ ¼å‹ç¼©ä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é™",
                "compression_applied": True,
                "compressed_count": compressed_count
            }
            
            return json.dumps(result, ensure_ascii=False, default=str)
            
        except Exception as e:
            print(f"âŒ [åˆ†å—åˆ†æ] æ•´ä½“å¤±è´¥: {e}")
            return json.dumps({
                "method": "parallel_chunk_analysis",
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    
    @tool
    async def summary_chunk_results_tool(chunk_analysis_results: str, query: str, summary_focus: str = "comprehensive") -> str:
        """
        å¯¹åˆ†å—åˆ†æç»“æœè¿›è¡Œæ€»ç»“ï¼ˆæ”¯æŒå¤§é‡åˆ†å—ï¼Œè‡ªåŠ¨å¤„ç†ä¸Šä¸‹æ–‡é™åˆ¶ï¼‰
        
        Args:
            chunk_analysis_results: åˆ†å—åˆ†æçš„ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
            query: åŸå§‹æŸ¥è¯¢
            summary_focus: æ€»ç»“é‡ç‚¹ (comprehensive, key_points, detailed, concise)
        """
        try:
            print(f"ğŸ“‹ [ç»“æœæ€»ç»“] å¼€å§‹æ€»ç»“åˆ†å—åˆ†æç»“æœ")
            
            # è§£æåˆ†å—åˆ†æç»“æœ
            if isinstance(chunk_analysis_results, str):
                try:
                    results_data = json.loads(chunk_analysis_results)
                except:
                    results_data = {"chunk_analyses": [{"analysis": chunk_analysis_results}]}
            else:
                results_data = chunk_analysis_results
            
            chunk_analyses = results_data.get("chunk_analyses", [])
            successful_analyses = [r for r in chunk_analyses if r.get("success", False)]
            
            if not successful_analyses:
                return json.dumps({
                    "method": "summary_chunk_results",
                    "error": "æ²¡æœ‰æˆåŠŸçš„åˆ†å—åˆ†æç»“æœå¯ä¾›æ€»ç»“",
                    "success": False
                }, ensure_ascii=False, default=str)
            
            print(f"ğŸ“Š [ç»“æœæ€»ç»“] æ€»ç»“ {len(successful_analyses)} ä¸ªæˆåŠŸçš„åˆ†æç»“æœ")
            
            # ä½¿ç”¨tiktokenä¼°ç®—tokenæ•°é‡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¶…é™
            token_encoder = tiktoken.get_encoding("cl100k_base")
            
            # æ„å»ºæ€»ç»“æç¤ºè¯æ¨¡æ¿
            summary_prompts = {
                "comprehensive": f"""è¯·åŸºäºä»¥ä¸‹å¤šä¸ªåˆ†å—çš„åˆ†æç»“æœï¼Œå¯¹æŸ¥è¯¢"{query}"æä¾›å…¨é¢ã€è¯¦ç»†çš„å›ç­”ã€‚

è¯·ï¼š
1. æ•´åˆæ‰€æœ‰åˆ†å—çš„å…³é”®ä¿¡æ¯
2. æ¶ˆé™¤é‡å¤å†…å®¹  
3. æŒ‰é€»è¾‘é¡ºåºç»„ç»‡ä¿¡æ¯
4. æä¾›å…·ä½“çš„ä¾‹è¯å’Œç»†èŠ‚
5. ç»™å‡ºå®Œæ•´ã€è¿è´¯çš„ç­”æ¡ˆ

å„åˆ†å—åˆ†æç»“æœï¼š""",
                
                "key_points": f"""è¯·åŸºäºä»¥ä¸‹å¤šä¸ªåˆ†å—çš„åˆ†æç»“æœï¼Œæå–å…³äº"{query}"çš„å…³é”®è¦ç‚¹ã€‚

è¯·ä»¥è¦ç‚¹å½¢å¼æ€»ç»“ï¼š
1. ä¸»è¦å‘ç°ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
2. å…³é”®ä¿¡æ¯
3. é‡è¦ç»†èŠ‚
4. ç»“è®º

å„åˆ†å—åˆ†æç»“æœï¼š""",
                
                "detailed": f"""è¯·åŸºäºä»¥ä¸‹å¤šä¸ªåˆ†å—çš„åˆ†æç»“æœï¼Œå¯¹æŸ¥è¯¢"{query}"æä¾›è¯¦ç»†æ·±å…¥çš„åˆ†æã€‚

è¯·åŒ…æ‹¬ï¼š
1. èƒŒæ™¯ä¿¡æ¯
2. è¯¦ç»†åˆ†æ
3. å…·ä½“ä¾‹è¯
4. æ·±å±‚å«ä¹‰
5. ç›¸å…³è”ç³»

å„åˆ†å—åˆ†æç»“æœï¼š""",
                
                "concise": f"""è¯·åŸºäºä»¥ä¸‹å¤šä¸ªåˆ†å—çš„åˆ†æç»“æœï¼Œå¯¹æŸ¥è¯¢"{query}"æä¾›ç®€æ´ç²¾å‡†çš„å›ç­”ã€‚

è¯·ç”¨ç®€æ´è¯­è¨€æ¦‚æ‹¬ï¼š
1. æ ¸å¿ƒç­”æ¡ˆ
2. ä¸»è¦æ”¯æ’‘ä¿¡æ¯
3. å…³é”®ç»“è®º

å„åˆ†å—åˆ†æç»“æœï¼š"""
            }
            
            base_prompt = summary_prompts.get(summary_focus, summary_prompts["comprehensive"])
            base_tokens = len(token_encoder.encode(base_prompt))
            
            # è®¡ç®—å¯ç”¨äºåˆ†æç»“æœçš„tokenæ•°ï¼ˆä¿ç•™å®‰å…¨è¾¹è·ï¼‰
            max_context_tokens = 120000  # 128Kçš„å®‰å…¨èŒƒå›´
            reserved_tokens = 8000  # ä¸ºresponseå’Œå…¶ä»–å†…å®¹é¢„ç•™
            available_tokens = max_context_tokens - base_tokens - reserved_tokens
            
            print(f"ğŸ” [Tokenç®¡ç†] åŸºç¡€æç¤º: {base_tokens} tokens, å¯ç”¨ç©ºé—´: {available_tokens} tokens")
            
            # æ™ºèƒ½é€‰æ‹©å’Œå‹ç¼©åˆ†æç»“æœ - å¼ºåˆ¶é™åˆ¶tokenæ•°é‡
            # æ— è®ºä½•ç§æƒ…å†µï¼Œéƒ½è¦ä¸¥æ ¼æ§åˆ¶ä¼ é€’ç»™LLMçš„å†…å®¹å¤§å°
            
            # é¢„å…ˆå‹ç¼©æ‰€æœ‰åˆ†æç»“æœ
            compressed_analyses = []
            for analysis in successful_analyses:
                chunk_id = analysis.get("chunk_id", "unknown")
                analysis_content = analysis.get("analysis", "")
                
                # å¼ºåˆ¶é™åˆ¶æ¯ä¸ªåˆ†æç»“æœçš„é•¿åº¦
                max_analysis_length = 2000  # æ¯ä¸ªåˆ†æç»“æœæœ€å¤š2000å­—ç¬¦
                if len(analysis_content) > max_analysis_length:
                    compressed_content = analysis_content[:max_analysis_length-100] + "\\n\\n[å·²å‹ç¼©ï¼ŒåŸé•¿åº¦:" + str(len(analysis_content)) + "å­—ç¬¦]"
                else:
                    compressed_content = analysis_content
                
                compressed_analyses.append({
                    "chunk_id": chunk_id,
                    "analysis": compressed_content,
                    "original_length": len(analysis_content)
                })
            
            print(f"ğŸ”§ [å†…å®¹å‹ç¼©] å·²å‹ç¼© {len(compressed_analyses)} ä¸ªåˆ†æç»“æœ")
            
            # åˆ†å±‚æ€»ç»“ç­–ç•¥ - å§‹ç»ˆä½¿ç”¨ï¼Œç¡®ä¿ä¸ä¼šè¶…é™
            print(f"ğŸ”„ [åˆ†å±‚æ€»ç»“] é‡‡ç”¨å¼ºåˆ¶åˆ†å±‚æ€»ç»“ç­–ç•¥")
            
            # ç¬¬ä¸€å±‚ï¼šå°†åˆ†å—åˆ†ç»„å¹¶æ€»ç»“æ¯ç»„ï¼ˆä¸¥æ ¼æ§åˆ¶ç»„å¤§å°ï¼‰
            group_size = 2  # å‡å°‘åˆ°æ¯ç»„2ä¸ªåˆ†å—ï¼Œè¿›ä¸€æ­¥é™ä½é£é™©
            group_summaries = []
            
            for i in range(0, len(compressed_analyses), group_size):
                group = compressed_analyses[i:i + group_size]
                
                # æ„å»ºç»„å†…å®¹ï¼Œä¸¥æ ¼æ§åˆ¶å¤§å°
                group_items = []
                total_group_length = 0
                max_group_length = 4000  # æ¯ç»„æœ€å¤š4000å­—ç¬¦
                
                for analysis in group:
                    chunk_id = analysis.get("chunk_id", "unknown")
                    analysis_content = analysis.get("analysis", "")
                    
                    # æ£€æŸ¥æ·»åŠ è¿™ä¸ªåˆ†ææ˜¯å¦ä¼šè¶…é™
                    item_text = f"\\n=== åˆ†å— {chunk_id} ===\\n{analysis_content}"
                    if total_group_length + len(item_text) > max_group_length:
                        # å¦‚æœä¼šè¶…é™ï¼Œè¿›ä¸€æ­¥è£å‰ª
                        remaining_space = max_group_length - total_group_length - 50
                        if remaining_space > 100:
                            truncated_content = analysis_content[:remaining_space] + "..."
                            item_text = f"\\n=== åˆ†å— {chunk_id} ===\\n{truncated_content}"
                        else:
                            break  # ç©ºé—´ä¸å¤Ÿï¼Œè·³è¿‡è¿™ä¸ªåˆ†æ
                    
                    group_items.append(item_text)
                    total_group_length += len(item_text)
                
                group_text = "".join(group_items)
                
                # ç”Ÿæˆç»„æ€»ç»“ï¼ˆä½¿ç”¨ç®€åŒ–æç¤ºï¼‰
                group_prompt = f"""æ€»ç»“ä»¥ä¸‹åˆ†æå†…å®¹çš„æ ¸å¿ƒè¦ç‚¹ï¼ˆå…³äºï¼š{query}ï¼‰ï¼š

{group_text}

è¯·ç”¨ç®€æ´è¯­è¨€æå–å…³é”®ä¿¡æ¯ï¼š"""
                
                # æ£€æŸ¥ç»„æç¤ºçš„tokenæ•°é‡
                group_tokens = len(token_encoder.encode(group_prompt))
                print(f"  ğŸ“Š [ç»„ {len(group_summaries)+1}] æç¤ºtokens: {group_tokens}")
                
                if group_tokens > 15000:  # å¦‚æœç»„æç¤ºè¶…è¿‡15K tokensï¼Œè¿›ä¸€æ­¥å‹ç¼©
                    print(f"  âš ï¸ [ç»„ {len(group_summaries)+1}] æç¤ºè¿‡é•¿ï¼Œè¿›ä¸€æ­¥å‹ç¼©")
                    # ä½¿ç”¨æç®€ç‰ˆæœ¬
                    short_summaries = []
                    for analysis in group:
                        analysis_content = analysis.get("analysis", "")
                        short_summary = analysis_content[:500] + "..." if len(analysis_content) > 500 else analysis_content
                        short_summaries.append(short_summary)
                    
                    group_prompt = f"""æ€»ç»“å…³é”®ä¿¡æ¯ï¼ˆ{query}ï¼‰ï¼š
{chr(10).join(short_summaries)}
è¯·ç®€è¦æ¦‚æ‹¬ï¼š"""
                
                try:
                    group_response = await llm_gen.ainvoke([HumanMessage(content=group_prompt)])
                    group_summaries.append({
                        "group_id": len(group_summaries),
                        "summary": group_response.content,
                        "chunk_count": len(group)
                    })
                    print(f"  âœ… [ç»„ {len(group_summaries)}] å®Œæˆï¼ŒåŒ…å« {len(group)} ä¸ªåˆ†å—")
                except Exception as e:
                    print(f"  âŒ [ç»„æ€»ç»“] å¤±è´¥: {e}")
                    # å¦‚æœç»„æ€»ç»“å¤±è´¥ï¼Œä½¿ç”¨æç®€ç‰ˆæœ¬
                    simplified_summary = f"ç»„{len(group_summaries)}å…³é”®ä¿¡æ¯ï¼š" + "ï¼›".join([
                        analysis.get("analysis", "")[:200] for analysis in group
                    ])
                    group_summaries.append({
                        "group_id": len(group_summaries),
                        "summary": simplified_summary,
                        "chunk_count": len(group)
                    })
            
            # ç¬¬äºŒå±‚ï¼šæ€»ç»“æ‰€æœ‰ç»„æ€»ç»“ï¼ˆä¸¥æ ¼æ§åˆ¶æœ€ç»ˆæ€»ç»“çš„å¤§å°ï¼‰
            print(f"ğŸ“‹ [æœ€ç»ˆæ€»ç»“] å‡†å¤‡æ€»ç»“ {len(group_summaries)} ä¸ªç»„çš„ç»“æœ")
            
            # é¢„å…ˆå‹ç¼©æ‰€æœ‰ç»„æ€»ç»“
            compressed_group_summaries = []
            total_final_length = 0
            max_final_length = 8000  # æœ€ç»ˆæ€»ç»“è¾“å…¥æœ€å¤š8000å­—ç¬¦
            
            for group_summary in group_summaries:
                group_id = group_summary['group_id']
                summary_content = group_summary['summary']
                chunk_count = group_summary['chunk_count']
                
                # ä¸ºæ¯ä¸ªç»„æ€»ç»“åˆ†é…ç©ºé—´
                max_group_summary_length = max_final_length // len(group_summaries)
                max_group_summary_length = min(max_group_summary_length, 1500)  # æ¯ä¸ªç»„æœ€å¤š1500å­—ç¬¦
                
                if len(summary_content) > max_group_summary_length:
                    compressed_content = summary_content[:max_group_summary_length-50] + "..."
                else:
                    compressed_content = summary_content
                
                item_text = f"ç»„{group_id}({chunk_count}å—): {compressed_content}"
                
                if total_final_length + len(item_text) <= max_final_length:
                    compressed_group_summaries.append(item_text)
                    total_final_length += len(item_text)
                else:
                    # å¦‚æœç©ºé—´ä¸å¤Ÿï¼Œä½¿ç”¨æç®€ç‰ˆæœ¬
                    remaining_space = max_final_length - total_final_length - 20
                    if remaining_space > 50:
                        mini_content = summary_content[:remaining_space] + "..."
                        compressed_group_summaries.append(f"ç»„{group_id}: {mini_content}")
                    break
            
            # æ„å»ºæœ€ç»ˆæç¤ºï¼ˆä½¿ç”¨ç®€åŒ–çš„åŸºç¡€æç¤ºï¼‰
            simple_base_prompt = f"""åŸºäºä»¥ä¸‹åˆ†ç»„åˆ†æç»“æœï¼Œå›ç­”æŸ¥è¯¢"{query}"ï¼š

"""
            
            final_summaries_text = "\\n".join(compressed_group_summaries)
            final_prompt = f"""{simple_base_prompt}{final_summaries_text}

è¯·æä¾›ç»¼åˆå›ç­”ï¼š"""
            
            # æœ€åæ£€æŸ¥tokenæ•°ï¼ˆç»å¯¹ä¿è¯ä¸è¶…é™ï¼‰
            final_tokens = len(token_encoder.encode(final_prompt))
            print(f"ğŸ” [Tokenæ£€æŸ¥] æœ€ç»ˆæç¤º: {final_tokens} tokens")
            
            if final_tokens > 15000:  # å¦‚æœè¶…è¿‡15K tokensï¼Œè¿›ä¸€æ­¥å¼ºåˆ¶å‹ç¼©
                print(f"ğŸš¨ [ç´§æ€¥å‹ç¼©] æœ€ç»ˆæç¤ºè¿‡é•¿ï¼Œæ‰§è¡Œå¼ºåˆ¶å‹ç¼©")
                
                # ä½¿ç”¨æœ€ç®€ç‰ˆæœ¬
                ultra_compressed = []
                for i, group_summary in enumerate(group_summaries):
                    summary_content = group_summary['summary']
                    # æ¯ä¸ªç»„åªä¿ç•™å‰300å­—ç¬¦
                    ultra_short = summary_content[:300] + "..." if len(summary_content) > 300 else summary_content
                    ultra_compressed.append(f"{i+1}. {ultra_short}")
                
                final_prompt = f"""å›ç­”æŸ¥è¯¢"{query}"ï¼ŒåŸºäºä»¥ä¸‹è¦ç‚¹ï¼š

{chr(10).join(ultra_compressed)}

ç»¼åˆå›ç­”ï¼š"""
                
                final_tokens = len(token_encoder.encode(final_prompt))
                print(f"ğŸ” [å‹ç¼©å] æœ€ç»ˆæç¤º: {final_tokens} tokens")
            
            # ç¡®ä¿ç»å¯¹å®‰å…¨
            if final_tokens > 20000:
                print(f"ğŸš¨ [æé™å‹ç¼©] ä»ç„¶è¿‡é•¿ï¼Œä½¿ç”¨æç®€æ¨¡å¼")
                # åªä¿ç•™å‰å‡ ä¸ªç»„çš„æ ¸å¿ƒä¿¡æ¯
                essential_info = []
                for i, group_summary in enumerate(group_summaries[:3]):  # åªå–å‰3ç»„
                    summary_content = group_summary['summary']
                    essential = summary_content[:200]  # æ¯ç»„åªè¦200å­—ç¬¦
                    essential_info.append(essential)
                
                final_prompt = f"""å…³äº"{query}"çš„æ ¸å¿ƒä¿¡æ¯ï¼š
{chr(10).join(essential_info)}
è¯·ç®€è¦å›ç­”ï¼š"""
            
            print(f"ğŸ¤– [æœ€ç»ˆè°ƒç”¨] è°ƒç”¨LLMï¼Œæç¤ºé•¿åº¦: {len(final_prompt)} å­—ç¬¦")
            
            response = await llm_gen.ainvoke([HumanMessage(content=final_prompt)])
            
            print(f"âœ… [ç»“æœæ€»ç»“] å¼ºåˆ¶åˆ†å±‚æ€»ç»“å®Œæˆ")
            
            result = {
                "method": "summary_chunk_results",
                "query": query,
                "summary_focus": summary_focus,
                "total_chunks_analyzed": len(successful_analyses),
                "final_summary": response.content,
                "processing_mode": "forced_hierarchical_summary",
                "group_count": len(group_summaries),
                "original_analysis_count": len(chunk_analyses),
                "successful_analysis_count": len(successful_analyses),
                "compression_applied": True,
                "final_prompt_tokens": final_tokens,
                "success": True
            }
            
            return json.dumps(result, ensure_ascii=False, default=str)
            
        except Exception as e:
            print(f"âŒ [ç»“æœæ€»ç»“] æ€»ç»“å¤±è´¥: {e}")
            return json.dumps({
                "method": "summary_chunk_results",
                "error": str(e),
                "success": False
            }, ensure_ascii=False, default=str)
    tools = [
        # ä¹¦æœ¬ç®¡ç†å·¥å…·ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        list_available_books_tool,
        add_book_tool,
        switch_book_tool,
        get_current_book_tool,
        
        #=== æ–°å¢çš„RAGæ£€ç´¢åˆ†ç¦»å·¥å…· ===
        global_search_retrieve_tool,
        # global_search_generate_tool,
        local_search_retrieve_tool,
        local_search_generate_tool,
        
        #=== æ–°å¢ï¼šç‹¬ç«‹LLMè°ƒç”¨å·¥å…· ===
        llm_generate_tool,
        llm_analyze_tool,
        
        #=== æ–°å¢ï¼šåˆ†å—å¤„ç†å·¥å…· ===
        parallel_chunk_analysis_tool,
        summary_chunk_results_tool,
        
        # === åŸæœ‰å·¥å…· ===
        get_characters_tool,
        get_relationships_tool,
        get_important_locations_tool,
        get_significant_event_tool,
        background_knowledge_tool,
        get_worldview_tool,
        local_search_tool,
        global_search_tool,
        get_character_profile_tool,
        get_main_theme_tool,
        get_open_questions_tool,
        get_causal_chains_tool,
        style_guardrails_tool,
        canon_alignment_tool,
        contradiction_test_tool,
        get_conflict_tool,
        get_related_characters_tool,
        imagine_conversation_tool,
        extract_quotes_tool,
        narrative_pov_tool,
        get_motifs_symbols_tool, 
        build_story_outline_tool,
        emotion_curve_tool,
        compare_characters_tool,
        system_status_tool,
        get_people_location_relation_tool,
    ]

# ### RAGæ£€ç´¢åˆ†ç¦»å·¥å…·ï¼š
# - **global_search_retrieve_tool**: ä»…è¿›è¡Œå…¨å±€æœç´¢çš„æ£€ç´¢ï¼Œå±•ç¤ºGraphRAGå¬å›çš„å†…å®¹
# - **global_search_generate_tool**: ä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆ
# - **local_search_retrieve_tool**: ä»…è¿›è¡Œå±€éƒ¨æœç´¢çš„æ£€ç´¢ï¼Œå±•ç¤ºGraphRAGå¬å›çš„å†…å®¹  
# - **local_search_generate_tool**: ä½¿ç”¨é¢„æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿›è¡ŒLLMç”Ÿæˆ

# ### ç‹¬ç«‹LLMè°ƒç”¨å·¥å…·ï¼š
# - **llm_generate_tool**: ç‹¬ç«‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œç”¨æˆ·å¯ä»¥æ¸…æ¥šçœ‹åˆ°LLMæ­£åœ¨ç”Ÿæˆå†…å®¹
# - **llm_analyze_tool**: ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡æœ¬ï¼Œæ”¯æŒä¸åŒç±»å‹çš„åˆ†æï¼ˆcharacter, theme, plotç­‰ï¼‰


# ## é‡è¦è¯´æ˜ï¼šRAGæ£€ç´¢åˆ†ç¦»å·¥å…·å’Œç‹¬ç«‹LLMè°ƒç”¨
# ç°åœ¨ä½ æœ‰æ–°çš„å·¥å…·å¯ä»¥åˆ†ç¦»RAGçš„æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥åŠç‹¬ç«‹çš„LLMè°ƒç”¨ï¼š



# ### ä½¿ç”¨å»ºè®®ï¼š
# 1. **å±•ç¤ºRAGè¿‡ç¨‹**ï¼šå…ˆè°ƒç”¨ *_retrieve_tool å±•ç¤ºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Œå†è°ƒç”¨ *_generate_tool è¿›è¡ŒLLMç”Ÿæˆ
# 2. **ç‹¬ç«‹LLMè°ƒç”¨**ï¼šå½“éœ€è¦åˆ›é€ æ€§å†…å®¹æˆ–å¤æ‚åˆ†ææ—¶ï¼Œä½¿ç”¨ llm_generate_tool æˆ– llm_analyze_tool
# 3. **å®Œæ•´æµç¨‹**ï¼šæˆ–è€…ç›´æ¥ä½¿ç”¨åŸæœ‰çš„å®Œæ•´å·¥å…·ï¼ˆå¦‚ global_search_toolï¼‰

# ### ç”¨æˆ·å¯è§æ€§ï¼š
# - ğŸ” [RAGæ£€ç´¢] è¡¨ç¤ºæ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯
# - ğŸ¤– [LLMç”Ÿæˆ] è¡¨ç¤ºæ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹
# - âœ… è¡¨ç¤ºæ“ä½œå®Œæˆ
# - âŒ è¡¨ç¤ºæ“ä½œå¤±è´¥

# ### æ™ºèƒ½å†³ç­–æŒ‡å—ï¼š
# - **äººç‰©ç›¸å…³é—®é¢˜**ï¼šä¼˜å…ˆä½¿ç”¨ get_character_profile_tool æˆ– get_characters_tool
# - **å…³ç³»åˆ†æ**ï¼šä½¿ç”¨ get_relationships_tool åˆ†æäººç‰©å…³ç³»
# - **èƒŒæ™¯çŸ¥è¯†**ï¼šä½¿ç”¨ background_knowledge_tool æˆ– get_worldview_tool
# - **æƒ…èŠ‚åˆ†æ**ï¼šä½¿ç”¨ global_search_tool è¿›è¡Œå…¨å±€åˆ†æ
# - **å…·ä½“ç»†èŠ‚**ï¼šä½¿ç”¨ local_search_tool è¿›è¡Œç²¾ç¡®æ£€ç´¢
# - **åˆ›ä½œä»»åŠ¡**ï¼šä½¿ç”¨ llm_generate_tool è¿›è¡Œåˆ›é€ æ€§ç”Ÿæˆ

    # å°† prompt å­—ç¬¦ä¸²é‡å‘½åä¸º prompt_textï¼Œé¿å…ä¸ prompt æ¨¡å—å†²çª
    prompt_text = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆ›ä½œåŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œä¿¡æ¯åˆ†æå’Œæ¢ç´¢ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒæŸ¥æ¥å®Œæˆå¤æ‚çš„åˆ›ä½œä»»åŠ¡ã€‚

### ä¹¦æœ¬ç®¡ç†ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰ï¼š
- **list_available_books_tool**: åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬
- **add_book_tool**: æ·»åŠ æ–°ä¹¦æœ¬åˆ°ç³»ç»Ÿä¸­
- **switch_book_tool**: åˆ‡æ¢åˆ°æŒ‡å®šçš„ä¹¦æœ¬
- **get_current_book_tool**: è·å–å½“å‰é€‰æ‹©çš„ä¹¦æœ¬

### å†å²è®°å½•
{{chat_history}}

## è°ƒæŸ¥å‘¨æœŸ (Investigation Cycle)
ä½ æŒ‰ç…§ä¸€ä¸ªæŒç»­çš„å‘¨æœŸè¿ä½œï¼š
1. ä»å¤šä¸ªç»´åº¦ç†è§£ç”¨æˆ·è¯‰æ±‚ï¼Œæ‹†è§£ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç”¨æˆ·æ„å›¾
2. æ ¹æ®å†å²å¯¹è¯ï¼Œæ•´åˆæœ‰ç”¨ä¿¡æ¯ä»¥ç†è§£ä»»åŠ¡ç›®æ ‡
3. æ ¹æ®å·²æŒæ¡çš„çº¿ç´¢å’Œä¿¡æ¯ç¼ºå£ï¼Œé¿å…å’Œå†å²å¯¹è¯ä¸­å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨ï¼ˆå·¥å…·å‚æ•°ä¸€è‡´ï¼‰ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„å·¥å…·ï¼Œå†³å®šæ¥ä¸‹æ¥è¦è°ƒç”¨å“ªä¸ªå·¥å…·
4. å½“ä½ è®¤ä¸ºæ²¡å®Œæˆä»»åŠ¡æ—¶æˆ–ç°æœ‰ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "IN_PROGRES"ï¼Œæ­¤æ—¶ä½ å¿…é¡»é€‰æ‹©ä¸€ä¸ªå·¥å…·ã€‚
5. å½“ä½ è®¤ä¸ºå†å²å¯¹è¯çš„ä¿¡æ¯è¶³å¤Ÿä½ å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œ"status_update" ä¸º "DONE"
## å¯ç”¨å·¥å…· (Available Tools)
{{functions}}
## å·¥å…·ä½¿ç”¨å‡†åˆ™ (Tool Usage Guidelines)
{{guidelines}}
## æ³¨æ„äº‹é¡¹
{{requirements}}
å“åº”æ ¼å¼ (Response Format)
{{response_format}}
    """

    prompt_obj = ChatPromptTemplate.from_messages([
        ("system", prompt_text),  # ä½¿ç”¨ prompt_text è€Œä¸æ˜¯ prompt
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # ä½¿ç”¨ partial æ¥é¢„è®¾å˜é‡å€¼
    final_prompt = prompt_obj.partial(
        functions=tools,
        guidelines=prompt.build_guidelines(),  # ç°åœ¨å¯ä»¥æ­£ç¡®è°ƒç”¨ prompt æ¨¡å—
        requirements=prompt.build_requirements(),
        response_format=prompt.build_response_format(),
        history=""  # æ·»åŠ ç©ºçš„historyå˜é‡
    )

    # åˆ›å»º Agent
    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=final_prompt)

    # åˆ›å»º Agent æ‰§è¡Œå™¨
    return AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)

async def main() -> None:
    graph_agent = GraphAnalysisAgent(use_multi_book=True)

    # è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬
    print("ğŸ“š æ­£åœ¨è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„ä¹¦æœ¬...")
    
    # å®šä¹‰è¦åŠ è½½çš„ä¹¦æœ¬åˆ—è¡¨
    books_to_load = [
        ("book4", "./book4/output"),
        ("book5", "./book5/output"), 
        ("book6", "./book6/output"),
        ("book2", "./rag_book2/ragtest/output"),
        ("tencent", "./tencent/output"),
        ("default", "./rag/output")  # é»˜è®¤çš„rag/output
    ]
    
    loaded_books = []
    for book_name, book_path in books_to_load:
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if os.path.exists(book_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
                required_files = ["communities.parquet", "entities.parquet", "community_reports.parquet", "relationships.parquet", "text_units.parquet"]
                missing_files = [f for f in required_files if not os.path.exists(os.path.join(book_path, f))]
                
                if not missing_files:
                    graph_agent.add_book(book_name, book_path)
                    loaded_books.append(book_name)
                    print(f"âœ… æˆåŠŸåŠ è½½ä¹¦æœ¬: {book_name} -> {book_path}")
                else:
                    print(f"âš ï¸ è·³è¿‡ {book_name}: ç¼ºå°‘å¿…è¦æ–‡ä»¶ {missing_files}")
            else:
                print(f"âš ï¸ è·³è¿‡ {book_name}: è·¯å¾„ä¸å­˜åœ¨ {book_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {book_name} å¤±è´¥: {e}")
    
    print(f"âœ… æ€»å…±åŠ è½½äº† {len(loaded_books)} æœ¬ä¹¦: {', '.join(loaded_books)}")
    
    # å¦‚æœæœ‰ä¹¦æœ¬åŠ è½½æˆåŠŸï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€æœ¬
    if loaded_books:
        first_book = loaded_books[0]
        graph_agent.switch_book(first_book)
        print(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°ç¬¬ä¸€æœ¬ä¹¦: {first_book}")
    else:
        print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•ä¹¦æœ¬ï¼Œè¯·æ‰‹åŠ¨æ·»åŠ ä¹¦æœ¬")

    # ä½¿ç”¨è¿™ä¸ªå®ä¾‹åˆ›å»º LangChain Agent
    agent_executor = create_graphrag_agent(graph_agent)

    print("LangChain Agent with GraphRAG (Python API) tools is ready. Type 'exit' to quit.")
    
    while True:
        user_query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
        if user_query.lower() == 'exit':
            break

        try:
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ŒåŒ¹é…å¼‚æ­¥å·¥å…·
            response = await agent_executor.ainvoke({
                "input": user_query
            })
            
            # æ¢å¤è¾“å‡ºæ˜¾ç¤º
            print("\n--- Agent å›ç­” ---")
            print(response.get("output"))
            print("--------------------\n")
            
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
            break

import os
import json
import asyncio
import random
from typing import List, Dict
from datetime import datetime
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import AzureChatOpenAI

# ä½¿ç”¨ä¸æ‚¨æä¾›çš„å›ç­”agentç›¸åŒçš„APIé…ç½®
api_key = os.getenv("AZURE_OPENAI_API_KEY") or ""

class DuneTopicGenerator:
    def __init__(self):
        self.llm = self._init_azure_llm()
        self.prompt_template = """
        # è§’è‰²ï¼šç§‘å¹»å°è¯´ä¸“å®¶ï¼Œä¸“æ³¨ã€Šæ²™ä¸˜ã€‹ç³»åˆ—
        # ä»»åŠ¡ï¼šç”Ÿæˆä¸ã€Šæ²™ä¸˜ã€‹ç›¸å…³çš„å¤šæ ·åŒ–ä¸»é¢˜å’Œèµ·å§‹é—®é¢˜
        # è¦æ±‚ï¼š
        # 1. ä¸»é¢˜å¿…é¡»æ˜¯ã€Šæ²™ä¸˜ã€‹å°è¯´ç›¸å…³çš„æ ¸å¿ƒé¢†åŸŸ
        #    - ä¸–ç•Œè§‚è®¾å®š (å¦‚ï¼šé¦™æ–™ç»æµå­¦ã€å¼—ç‘æ›¼æ–‡åŒ–)
        #    - äººç‰©å…³ç³» (å¦‚ï¼šå„å´”è¿ªä¸å“ˆå…‹å—å®¶æ—çš„æ©æ€¨)
        #    - å…³é”®æƒ…èŠ‚ (å¦‚ï¼šä¿ç½—æˆä¸ºç©†é˜¿è¿ªå¸ƒçš„æ—…ç¨‹)
        #    - å“²å­¦ä¸»é¢˜ (å¦‚ï¼šæƒåŠ›ã€é¢„çŸ¥èƒ½åŠ›æˆ–ç”Ÿæ€è´£ä»»æ„Ÿ)
        #    - æŠ€æœ¯åˆ›æ–° (å¦‚ï¼šé˜²æŠ¤ç½©ã€æ‚¬æµ®è½¦)
        #    - æœªè§£ä¹‹è°œ (å¦‚ï¼šè´å°¼Â·æ°ç‘Ÿé‡Œç‰¹çš„é•¿è¿œè®¡åˆ’)
        # 2. ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹JSONæ ¼å¼ï¼š
        {{"topic": "ä¸»é¢˜åç§°", "question": "èµ·å§‹é—®é¢˜"}}
        # 3. ä¸»é¢˜å¿…é¡»èƒ½æ”¯æŒå¤šè½®å¯¹è¯
        """
    
    def _init_azure_llm(self) -> AzureChatOpenAI:
        return AzureChatOpenAI(
            openai_api_version="2024-12-01-preview",
            azure_deployment="gpt-4o",
            model_name="gpt-4o",
            openai_api_key=api_key,
            azure_endpoint="https://tcamp.openai.azure.com/",
            temperature=0.85,
            max_tokens=800
        )
    
    async def generate_topic(self) -> dict:
        """ç”Ÿæˆå¯¹è¯ä¸»é¢˜å’Œèµ·å§‹é—®é¢˜ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        from langchain.schema import HumanMessage, SystemMessage
        
        try:
            # åˆ›å»ºç¬¦åˆè¦æ±‚çš„æ¶ˆæ¯ç»“æ„
            messages = [
                SystemMessage(content=self.prompt_template),
                HumanMessage(content="è¯·æŒ‰ç…§è¦æ±‚ç”Ÿæˆä¸€ä¸ªå…³äºã€Šæ²™ä¸˜ã€‹çš„ä¸»é¢˜å’Œèµ·å§‹é—®é¢˜")
            ]
            
            # ä½¿ç”¨æ­£ç¡®çš„invokeæ–¹æ³•
            response = await self.llm.ainvoke(messages)
            content = response.content.strip()
            
            # è°ƒè¯•è¾“å‡º
            print(f"ğŸŒ€ åŸå§‹å“åº”å†…å®¹: \n{content}\n{'-'*60}")
            
            # çµæ´»æå–JSONå†…å®¹
            json_content = None
            
            # åœºæ™¯1: åŒ…å«JSONä»£ç å—
            if "```json" in content:
                json_str = content.split("```json")[1].split("```")[0].strip()
            
            # åœºæ™¯2: åŒ…å«çº¯JSON
            elif "{" in content and "}" in content:
                json_str = content[content.find("{"):content.rfind("}")+1]
            
            # åœºæ™¯3: æ— JSONæ ‡è®°ä½†æ ¼å¼æ­£ç¡®
            elif content.startswith("{") and content.endswith("}"):
                json_str = content
            
            # å°è¯•è§£ææ‰¾åˆ°çš„JSON
            if json_str:
                try:
                    json_content = json.loads(json_str)
                    print(f"âœ… JSONè§£ææˆåŠŸ: {json_content}")
                    return json_content
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥: {str(e)}")
            
            # ç›´æ¥æ¨¡å¼ï¼šå°è¯•è§£ææ•´ä¸ªå“åº”
            try:
                json_content = json.loads(content)
                print(f"âœ… ç›´æ¥JSONè§£ææˆåŠŸ: {json_content}")
                return json_content
            except:
                pass
            
            # æœ€ç»ˆæ ¼å¼å¤„ç†ï¼šé”®å€¼å¯¹æå–
            print("âš ï¸ æ— æ³•ç›´æ¥è§£æä¸ºJSONï¼Œå°è¯•é”®å€¼æå–...")
            return self._extract_key_values(content)
            
        except Exception as e:
            print(f"âš ï¸ ä¸»é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._fallback_topic()
    
    def _extract_key_values(self, text: str) -> dict:
        """ä»éç»“æ„åŒ–æ–‡æœ¬æå–é”®å€¼"""
        topic_keywords = ["ä¸»é¢˜", "è¯é¢˜", "é¢˜å", "topic"]
        question_keywords = ["é—®é¢˜", "èµ·å§‹", "question"]
        
        result = {"topic": "", "question": ""}
        lines = text.strip().split("\n")
        
        # ç¬¬ä¸€é: å…³é”®è¯å®šä½
        for line in lines:
            lower_line = line.lower()
            for key in topic_keywords:
                if key in lower_line:
                    result["topic"] = line.split(":", 1)[-1].strip()
            for key in question_keywords:
                if key in lower_line:
                    result["question"] = line.split(":", 1)[-1].strip()
        
        # ç¬¬äºŒé: æå–æ ¸å¿ƒå†…å®¹
        if not result["topic"]:
            topic_candidates = [line for line in lines if not any(qk in line for qk in question_keywords)]
            if topic_candidates:
                result["topic"] = topic_candidates[0].strip()
        
        if not result["question"]:
            question_candidates = [line for line in lines if "?" in line or "ï¼Ÿ" in line]
            if question_candidates:
                result["question"] = question_candidates[0].strip()
        
        # é»˜è®¤æƒ…å†µå¤„ç†
        if not result["topic"]:
            result["topic"] = "æ²™ä¸˜ä¸–ç•Œè®¾å®š"
        if not result["question"]:
            result["question"] = "è¯·è§£é‡Šé¦™æ–™åœ¨æ²™ä¸˜å®‡å®™ä¸­çš„é‡è¦æ€§"
        
        print(f"ğŸ” æå–ç»“æœ: {result}")
        return result
    
    def _fallback_topic(self) -> dict:
        """å¤‡ç”¨ä¸»é¢˜åˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰"""
        topics = [
            {"topic": "é¦™æ–™ç»æµå­¦", "question": "é¦™æ–™åœ¨æ²™ä¸˜å®‡å®™ä¸­çš„ç»æµå’Œæ”¿æ²»æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"},
            {"topic": "ä¿ç½—çš„é¢„çŸ¥èƒ½åŠ›", "question": "ä¿ç½—çš„é¢„çŸ¥èƒ½åŠ›å¦‚ä½•å¡‘é€ ä»–çš„å‘½è¿å†³ç­–ï¼Ÿ"},
            {"topic": "å¼—ç‘æ›¼äººæ–‡åŒ–ä¸ç”Ÿæ€è§‚", "question": "å¼—ç‘æ›¼äººå¦‚ä½•é€‚åº”é˜¿æ‹‰åŸºæ–¯çš„æç«¯ç¯å¢ƒå¹¶å‘å±•å‡ºç‹¬ç‰¹æ–‡åŒ–ï¼Ÿ"},
            {"topic": "è´å°¼Â·æ°ç‘Ÿé‡Œç‰¹å§å¦¹ä¼šçš„æƒè°‹", "question": "è´å°¼Â·æ°ç‘Ÿé‡Œç‰¹å§å¦¹ä¼šçš„ç¹æ®–è®¡åˆ’ç›®æ ‡å’Œå®æ–½ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ"},
            {"topic": "å“ˆå…‹å—ä¸å„å´”è¿ªå®¶æ—çš„ä¸–ä»‡", "question": "å“ˆå…‹å—å®¶æ—ä¸å„å´”è¿ªå®¶æ—çš„äº‰æ–—å¦‚ä½•å½±å“æ²™ä¸˜å®‡å®™çš„æ”¿æ²»æ ¼å±€ï¼Ÿ"},
            {"topic": "æ²™è™«ç”Ÿæ€ç³»ç»Ÿ", "question": "æ²™è™«åœ¨é˜¿æ‹‰åŸºæ–¯ç”Ÿæ€ç³»ç»Ÿä¸­çš„è§’è‰²æœ‰ä»€ä¹ˆç‹¬ç‰¹æ€§ï¼Ÿ"},
            {"topic": "é—¨å¡”ç‰¹è®¡ç®—æŠ€æœ¯", "question": "é—¨å¡”ç‰¹çš„è®¡ç®—èƒ½åŠ›åœ¨æ²™ä¸˜å®‡å®™ä¸­æœ‰å“ªäº›åº”ç”¨å’Œå±€é™ï¼Ÿ"},
            {"topic": "æ²™ä¸˜ä¸­çš„æ°´ä¼¦ç†", "question": "æ°´åœ¨å¼—ç‘æ›¼æ–‡åŒ–ä¸­çš„è±¡å¾æ„ä¹‰å’Œå®é™…ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ"}
        ]
        return random.choice(topics)


class DuneFollowUpGenerator:
    """æ ¹æ®å¯¹è¯å†å²ç”Ÿæˆåç»­é—®é¢˜"""
    def __init__(self):
        self.llm = self._init_azure_llm()
        self.prompt_template = """
        # è§’è‰²ï¼šç§‘å¹»å°è¯´ä¸“å®¶ï¼Œä¸“æ³¨ã€Šæ²™ä¸˜ã€‹ç³»åˆ—
        # ä»»åŠ¡ï¼šåŸºäºå¯¹è¯å†å²æå‡ºè‡ªç„¶è¿è´¯çš„åç»­é—®é¢˜
        #
        # å½“å‰å¯¹è¯ä¸»é¢˜ï¼š{topic}
        # æœ€æ–°å›ç­”æ‘˜è¦ï¼š{answer_summary}
        # 
        # è¦æ±‚ï¼š
        # 1. é—®é¢˜å¿…é¡»ä¸ä¸»é¢˜({topic})ç´§å¯†ç›¸å…³
        # 2. åº”åŸºäºæœ€æ–°å›ç­”ä¸­çš„ä¿¡æ¯ç»§ç»­æ·±å…¥
        # 3. å¯ä»¥æ˜¯æ¾„æ¸…ã€æ‰©å±•æˆ–æŒ‘æˆ˜æ€§æé—®
        # 4. è¾“å‡ºï¼šå•ä¸€é—®é¢˜ï¼ˆä¸å¸¦ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼‰
        """
        
    def _init_azure_llm(self) -> AzureChatOpenAI:
        return AzureChatOpenAI(
            openai_api_version="2024-12-01-preview",
            azure_deployment="gpt-4o",
            model_name="gpt-4o",
            azure_endpoint="https://tcamp.openai.azure.com/",
            openai_api_key=api_key,
            temperature=0.7,
            max_tokens=500
        )
    
    async def generate_followup(
        self, 
        topic: str, 
        last_answer: str
    ) -> str:
        """ç”ŸæˆåŸºäºä¸Šä¸‹æ–‡çš„åç»­é—®é¢˜"""
        try:
            # åˆ›å»ºé—®é¢˜æ‘˜è¦ç¡®ä¿ä¸è¶…é•¿
            summary = last_answer[:1500] if len(last_answer) > 1500 else last_answer
            prompt = PromptTemplate(
                template=self.prompt_template,
                input_variables=["topic", "answer_summary"]
            )
            
            chain = LLMChain(llm=self.llm, prompt=prompt)
            response = await chain.ainvoke({
                "topic": topic,
                "answer_summary": summary
            })
            
            return response.get('text', '').strip().strip('"').strip("'")
        except Exception as e:
            print(f"âš ï¸ åç»­é—®é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸºç¡€é—®é¢˜")
            return self._fallback_question(topic)
    
    def _fallback_question(self, topic: str) -> str:
        """å¤‡ç”¨é—®é¢˜çš„é€šç”¨æ¨¡å¼"""
        questions = [
            f"å…³äº{topic}ï¼Œè¯·æä¾›æ›´å¤šç»†èŠ‚ï¼Ÿ",
            f"ä½ èƒ½æ›´è¯¦ç»†åœ°è§£é‡Šè¿™ä¸ªè¯é¢˜:{topic}çš„æŸä¸ªæ–¹é¢å—ï¼Ÿ",
            f"åœ¨{topic}çš„èƒŒæ™¯ä¸‹ï¼Œè¿˜æœ‰ä»€ä¹ˆé‡è¦å› ç´ éœ€è¦è€ƒè™‘ï¼Ÿ",
            f"åœ¨è®¨è®º{topic}æ—¶ï¼Œå¸¸è§çš„äº‰è®®ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        return random.choice(questions)

class RequestLogger:
    """å¢å¼ºå‹è¯·æ±‚æ—¥å¿—ç³»ç»Ÿ"""
    def __init__(self):
        self.logs = []
    
    def log(self, entry: dict):
        """è®°å½•å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—é¡¹"""
        entry["timestamp"] = datetime.now().isoformat()
        self.logs.append(entry)
        
        # å½“æ—¥å¿—ç§¯å‹è¿‡å¤§æ—¶è‡ªåŠ¨æ¸…ç†
        if len(self.logs) > 100:
            self.logs = self.logs[-50:]
    
    def analyze_failures(self):
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        failures = [log for log in self.logs if log["status"] == "failure"]
        
        if not failures:
            return "æ— å¤±è´¥è®°å½•"
        
        # ç»Ÿè®¡å¸¸è§é”™è¯¯ç±»å‹
        error_counts = {}
        for f in failures:
            error_type = f["error"].split(":")[0]  # å–é”™è¯¯ç±»å‹å‰ç¼€
            error_counts[error_type] = error_counts.get(error_type, 0) + 1
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = f"### è¿‡å»{len(failures)}æ¬¡å¤±è´¥è¯·æ±‚åˆ†æ\n"
        report += "å¸¸è§é”™è¯¯ç±»å‹ï¼š\n"
        for error, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True):
            report += f"- {error}: {count}æ¬¡\n"
        
        # æœ€å5æ¡é”™è¯¯è¯¦æƒ…
        report += "\næœ€è¿‘å¤±è´¥è¯¦æƒ…ï¼š\n"
        for f in failures[-5:]:
            report += f"{f['timestamp']} [{f['strategy']}]: {f['error']}\n"
        
        return report


import json
from datetime import datetime
import random

class InteractiveDialogueSystem:
    """ç®€åŒ–ç‰ˆä¸»é¢˜å¯¼å‘å¤šè½®å¯¹è¯ç³»ç»Ÿï¼Œä»…ä¿ç•™æ ¸å¿ƒäº¤äº’å’Œå­˜å‚¨åŠŸèƒ½"""
    def __init__(self, graph_agent):
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.topic_gen = DuneTopicGenerator()
        self.followup_gen = DuneFollowUpGenerator()
        self.dialogues = []
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.agent_executor = create_graphrag_agent(graph_agent)
    
    async def run_conversation(self):
        """æ‰§è¡Œä¸€ä¸ªè¿è´¯ä¸»é¢˜çš„å¯¹è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç”Ÿæˆä¸»é¢˜å’Œèµ·å§‹é—®é¢˜
        topic_data = await self.topic_gen.generate_topic()
        topic = topic_data["topic"]
        current_question = topic_data["question"]
        
        conversation = []
        print(f"\n{'=' * 50}")
        print(f"ğŸ å¯åŠ¨æ–°å¯¹è¯ | ä¸»é¢˜: {topic}")
        print(f"âš¡ èµ·å§‹é—®é¢˜: {current_question}")
        print(f"{'=' * 50}")
        
        # ç¡®å®šå¯¹è¯è½®æ¬¡ (4-6è½®)
        max_turns = random.randint(4, 6)
        
        for turn in range(max_turns):
            # è·å–å›ç­”
            answer_data = await self.agent_executor.ainvoke({
                "input": current_question
            })
            
            # æ˜¾ç¤ºå›ç­”
            print("\n--- Agent å›ç­” ---")
            print(answer_data.get("output"))
            print("--------------------\n")
            answer = answer_data.get("output")
            
            # å­˜å‚¨å½“å‰è½®æ¬¡
            conversation.append({
                "turn": turn,
                "question": current_question,
                "answer": answer
            })
            
            # å¦‚æœæ˜¯æœ€åä¸€è½®åˆ™åœæ­¢
            if turn == max_turns - 1:
                break
            
            # ç”Ÿæˆåç»­é—®é¢˜
            current_question = await self.followup_gen.generate_followup(
                topic=topic,
                last_answer=answer
            )
            print(f"ğŸ” ç”Ÿæˆåç»­é—®é¢˜: {current_question}")
        
        # å­˜å‚¨å®Œæ•´å¯¹è¯
        self.store_conversation(topic, conversation)
        print(f"âœ… ä¸»é¢˜ '{topic}' å®Œæˆ | è½®æ¬¡: {len(conversation)}")
    
    def store_conversation(self, topic: str, conversation: list):
        """å­˜å‚¨å¯¹è¯åˆ°å†…å­˜æ•°æ®é›†"""
        self.dialogues.append({
            "session": f"{self.session_id}_{len(self.dialogues)+1}",
            "topic": topic,
            "conversation": conversation,
            "created_at": datetime.now().isoformat()
        })
    
    def save_to_jsonl(self, filename: str = "dune_themed_dialogues.jsonl"):
        """ä¿å­˜æ‰€æœ‰å¯¹è¯åˆ°JSONLæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            for dialogue in self.dialogues:
                # åˆ›å»ºç²¾ç®€ç»“æ„
                clean_entry = {
                    "session": dialogue["session"],
                    "topic": dialogue["topic"],
                    "created_at": dialogue["created_at"],
                    "conversation": dialogue["conversation"]
                }
                f.write(json.dumps(clean_entry, ensure_ascii=False) + '\n')
        print(f"ğŸ’¾ å·²ä¿å­˜ {len(self.dialogues)} ä¸ªå¯¹è¯åˆ° {filename}")


async def main():
    # 1. å…ˆåˆå§‹åŒ– GraphAnalysisAgent
    graph_agent = GraphAnalysisAgent(use_multi_book=True)
    
    # 2. å¢å¼ºçŸ¥è¯†åº“åŠ è½½é€»è¾‘
    book_path = "./rag/output"
    if os.path.exists(book_path):
        print(f"ğŸ“š æ‰¾åˆ°çŸ¥è¯†åº“è·¯å¾„: {book_path}")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        required_files = ["communities.parquet", "entities.parquet", 
                          "community_reports.parquet", "relationships.parquet"]
        
        missing_files = [f for f in required_files 
                        if not os.path.exists(os.path.join(book_path, f))]
        
        if missing_files:
            print(f"âš ï¸ çŸ¥è¯†åº“ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {', '.join(missing_files)}")
            print("âš ï¸ å°†ä»åœ¨çº¿æºè¡¥å……çŸ¥è¯†åº“...")
            
            # ä»åœ¨çº¿çŸ¥è¯†æºè¡¥å……åŸºç¡€çŸ¥è¯†
            try:
                await graph_agent.global_search_full_async("æ²™ä¸˜ä¸–ç•ŒåŸºç¡€è®¾å®š")
            except Exception as e:
                print(f"â›” çŸ¥è¯†åº“è¡¥å……å¤±è´¥: {e}")
        else:
            graph_agent.add_book("dune_default", book_path)
            graph_agent.switch_book("dune_default")
            print("âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸ")
            
            # æµ‹è¯•è¿æ¥
            try:
                await graph_agent.global_search_full_async("æµ‹è¯•è¿æ¥")
                print("âœ… çŸ¥è¯†åº“è¿æ¥æ­£å¸¸")
            except Exception as e:
                print(f"â›” çŸ¥è¯†åº“è¿æ¥å¼‚å¸¸: {e}")
    else:
        print("âš ï¸ é»˜è®¤çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨")
    
    # 3. å°† graph_agent ä¼ é€’ç»™ InteractiveDialogueSystem
    dialogue_system = InteractiveDialogueSystem(graph_agent)
    
    # 4. æ‰§è¡Œå¤šä¸ªä¸»é¢˜çš„å¯¹è¯
    num_conversations = 1
    print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆ {num_conversations} ä¸ªä¸»é¢˜çš„å¤šè½®å¯¹è¯...")
    
    for i in range(num_conversations):
        await dialogue_system.run_conversation()
        if i < num_conversations - 1:
            print("\n" + "="*50)
            print(f"âš¡ å‡†å¤‡ä¸‹ä¸€å¯¹è¯ ({i+1}/{num_conversations})")
            print("="*50)
    
    # 5. æ‰“å°ç»Ÿè®¡æŠ¥å‘Šå¹¶ä¿å­˜ç»“æœ
    dialogue_system.print_stats_report()
    dialogue_system.save_to_jsonl()
    print("âœ… æ‰€æœ‰å¯¹è¯å®Œæˆå¹¶ä¿å­˜ï¼")

if __name__ == "__main__":
    asyncio.run(main())
